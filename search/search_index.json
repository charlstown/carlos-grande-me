{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"carlosgrande.me","text":"<ul> <li>All</li> <li>Articles</li> <li>Case studies</li> <li>Notebooks</li> <li>Projects</li> <li>Resourcess</li> </ul>"},{"location":"about-me/","title":"About me","text":"<p>Welcome to my blog! I'm Carlos Grande a Data Engineer, Architect, and Coding enthusiast. In this site, you'll find projects, articles, and resources about data, coding, business, and AI. I hope you enjoy my content and find it useful.</p> <p>The main reason I started to write this blog was to collect small pieces of knowledge and some of my favorite projects as a life cheat sheet or a personal guide. I wanted to follow an open-data practice inviting everyone to use my resources and improve my projects and ideas.</p>"},{"location":"about-me/#my-digital-self","title":"My digital self","text":"<ul> <li> LinkedIn</li> <li> Behance</li> <li> Github</li> <li> KitCo</li> </ul>"},{"location":"notebooks/","title":"Notebooks","text":"<p>This section is my personal collection of notebooks across various domains. Here, you'll find detailed explorations and practical exercises on business strategies, coding challenges, data science, and data architecture.</p>"},{"location":"notebooks/#business-insights","title":"Business Insights","text":"<p>My notes on team dynamics, strategic frameworks, and business models to sharpen my organizational skills.</p> <p> Start exploring</p>"},{"location":"notebooks/#coding-challenges","title":"Coding Challenges","text":"<p>My collection of coding notebooks with patterns, challenges, and problem-solving exercises to refine my skills.</p> <p> Start exploring</p>"},{"location":"notebooks/#data-science","title":"Data Science","text":"<p>My personal notebooks for data analysis, machine learning experiments, and key data science techniques.</p> <p> Start exploring</p>"},{"location":"notebooks/#data-architecture","title":"Data Architecture","text":"<p>My references for technical architectures, cloud solutions, and technology insights to guide my data architecture work.</p> <p> Start exploring</p>"},{"location":"notebooks/business/business-model-design/","title":"The Business model design","text":"<p>Lean startup is a methodology for developing businesses and products that aim to shorten product development cycles and rapidly discover if a proposed business model is viable; this is achieved by adopting a combination of business-hypothesis-driven experimentation, iterative product releases, and validated learning. (Ries, 2011)</p> <p>\"A business model describes the rationale of how an organization creates, delivers, and captures value, in economic, social, cultural, or other contexts.\" (Geissdoerfer, Savaget, Evans. 2017)</p>"},{"location":"notebooks/business/business-model-design/#the-market-size","title":"The market size","text":"<p>Following the definition from Alexa blog: \"Market size is the number of individuals in a certain market segment who are potential buyers. Companies should determine market size before launching a new product or service.\" (What is Market Size).</p> <p>One of the very first steps in developing a business model is estimating the market size. It tells you and your partners, team, and investors how much potential business is out there.</p>"},{"location":"notebooks/business/business-model-design/#tam-sam-som","title":"TAM, SAM, SOM","text":"<p>1. TAM (Total Available Market): is the total available universe behind the problem I want to solve with my business. You can make rough estimations from the annual market turnover.</p> <p>2. SAM (Serviceable Available Market):  is the optimum portion of TAM targeted and served by a company's products or services (no shared market).</p> <p>3. SOM (Share of the market): is the percentage of SAM which is realistically reached by the chosen channels.</p>"},{"location":"notebooks/business/business-model-design/#great-estimation-examples","title":"Great estimation examples","text":"<p>These problems are so entertaining. Remember, we are not looking for an exact answer, and there are many different ways of estimating the answer.  To solve this we need to break down the problem.</p>"},{"location":"notebooks/business/business-model-design/#how-many-traffics-lights-are-in-madrid-city","title":"How many traffics lights are in Madrid city?","text":"<p>I prefer to start with a physical estimation approach like the number of buildings in a city, the number of boats in the sea... These type of estimations help you treat physical data and lose some fear of rough calculations.</p>"},{"location":"notebooks/business/business-model-design/#solution-a","title":"SOLUTION A","text":"<p>If we want to estimate the number of traffic lights inside the road ring M-30 in Madrid, one approach could be getting the M-30 Length then the surface inside, and finally the average distance between intersections.</p> <p>Data you need to know: M-30 length = 32,5km The distance between intersections = 100 m One intersection has 4 ligths</p> <p></p>"},{"location":"notebooks/business/business-model-design/#solution-b","title":"SOLUTION B","text":"<p>If we want to estimate the number of traffic lights inside the road another solution could be measuring roughly the number of urban squares inside the Madrid almond.</p> <p>Data you need to know: 1 Square = 100x100 m One square = 8 traffic lights Madrid is around x wide by 1.5x high</p> <p></p>"},{"location":"notebooks/business/business-model-design/#how-many-mechanics-are-in-madrid-province","title":"How many mechanics are in Madrid province?","text":"<p>Now let's work on a harder estimation. This is the classic example originated by Enrico Fermi. To solve this we need to break down the problem. We need to estimate the following:</p> <ul> <li> <p>Population in Madrid province (\\(P_m\\))</p> <p>\\(P_m \\thickapprox 6,5 \\cdotp 10^6\\)</p> </li> <li> <p>Number of total cars in Madrid (\\(N_{cars}\\))</p> <p>\\(1 family \\thickapprox 4 people \\implies 2 cars\\)</p> <p>\\(\\frac{4}{2} \\simeq \\frac{6,5\\cdotp 10^6}{N_{cars}}\\)</p> <p>\\(N_{cars} \\simeq \\frac{6,5\\cdotp 10^6}{2} \\simeq 3,25\\cdotp 10^6\\)</p> </li> <li> <p>How often each car has a breakdown per year (\\(N_{fixes}\\))</p> <p>\\((1 carcheck + 1 fix)/year\\)</p> <p>\\(N_{fixes} \\simeq 2/year\\)</p> </li> <li> <p>How much time it takes to repair each car (\\(t_{repair}\\))</p> <p>\\(3h &gt; t_{repair} &gt; 20h\\)</p> <p>\\(t_{repair} \\simeq 8h\\)</p> </li> <li> <p>How much time each mechanic works per year (\\(t_{work}\\))</p> <p>\\(1 worker \\implies 40h \\cdotp 50 weeks\\)</p> <p>\\(t_{work} \\simeq 2000h/year\\)</p> </li> </ul>"},{"location":"notebooks/business/business-model-design/#resolution","title":"Resolution","text":"<ul> <li> <p>Total time needed to repair all the cars in Madrid per year (\\(T\\))</p> <p>\\(T = N_{cars} \\cdotp N_{fixes} \\cdotp t_{repair}\\)</p> <p>\\(T = 3,25 \\cdotp 10^6 \\cdotp 2 \\cdotp 8\\)</p> <p>\\(T = 52 \\cdotp 10^6 hours/year\\)</p> </li> <li> <p>Number of Mechanics in Madrid N_{mechanics}</p> <p>\\(N_{mechanics} = \\frac{T}{t_{work}}\\)</p> <p>$N_{mechanics} = \\frac{52 \\cdotp 10^6}{2 \\cdotp 10^3} $</p> <p>\\(N_{mechanics} = 26.000\\)</p> </li> </ul> <p>This estimate suggests that Madrid likely requires around 26,000 mechanics to service its 3.25 million cars, based on repair frequency and work hours. While not exact, this approximation highlights the importance of structured problem-solving in making educated guesses.</p>"},{"location":"notebooks/business/business-model-design/#the-business-model","title":"The Business Model","text":"<p>A great example of this relation between the product and the business model could be the coffee bean as a product. </p> <p>Company A: serves coffee directly to consumers in a place where the coffee experience was about creating a sense of community. A lounge space with sofas and wifi where you feel so comfortable. Can you guess the name of this company A? yes, it's Starbucks.</p> <p>Company B: sells coffee ready to make in capsules and coffee machines with excellent design and ease of use, using a two-channel approach, retail and direct. Can you guess the name of this company A? yes, it's Nespresso.</p> <p>With these two examples, we can see how the business model is the real company product.</p>"},{"location":"notebooks/business/business-model-design/#6-formulas-to-develop-your-business-model","title":"6 formulas to develop your business model","text":"<ol> <li> <p>Solve a real problem: you can always start the business development process by thinking about an actual problem that needs to be solved or a solution that is barely solved.</p> <p>A great example of this situation is the hand dryer invention. Until 2006 the hand dryer used the wide jet of heated air that didn't dry the hands so well. In 2006 Dyson launches the Dyson Airblade, a hand dryer that uses a thin layer of unheated air traveling at around 640 k/h, drying hands in 10 seconds and using less electricity than conventional hand dryers. This innovation solved the original problem even though there was another solution already in the market.</p> </li> <li> <p>Detect opportunities in the market: there are three areas sociological, technological, and legislative, where you can find new opportunities. From the sociological side, you can analyze habits, trends, population growth, etc. From the technological side, you can use new inventions like patents, sensors, batteries, etc. to generate new opportunities. From the legislative side, you can take advantage of new laws that create a problem for other entities.</p> </li> <li> <p>Analyze your competitors: another approach could be analyzing the customers from the competitors. You can study direct and indirect customers, or not served either discontent customers. Another analysis is becoming a client from your competitors and tests the services or the products.</p> </li> <li> <p>Explore your business opportunities: an important question you can ask yourself is: What have I learned from my actual business or the things I do? The answer to this question can give you a completely new business model. A case study for this point could be Amazon or KH Lloreda companies.</p> <p>Amazon started as an online library marketplace, taking advantage of the cloud and computer knowledge, they began to develop a new business model based on providing on-demand cloud computing platforms and APIs to individuals, companies, and government.</p> <p>On the other side, KH Lloreda started in 1949, as a metallic linings provider developing their own cleaning product due to the inexistence in the market. In 1994, the company undertook a strategic market change, focusing on cleaning products and developing KH-7 becoming the fifth most valorized brand in Spain.</p> </li> <li> <p>Open innovation: another method to discover new ideas is partnering with other entrepreneurs that have a great business model, product, or service but aren't able to develop their idea. You can find so many online platforms like kickstarted or ides4all where you can find new and trending ideas. You shouldn't underestimate thousands of people proposing ideas.</p> </li> <li> <p>Hybridization: another method to develop a business model is by hybridization, mixing products with services, or vice versa. A great example of this method is the Smartbox company which, developed a business model using a service as a product, in this case, experiences becoming them into a gift box.</p> </li> </ol>"},{"location":"notebooks/business/business-model-design/#6-resources-by-google-to-discover-market-insights-for-your-business","title":"6 Resources by google to discover market insights for your business","text":"<p>These three tools are essential to analyze the market and discover the trends, ideas, and problems to solve in the actual society.</p>"},{"location":"notebooks/business/business-model-design/#google-trends","title":"Google Trends","text":"<p>A Google website that analyzes the popularity of top search queries in Google Search across various regions and languages. The website uses graphs to compare the search volume of different queries over time.</p> <p> Google trends</p>"},{"location":"notebooks/business/business-model-design/#google-insights-library","title":"Google Insights Library","text":"<p>Stay up to date with all the latest research from across Think with Google. Insights Library is your destination to search, discover, and interact with the insights that will fuel your business.</p> <p> Google insights</p>"},{"location":"notebooks/business/business-model-design/#google-find-my-audience","title":"Google Find My Audience","text":"<p>Find My Audience helps you understand who your most valuable customers are on YouTube \u2014 so you can discover new audiences and learn how to reach them individually with relevant messages.</p> <p> Google find my audience</p>"},{"location":"notebooks/business/business-model-design/#google-market-finder","title":"Google Market Finder","text":"<p>Free tool to identify new potential markets, discover helpful operational information and start selling to customers at home and around the world.</p> <p> Market finder</p>"},{"location":"notebooks/business/business-model-design/#rising-retail-categories","title":"Rising Retail Categories","text":"<p>Use this interactive tool to understand fast-rising retail categories in Google Search, the locations where they\u2019re growing, and the queries associated with them. The data will update daily to reflect changes in Search interests.</p> <p> Trends category</p>"},{"location":"notebooks/business/business-model-design/#google-grow-my-store","title":"Google Grow My Store","text":"<p>Google offers a service to analyze your retail site, give you an overall score and offer you detailed insights and recommendations to help you strengthen your business.</p> <p> Grow my store</p>"},{"location":"notebooks/business/business-model-design/#mapping-the-stakeholders","title":"Mapping the stakeholders","text":"<p>Stakeholders are those people, groups, or individuals who either have the power to affect, or are affected by the endeavour you're engaged with. Stakeholders are affected and can affect your endeavours to varying degrees, and the degrees should be considered when analysing and mapping out the stakeholder landscape.</p> <p>As Lean Startup declares, the golden rule is to validate the customer environment and the product/service to discover rapidly if a proposed business model is viable. Accordingly, the next step to understand better our business model is to conduct a stakeholder analysis. This activity helps you identify project stakeholders, their expectations, and their relationships. There are three steps to follow in Stakeholder Analysis:</p> <ol> <li> <p>Identify who your stakeholders are. Think of all the people/organizations that are affected by your work. Who gives value to whom or have an interest in it.</p> <p>There are two types of stakeholders, the inside stakeholders like employees, chiefs, co-founders, etc., and the outside stakeholders like providers, society, government, clients, etc.</p> </li> <li> <p>Map your stakeholders. A significant way to map the stakeholders is by dividing them into two different groups, the inside, and the outside. Subsequently, you can draw them by their relations following the value proposition, the buyers, and the sellers. After generating this map you should ask yourself where is located your business model inside this map.</p> <p></p> </li> <li> <p>Understand Your Key Stakeholders. A simple way to summarize the level of backing you have from your stakeholders is to color-code them. For example, show advocates and supporters in green, blockers and critics in red, and those who are neutral in orange.</p> </li> </ol>"},{"location":"notebooks/business/business-model-design/#business-model-canvas","title":"Business model canvas","text":"<p>A Business Model Canvas is a strategic management and lean startup template for developing new or documenting existing business models. It helps visualize what is important and forces users to address key areas. It can also be used by a team (employees and/or advisors) to understand relationships and reach agreements.</p> <p></p>"},{"location":"notebooks/business/business-model-design/#1-customer-segments","title":"1. Customer segments","text":"<p>Customer segments are the community of customers or businesses that you are aiming to sell your product or services to. Customer segments is one of the most important building blocks in the business model canvas for your business, so getting this building block right is key to your success.</p> <ul> <li> <p>Define your client/s type: Try to summarize your client features like age, hobbies, routines, environment...</p> </li> <li> <p>Estimate and define your TAM, SAM and SOM:</p> <ul> <li>TAM: total addresssable market</li> <li>SAM: serviceable available market</li> <li>SOM: serviceable obtainable market</li> </ul> </li> <li> <p>Define the problem or need solved with your business model: Focus and describes how you solve your client\u2019s need.</p> </li> </ul>"},{"location":"notebooks/business/business-model-design/#2-value-propositions","title":"2. Value propositions","text":"<p>A value proposition is designed to convince a potential customer that your particular product or service will add more value or better solve a problem than your competition. It should answer the fundamental question of \u201cWhy should I buy your product instead of your competitor\u2019s product?\u201d</p> <ul> <li> <p>Describe your product or service: Describe and list your value proposition, focus on the problem you are solving.</p> </li> <li> <p>List the benefits that your idea provides to your client segment: List all the advantages that your idea provide to the market you are serving to.</p> </li> <li> <p>Describe the reason why your potential client is choosing you: Describe the reason and link it your client\u2019s type.</p> </li> </ul>"},{"location":"notebooks/business/business-model-design/#3-customer-relationships","title":"3. Customer relationships","text":"<p>Customer relationships describes the type of relationship a company establishes with it\u2019s specific customer segments. Customer relationships are driven by customer acquisition, customer retention, and boosting sales \u2013 in other words you need to get, keep, and grow your customer relationships.</p> <ul> <li> <p>Capture: List at least three the activities to gain clients</p> </li> <li> <p>Retention: list the activities to mantain clients</p> </li> <li> <p>Growth: list the activities to offer more servicies to your clients</p> </li> </ul> <p>In this section, it would be helpful to estimate the conversion rates from these activities.</p>"},{"location":"notebooks/business/business-model-design/#4-channels","title":"4. Channels","text":"<p>Channels are ways for you to reach your Customer Segments. And remember that in the initial stages it\u2019s important not to think about scale but to focus on learning. With that in mind try to think which channels will give you enough access to your Customer Segments at the same time give you enough learning. Channels can be email, social, CPC ads, blogs, articles, trade shows, radio &amp; TV, webinars etc. and BTW you don\u2019t have to be on all of them, just where your Customer Segments are.</p> <p>Channels: Describe 2-3 channels to provide your value proposition.</p> <ul> <li> <p>By form: directs and indirects</p> </li> <li> <p>By type: producer, wholesaler, retailer, and consumer.</p> </li> </ul> <p>Remember that one more channel implies one more cost too.</p>"},{"location":"notebooks/business/business-model-design/#5-revenue-streams","title":"5. Revenue streams","text":"<p>How you price your business will depend on the type of model it is, however, it\u2019s quite common for startups to lower their cost, even offer it for free to gain traction, however, this can pose a few problems. The key being it actually delays/avoids validation. Getting people to sign up for something for free is a lot different than asking them to pay. There is also the idea of perceived value.</p> <ul> <li> <p>Revenue model: Describe your revenue model type like pay per use, license, renting, asset sale, advertising, subscription...</p> </li> <li> <p>Price estimation: 3 alternatives</p> <ul> <li>By cost-benefit</li> <li>By competitors</li> <li>By demand curve</li> </ul> </li> </ul>"},{"location":"notebooks/business/business-model-design/#6-key-activities","title":"6. Key activities","text":"<p>These are really the activities that you do on a daily basis and what you need to understand is your strengths and weaknesses to which you can play on. These may include sales and marketing, research and development, manufacturing or distribution. Perhaps if you were an app development or software company your core focus should be to program and develop.</p> <p>Define this section with action verbs. Find the key activities to provide your value proposition to your customers. Ex: Develop an app, Website construction, customer service, software development, etc.</p> <p>Focus on those essential activities that enable your value proposition.</p>"},{"location":"notebooks/business/business-model-design/#7-key-resources","title":"7. Key resources","text":"<p>This is really the part that explains how you will create your value proposition. Have a think about what types of products, services, assets you may need to create your value proposition. This often includes intellectual property, talent, and infrastructure. Perhaps you have a website that is unique, perhaps you have offices or property that are very unique.</p> <p>Elements needed to provide your value proposition. 4 types:</p> <ul> <li>Physical resources: Computers, offices, stores, etc.</li> <li>Intellectual resources: Licenses, contracts, patents, etc.</li> <li>Human resources: Customer service, designers, etc.</li> <li>Liquid assets</li> </ul>"},{"location":"notebooks/business/business-model-design/#8-key-partners","title":"8. Key partners","text":"<p>Define your strategic allies essential for your business model.</p> <p>Ex: distribution, shipping, hosting provider, software development, marketing, customer service, etc.</p> <p>4 Different partner types:</p> <ul> <li>Buyer-Supplier partnership</li> <li>Coopetition and partnering</li> <li>Strategic Alliance</li> <li>Complementary partnering</li> </ul>"},{"location":"notebooks/business/business-model-design/#9-cost-structure","title":"9. Cost structure","text":"<p>Here you should list all the operational costs for taking this business to market. How much will it cost to build your idea? What is your burn rate \u2014 your total monthly running costs? How much will it cost to interview your customer segment? How much do market research papers cost? etc. You can then use these costs and potential revenue streams to calculate a rough break-even point.</p> <ul> <li>Key activities costs (internal): App development, design, customer service, shiping etc.</li> <li>Key resources costs (internal): Office space, warehouse, computers, licenses, designers, etc.</li> <li>Key partners costs (external): Shipping, providers, distribution, customer services.</li> </ul>"},{"location":"notebooks/business/business-model-design/#references-and-links","title":"References and links","text":"<ul> <li> <p>What is Market Size, Alexa blog.</p> </li> <li> <p>steve_mullen, An Introduction to Lean Canvas.</p> </li> <li> <p>Ries, E. (2011) personal interview. 2011</p> </li> <li> <p>Geissdoerfer, M., and Savaget, P., and Evans, S. (2017) The Cambridge Business Model Innovation Process</p> </li> <li> <p>Guesstimation book</p> </li> </ul>"},{"location":"notebooks/business/team-topologies/","title":"Team topologies","text":"<p>I recently joined a new project to design an Internal Developer Platform (IDP) from the scratch. I was so interested in analyzing the organization of the teams, its typologies and its interactions. I started looking for references and found the book The Team Topologies: Organizing Business and Technology Teams for Fast Flow by Matthew Skelton and Manuel Pais.</p> <p>I've long enjoyed learning from Matthew's and Manuel's work, and totally recommend their content. It's great to see that their vision for organizing teams has been collated into a single book.  For anyone in the tech/organization design field, Team Topologies is well worth reading. I share in this post my notes on reading this great book.</p>"},{"location":"notebooks/business/team-topologies/#1-team-concepts","title":"1 Team concepts","text":""},{"location":"notebooks/business/team-topologies/#11-the-reverse-conway-maneuver","title":"1.1 The Reverse Conway Maneuver","text":"<p>The reverse Conway maneuver gained traction in the technology world around 2015 and has been applied in many organizations since. Accelerate: The Science of Dev Ops by Nicole Forsgren, PhD, Jez Humble, and Gene Kim supports the importance of this strategy for high-performing organizations.</p> <p>Conway law: Organizations wich design systems, are constrained to produce designs wich are copies of the communication structures of these organizations.</p> <p></p> <p>Conway\u2019s law tells us that an organization\u2019s structure and the actual communication paths between teams persevere in the resulting architecture of the systems built. They void the attempts of designing software as a separate activity from the design of the teams themselves.</p> <p>The effects of this simple law are far reaching. On one hand, the organization\u2019s design limits the number of possible solutions for a given system\u2019s architecture. On the other hand, the speed of software delivery is strongly affected by how many team dependencies the organization design instills.</p> <p>Fast flow requires restricting communication between teams. Team collaboration is important for gray areas of development, where discovery and expertise is needed to make progress. But in areas where execution prevails\u2014not discovery\u2014communication becomes an unnecessary overhead.</p> <p>In short, by considering the impact of Conway\u2019s law when designing software architectures and/or reorganizing team structures, you will be able to takeadvantage of the isomorphic force at play, which converges the software architecture and the team design.</p> <p>Organizations should evolve their team and organizational structure to achieve the desired architecture. The goal is for your architecture to support the ability of teams to get their work done\u2014from design through to deployment\u2014without requiring high-bandwidth communication between teams.</p>"},{"location":"notebooks/business/team-topologies/#12-team-first-approach","title":"1.2 Team First Approach","text":"<p>A research by Google on their own teams found that when it comes to measuring performance, teams matter more than individuals. It is a good approach to start with the team for effective software delivery. There are multiple aspects to consider like team size, team domains, team relationships, and team cognition.</p>"},{"location":"notebooks/business/team-topologies/#121-limiting-the-size-of-the-teams","title":"1.2.1 Limiting the Size of the teams","text":"<p>Amazon, for instance, is known for limiting the size of its software teams to those that can be fed by two pizzas. This limit, recommended by popular frameworks such as Scrum, derives from evolutionary limits on group recognition and trust known as Dunbar\u2019s number (after anthropologist Robin Dunbar). Dunbar found fifteen to be the limit of the number of people one person can trust deeply. From those, only around five people can be known and trusted closely.</p> <p></p> <p>In the context of products and services enabled by software systems, the limits exposed by Dunbar\u2019s number mean that the number of people in different business lines or streams of work should also explicitly be limited when the number of people in a department exceeds fifty (or 150, or 500), the internal and external dynamics with other groupings will change. This, in turn, means that the software architecture needs to be realigned with the new team groupings so that teams can continue to own the architecture effectively. This is an example of what we like to call \u201cteam-first architecture,\u201d which requires a substantially new way of thinking for many organizations</p> <ul> <li>A single team: around five to eight people (based on industry experience)</li> <li>In high-trust organizations: no more than fifteen people</li> <li>Families (\u201ctribes\u201d): groupings of teams of no more than fifty people</li> <li>In high-trust organizations: groupings of no more than 150 people</li> <li>Divisions/streams/profit &amp; loss (P&amp;L) lines: groupings of no more than 150 or 500 people.</li> </ul> <p>Organizations can be composed from Dunbar-compatible groupings of these sizes; when one of the limits is reached, the need to split off another unit as a semi-independent grouping arises. We can visualize this \u201cscaling by Dunbar\u201d as concentric circles of increasingly larger or smaller groups (see Figure)</p>"},{"location":"notebooks/business/team-topologies/#122-limiting-the-number-and-type-of-domains-per-team","title":"1.2.2 Limiting the number and type of domains per team","text":"<p>To get started, identify distinct domains that each team has to deal with, and classify these domains into simple (most of the work has a clear path of action), complicated (changes need to be analyzed and might require a few iterations on the solution to get it right), or complex (solutions require a lot of experimentation and discovery). You should finetune the resulting classification by comparing pairs of domains across teams.</p> <ul> <li>The first heuristic is to assign each domain to a single team. If a domain is too large for a team, instead of splitting responsibilities of a single domain to multiple teams, first split the domain into subdomains and then assign each new subdomain to a single team.</li> <li>The second heuristic is that a single team should be able to accommodate two to three \u201csimple\u201d domains. Because such domains are quite procedural, the cost of context switching between domains is more bearable, as responses are more mechanical.</li> <li>The third heuristic is that a team responsible for a complex domain should not have any more domains assigned to them\u2014not even a simple one.</li> <li>The last heuristic is to avoid a single team responsible for two complicated domains. Instead, it\u2019s best to split the team into two separate teams of five people (by recruiting one or two more team members), so they can each be more focused and autonomous.</li> </ul> <p></p>"},{"location":"notebooks/business/team-topologies/#123-limiting-the-team-cognitive-load","title":"1.2.3 Limiting the team cognitive load","text":"<p>With a team-first approach, the team's responsibilities are matched to the cognitive load that the team can handle. For software-delivery teams a team-first approach to cognitive load means limiting the size of the software system that a team is expected to work with; that is, organizations should not allow a software subsystem to grow beyond the cognitive load of a team responsible for the software.</p> <p>Cognitive load was characterized in 1988 by psychologist Jhon Sweller as \"the total amount of mental effort being used in the working memory.\" Sweller defines three different kinds of cognitive load:</p> <ul> <li>Intrinsic cognitive load: relates to aspect of the task fundamental to the problem space. (How do I create a new method?)</li> <li>Extraneous cognitive load: relates to the environment in which the task is being done (How do I deploy this component again?)</li> <li>Germane cognitive load: relates to aspects of the task that need special attention for learning or high performance. (How should a service interact with other service)</li> </ul> <p></p> <p>For example, the intrinsic cognitive load for a developer could be the knowledge of the computer language being used, the extraneous cognitive load might be details of the commands needed to instantiate a dynamic testing environment, and the germane cognitive load could be the specific aspects of the business domain that the developer is programming.</p> <p>For effective delivery and operations of modern software systems, organizations should attempt to minimize intrinsic cognitive load (through training, good choice of technologies, hiring, pair programming, etc.) and eliminate extraneous cognitive load altogether (boring or superfluous tasks or commands that add little value to retain in the working memory and can often be automated away), leaving more space for germane cognitive load (which is where the \u201cvalue add\u201d thinking lies).</p>"},{"location":"notebooks/business/team-topologies/#13-organizational-sensing","title":"1.3 Organizational sensing","text":"<p>Organizational sensing uses teams and their internal and external communication as the \u201csenses\u201d of the organization (sight, sound, touch, smell, taste)\u2014what Peter Drucker calls \u201csynthetic sense organs for the outside.\u201d To sense things (and make sense of things), organisms need defined, reliable communication pathways. Similarly, with well-defined and stable communication pathways between teams, organizations can detect signals from across the organization and outside it, becoming something like an organism.</p> <p>Organizational sensing is the use of sociotechnical inputs (software, digital sensors, human groups, and \u201cweak signals\u201d) to provide rich awareness of the internal and external business environment.</p> <p>A key aspect of this sensory feedback is the use of IT operations teams as high-fidelity sensory input for development teams, requiring joined-up communications between teams running systems (Ops) and teams building systems (Dev). It is essential that organizations use signals from maintenance work as input to software-development activities. In the The DevOps Handbook, Gene Kim and colleagues define The Three Ways of DevOps for modern, high-performing organizations:</p> <ul> <li>Systems thinking: optimize for fast flow across the whole organization, not just in small parts.</li> <li>Feedback loops: Development informed and guided by Operations.</li> <li>Culture of continual experimentation and learning: sensing and feedback for every team interaction.</li> </ul> <p></p>"},{"location":"notebooks/business/team-topologies/#2-team-topologies","title":"2 Team Topologies","text":"<p>By deploying the four fundamental team topologies with the three core team interaction modes, organizations gain crucial clarity of purpose for their teams on an ongoing basis. Teams understand how, when, and why they need to collaborate with other teams; how, when, and why they should be consuming or providing something \u201cas a service\u201d; and how, when, and why they should provide or seek facilitation with another team. Thus, an organization should expect to see different kinds of interactions between different kinds of teams at any given time as the organization responds to new challenges.</p>"},{"location":"notebooks/business/team-topologies/#21-teams","title":"2.1 Teams","text":"<p>The four fundamental team topologies\u2014stream aligned, enabling, complicated subsystem, and platform\u2014should act as \u201cmagnets\u201d for all team types. All teams should move toward one of these four magnetic poles; that is, we should prefer these types, and aim to adopt the purpose, role, responsibility, and interaction behavior of these fundamental types for every team in our organization. Simplifying the types of teams to just these four helps to reduce ambiguity within the organization.</p> <p></p>"},{"location":"notebooks/business/team-topologies/#211-stream-aligned-teams","title":"2.1.1 Stream-aligned teams","text":"<p>A \u201cstream\u201d is the continuous flow of work aligned to a business domain or organizational capability. Continuous flow requires clarity of purpose and responsibility so that multiple teams can coexist, each with their own flow of work.</p> <p>Is a team aligned to a single, valuable stream of work; this might be a single product or service, a single set of features, a single user journey, or a single user persona.</p> <p>The stream-aligned team is the primary team type in an organization, and the purpose of the other fundamental team topologies is to reduce the burden on the stream-aligned teams.</p> <p>Because a stream-aligned team works on the full spectrum of delivery, they are, by necessity, closer to the customer and able to quickly incorporate feedback from customers while monitoring their software in production. Such a team can react to system problems in near real-time, steering the work as needed. In the words of Don Reinertsen: \u201cIn product development, we can change direction more quickly when we have a small team of highly skilled people instead of a large team.</p> <p>In most organizations, an effective team has a maximum size of around seven to nine people. Amazon, for instance, is known for limiting the size of its software teams to those that can be fed by two pizzas.</p> <p>The Amazon two-pizza-team model is an example of stream-aligned teams: the teams are substantially independent, have ownership over their services, and have responsibility for the runtime success of the software they write. The fact that Amazon has been using this model for over seventeen years shows how effective it can be to align teams to independent streams of change.</p>"},{"location":"notebooks/business/team-topologies/#212-platform-teams","title":"2.1.2 Platform teams","text":"<p>Jutta Eckstein has a suitable recommendation: \u201cTechnical-service teams should always regard themselves as pure service providers for the domain teams.\u201d</p> <p>The purpose of a platform team is to enable stream-aligned teams to deliver work with substantial autonomy. The platform team provides internal services to reduce the cognitive load that would be required from stream-aligned teams to develop these underlying services.</p> <p>In practice, platform teams are expected to focus on providing a smaller number of services of acceptable quality rather than a large number of services with many resilience and quality problems.</p> <p>In large organizations, a platform will need more than one team to build and run it (and in some cases, separate streams may each have their own platform). In these situations, a platform is composed of groups of other fundamental team types: stream aligned, enabling, complicated subsystem, and platform. Yes: the platform is itself built on a platform (see more on this later in this chapter).</p> <p>Traditionally, many infrastructure teams were responsible for all aspects of the live/production infrastructure, including any changes to applications deployed on that infrastructure. Converting an infrastructure team into a platform team enables rapid, safe flow of change both within the platform and\u2014crucially\u2014within stream-aligned teams.</p>"},{"location":"notebooks/business/team-topologies/#213-enabling-teams","title":"2.1.3 Enabling teams","text":"<p>How can a stream-aligned team with end-to-end ownership find the space for researching, reading about, learning, and practicing new skills? Stream-aligned teams are also under constant pressure to deliver and respond to change quickly.</p> <p>An enabling team is composed of specialists in a given technical (or product) domain, cross-cut to the stream-aligned teams and have the required bandwidth to research, try out options, and make informed suggestions on adequate tooling, practices, frameworks, and any of the ecosystem choices around the application stack.</p> <p>Enabling teams have a strongly collaborative nature; they thrive to understand the problems and shortcomings of stream-aligned teams in order to provide effective guidance. Jutta Eckstein calls them \u201cTechnical Consulting Teams. The end goal of an enabling team is to increase the autonomy of stream-aligned teams by growing their capabilities with a focus on their problems first, not the solutions per se. </p>"},{"location":"notebooks/business/team-topologies/#214-complicated-subsystem-teams","title":"2.1.4 Complicated subsystem teams","text":"<p>We can\u2019t expect to embed the necessary specialists in all the stream-aligned teams that make use of the subsystem; it would not be feasible, cost-effective, or in line with the stream-aligned team\u2019s goals.</p> <p>A complicated-subsystem team is responsible for building and maintaining a part of the system that depends heavily on specialist knowledge. The goal of this team is to reduce the cognitive load of stream-aligned teams working on systems that include or use the complicated subsystem.</p> <p>The critical difference between a traditional component team (created when a subsystem is identified as being or expected to be shared by multiple systems) and a complicated-subsystem team is that the complicated-subsystem team is created only when a subsystem needs mostly specialized knowledge. The decision is driven by team cognitive load, not by a perceived opportunity to share the component.</p>"},{"location":"notebooks/business/team-topologies/#22-teams-interactions","title":"2.2. Teams Interactions","text":"<p>When considering the relationship between any teams, a key decision is whether to collaborate with another team to achieve an objective or to treat the other team as providing a service. This choice between collaboration or consuming a service can be made at many different levels within the organization: consuming infrastructure as a service (from AWS, Azure, or Google Cloud, for instance), collaborating on logging and metrics, relying on a complicated-subsystem team to build a complex audio-processing codec, or working together on application deployment.</p> <p>To understand how and when to adapt the Team Topologies model for software systems, we need to define and understand three essential ways in which teams can and should interact, taking into account team-first dynamics and Conway\u2019s law.</p> <p></p>"},{"location":"notebooks/business/team-topologies/#221-collaboration","title":"2.2.1 Collaboration","text":"<p>During early phases of new systems development, and during periods where there is a need to quickly discover new information, technology limitations, and suitable practices, the collaboration mode is highly valuable. This is because team topologies that use collaboration can rapidly uncover new ways of working and unexpected behaviors of technologies.</p> <p>The collaboration team mode is suitable where a high degree of adaptability or discovery is needed, particularly when exploring new technologies or techniques.</p> <p>Two teams work closely together for a defined period to discover new patterns, approaches, and limitations. Responsibility is shared and boundaries blurred, but problems are solved rapidly and the organization learns quickly.</p> <p>The collaboration interaction mode is good for rapid discovery of new things, because it avoids costly hand-offs between teams.</p>"},{"location":"notebooks/business/team-topologies/#222-x-as-a-service","title":"2.2.2 X as a Service","text":"<p>The X-as-a-Service team interaction mode is suited to situations where there is a need for one or more teams to use a code library, component, API, or platform that \u201cjust works\u201d without much effort, where a component or aspect of the system can be effectively provided \u201cas a service\u201d by a distinct team or group of teams. The goal is to consume or provide something with minimal collaboration.</p> <p>One team consumes something (such as a service or an API) provided \u201cas a service\u201d from another team. Responsibilities are clearly delineated and\u2014if the boundary is effective\u2014the consuming team can deliver rapidly. The team providing the service seeks to make their service as easy to consume as possible.</p> <p>During later phases of systems development and periods where predictable delivery is needed (rather than discovery of new approaches), the X-as-a-Service model works best. In this model, teams can rely on certain aspects of their technology landscape being provided as a service by other teams (internal or external), allowing the team to focus on delivering their work.</p>"},{"location":"notebooks/business/team-topologies/#223-facilitating","title":"2.2.3 Facilitating","text":"<p>The facilitating team interaction mode is suited to situations where one or more teams would benefit from the active help of another team facilitating (or coaching) some aspect of their work. The facilitating interaction mode is the main operating mode of an enabling team and provides support and capabilities to many other teams, helping to enhance the productivity and effectiveness of these teams. </p> <p>One team helps another team to learn or adopt new approaches for a defined period of time. The team providing the facilitation aims to make the other team self-sufficient as soon as possible, while the team receiving the facilitation has an open-minded attitude to learning.</p> <p>Teams that interact using the facilitating mode typically work across many other teams, detecting and reducing cross-team problems and helping to inform the direction and capabilities of things like code libraries, APIs, and platforms provided as a service by other teams or organizations.</p>"},{"location":"notebooks/business/team-topologies/#3-team-topologies-vocabulary","title":"3. Team topologies vocabulary","text":"Term Definition Team Topologies: a model for organizational design that provides a key technology-agnostic mechanism for modern software-intensive enterprises to sense when a change in strategy is required (either from a business or technology point of view). Digital Platform: a digital platform is a foundation of self-service APIs, tools, services, knowledge and support which are arranged as a compelling internal product. Autonomous delivery teams can make use of the platform to deliver product features at a higher pace, with reduced coordination. Evan Bottcher\u2019s. Team: a stable grouping of five to nine people who work toward a shared goal as a unit as the smallest entity of delivery within the organization. Platform team: enables stream-aligned teams to deliver work with substantial autonomy. Enabling team: team(s) composed of specialists in a given technical (or product) domain; they help bridge the capability gap. stream-aligned team: a team aligned to a single, valuable stream of work. Dunbar\u2019s number: is a suggested cognitive limit to the number of people with whom one can maintain stable social relationships\u2014relationships in which an individual knows who each person is and how each person relates to every other person. This number was first proposed in the 1990s by British anthropologist Robin Dunbar, who found a correlation between primate brain size and average social group size. By using the average human brain size and extrapolating from the results of primates, he proposed that humans can comfortably maintain 150 stable relationships. Brooks\u2019s law: is an observation about software project management according to which \"adding manpower to a late software project makes it later\". It was coined by Fred Brooks in his 1975 book The Mythical Man-Month. According to Brooks, under certain conditions, an incremental person when added to a project makes it take more, not less time. Conway\u2019s law: law coined by Mel Conway that states that organizations, who design systems, are constrained to produce designs which are copies of the communication structures of these organizations. Cognitive load: the amount of working memory being used. Collaboration mode: team(s) working closely together with another team. Facilitating mode: team(s) helping (or being helped by) another team to clear impediments. X-as-a-Service mode: consuming or providing something with minimal collaboration. Domain complexity: how complex the problem is that is being solved via software. Fracture plane: a natural \u201cseam\u201d in the software system that allows it to be easily split into two or more parts. Organizational sensing: teams and their internal and external communication are the \u201csenses\u201d of the organization (sight, sound, touch, smell, taste)."},{"location":"notebooks/business/team-topologies/#references-and-links","title":"References and links","text":"<ul> <li>Building Stronger, Happier Engineering Teams with Team Topologies</li> <li>Team Topologies official website</li> <li>Team Topologies book</li> <li>Organizational evolution for accelerating delivery of comparison services at Uswitch</li> <li>Using Team Topologies to discover and improve reliability qualities</li> <li>Organizing Data Teams \u2014 Where to Make The Cut</li> <li>Data Team Organization</li> <li>How should our company structure our data team?</li> <li>More Notebooks like this here</li> </ul>"},{"location":"notebooks/coding/google-challenge-part-1/","title":"Google challenge part 1","text":"<ul> <li>Google Data Center 2079 By Beeple Crap</li> </ul> <p>I've been recently working with my friend \u00c1lvaro Sim\u00f3n Merino doing some research in Google about coding. We got an unusual message on the google site background, with a message:</p> <p>You're speaking our language. Up for a challenge?</p> <p>At first, we thought it was fake, a virus, or something like that. Then we realize the site was official! We could not believe to see Google sending us a challenge to solve and we accepted it immediately! Clicking on \u201cI want to play\u201d landed us on Google\u2019s Foobar page...</p> <ul> <li></li> <li>Google Foobar</li> </ul>"},{"location":"notebooks/coding/google-challenge-part-1/#what-is-google-foobar","title":"What is Google Foobar","text":"<p>Google Foobar challenge is a secret hiring process by the company to recruit top programmers and developers around the world. It is known that several developers at Google were hired by this process. To access to Foobar challenge you need to hit the secret key search pattern or be invited by someone that has reached level 3.</p> <p>The challenge consists of five levels with a total of nine questions, with the level of difficulty increasing at each level.</p> <p>This is how the Google Foobar begins:</p> <p>Why did you sign up for infiltration duty again? The pamphlets from Bunny HQ promised exotic and interesting missions, yet here you are drudging in the lowest level of Commander Lambda's organization. Hopefully you get that promotion soon...</p>"},{"location":"notebooks/coding/google-challenge-part-1/#level-i","title":"Level I","text":""},{"location":"notebooks/coding/google-challenge-part-1/#challenge-1-i-love-lance-janice","title":"Challenge 1: I Love Lance &amp; Janice","text":"<p>You've caught two of your fellow minions passing coded notes back and forth -- while they're on duty, no less! Worse, you're pretty sure it's not job-related -- they're both huge fans of the space soap opera \"\"Lance &amp; Janice\"\". You know how much Commander Lambda hates waste, so if you can prove that these minions are wasting time passing non-job-related notes, it'll put you that much closer to a promotion. </p> <p>Fortunately for you, the minions aren't exactly advanced cryptographers. In their code, every lowercase letter [a..z] is replaced with the corresponding one in [z..a], while every other character (including uppercase letters and punctuation) is left untouched.  That is, 'a' becomes 'z', 'b' becomes 'y', 'c' becomes 'x', etc.  For instance, the word \"\"vmxibkgrlm\"\", when decoded, would become \"\"encryption\"\".</p> <p>Write a function called solution(s) which takes in a string and returns the deciphered string so you can show the commander proof that these minions are talking about \"\"Lance &amp; Janice\"\" instead of doing their jobs.</p>"},{"location":"notebooks/coding/google-challenge-part-1/#python-cases","title":"Python cases","text":"<p>Your code should pass the following test cases. Note that it may also be run against hidden test cases not shown here.</p> <p>Test 01: <pre><code># Input\nsolution.solution(\"wrw blf hvv ozhg mrtsg'h vkrhlwv?\")\n\n# Output\n\"did you see last night's episode?\"\n</code></pre></p> <p>Test 02: <pre><code># Input\nsolution.solution(\"Yvzs! I xzm'g yvorvev Lzmxv olhg srh qly zg gsv xlolmb!!\")\n\n# Output\n\"Yeah! I can't believe Lance lost his job at the colony!!\"\n</code></pre></p>"},{"location":"notebooks/coding/google-challenge-part-1/#resolution","title":"Resolution","text":"<p>The first challenge was surprisingly easy. An atbash cipher!! By reversing the alphabet such that each letter is mapped to the letter in the same position in the reverse of the alphabet. Symbols and numbers are left intact in the same position.</p> <p><pre><code>def solution(x):\n    # Define a string with the alphabet\n    alpha = \"abcdefghijklmnopqrstuvwxyz\"\n    # Reverse the alphabet string\n    alpha_reverse = alpha[::-1]\n    # Generate a mapped dictionary\n    abc_map = dict(zip(alpha, alpha_reverse))\n    # Decode de message in a loop\n    decode_lst = [abc_map[letter] if letter in alpha else letter for letter in x]\n    # Join back the message in a string\n    decode_message = ''.join(decode_lst)\n    return decode_message\n</code></pre> <pre><code>solution(\"wrw blf hvv ozhg mrtsg'h vkrhlwv?\")\n</code></pre> \"did you see last night's episode?\"</p>"},{"location":"notebooks/coding/google-challenge-part-1/#level-ii","title":"Level II","text":""},{"location":"notebooks/coding/google-challenge-part-1/#challenge-2-bunny-worker-locations","title":"Challenge 2: Bunny Worker Locations","text":"<p>Keeping track of Commander Lambda's many bunny workers is starting to get tricky. You've been tasked with writing a program to match bunny worker IDs to cell locations.</p> <p>The LAMBCHOP doomsday device takes up much of the interior of Commander Lambda's space station, and as a result the work areas have an unusual layout. They are stacked in a triangular shape, and the bunny workers are given numerical IDs starting from the corner, as follows: <pre><code>| 7\n| 4 8\n| 2 5 9\n| 1 3 6 10\n</code></pre> Each cell can be represented as points (x, y), with x being the distance from the vertical wall, and y being the height from the ground. </p> <p>For example, the bunny worker at (1, 1) has ID 1, the bunny worker at (3, 2) has ID 9, and the bunny worker at (2,3) has ID 8. This pattern of numbering continues indefinitely (Commander Lambda has been adding a LOT of workers). </p> <p>Write a function solution(x, y) which returns the worker ID of the bunny at location (x, y). Each value of x and y will be at least 1 and no greater than 100,000. Since the worker ID can be very large, return your solution as a string representation of the number.</p>"},{"location":"notebooks/coding/google-challenge-part-1/#test-cases","title":"Test cases","text":"<p>Your code should pass the following test cases. Note that it may also be run against hidden test cases not shown here.</p> <p>Test 01: <pre><code># Input\nsolution.solution(\"wrw blf hvv ozhg mrtsg'h vkrhlwv?\")\n\n# Output\n\"did you see last night's episode?\"\n</code></pre></p> <p>Test 02: <pre><code># Input\nsolution.solution(\"Yvzs! I xzm'g yvorvev Lzmxv olhg srh qly zg gsv xlolmb!!\")\n\n# Output\n\"Yeah! I can't believe Lance lost his job at the colony!!\"\n</code></pre></p>"},{"location":"notebooks/coding/google-challenge-part-1/#resolution_1","title":"Resolution","text":"<p>The second challenge was easy too, but way more interesting. It purposes a triangular sequence represented with numbers, where you need to map the location of each number by coordinates returning the index of the position.</p> <p> Google challenge 02</p> <p>In my opinion, you can approach this challenge from multiple sides. The key, as always, is to divide the problem into smaller problems. So in my case, first I create a rule to get the row index in the triangle by summing both coordinates: - line 1 -&gt; (1, 1) -&gt; (1+1) - 1 row - line 2 -&gt; (1, 2), (2, 1) -&gt; (1+2) - 1 row</p> <p>Next, to get the previous value from a row you can sum the recursive sequence of that row: - line 3 -&gt; (1, 3), (2, 2), (3, 1) -&gt; 4 row - 2 row = 2 - row -&gt; 1 + 2 = 3</p> <p>Finally, I realized that by adding the X coordinate to the last number of a row, you can get the actual position: - (2, 3) -&gt; (2+3) - 2 row = 3 row -&gt; 1 + 2 + 3 = 6 -&gt; 6 + 2 = 8</p> <p>So to sum up: - coordinate-&gt; (2, 3) - Previous -&gt; 1+2+3 = 6 - Value -&gt; 6 + 2 = 8</p> <p><pre><code>def solution(x, y):\n    line = x + y -1\n    start_line = sum([i for i in range(line)])\n    worker_id = str(start_line + x)\n    return worker_id\n</code></pre> <pre><code>solution(2, 3)\n</code></pre> \"8\"</p>"},{"location":"notebooks/coding/google-challenge-part-1/#challenge-2-dont-get-volunteered","title":"Challenge 2: Don't Get Volunteered!","text":"<p>As a henchman on Commander Lambda's space station, you're expected to be resourceful, smart, and a quick thinker. It's not easy building a doomsday device and ordering the bunnies around at the same time, after all! In order to make sure that everyone is sufficiently quick-witted, Commander Lambda has installed new flooring outside the henchman dormitories. It looks like a chessboard, and every morning and evening you have to solve a new movement puzzle in order to cross the floor. That would be fine if you got to be the rook or the queen, but instead, you have to be the knight. Worse, if you take too much time solving the puzzle, you get \"volunteered\" as a test subject for the LAMBCHOP doomsday device!</p> <p>To help yourself get to and from your bunk every day, write a function called solution(src, dest) which takes in two parameters: the source square, on which you start, and the destination square, which is where you need to land to solve the puzzle.  The function should return an integer representing the smallest number of moves it will take for you to travel from the source square to the destination square using a chess knight's moves (that is, two squares in any direction immediately followed by one square perpendicular to that direction, or vice versa, in an \"L\" shape).  Both the source and destination squares will be an integer between 0 and 63, inclusive, and are numbered like the example chessboard below: <pre><code>-------------------------\n| 0| 1| 2| 3| 4| 5| 6| 7|\n-------------------------\n| 8| 9|10|11|12|13|14|15|\n-------------------------\n|16|17|18|19|20|21|22|23|\n-------------------------\n|24|25|26|27|28|29|30|31|\n-------------------------\n|32|33|34|35|36|37|38|39|\n-------------------------\n|40|41|42|43|44|45|46|47|\n-------------------------\n|48|49|50|51|52|53|54|55|\n-------------------------\n|56|57|58|59|60|61|62|63|\n-------------------------\n</code></pre></p>"},{"location":"notebooks/coding/google-challenge-part-1/#test-cases_1","title":"Test cases","text":"<p>Your code should pass the following test cases. Note that it may also be run against hidden test cases not shown here.</p> <p>Test 01: <pre><code># Input\nsolution.solution(19, 36)\n\n# Output\n1\n</code></pre></p> <p>Test 02: <pre><code># Input\nsolution.solution(0, 1)\n\n# Output\n3\n</code></pre></p>"},{"location":"notebooks/coding/google-challenge-part-1/#resolution_2","title":"Resolution","text":"<p>The third challenge was a great challenge. I enjoy solving this one. Given a chessboard, you need to write a function that calculates the smallest number of moves from one square to another square as a  chess knight.</p> <p>I'm sure you can apply more rules and optimize this solution. Although, in my case, I solved by extracting all the possible moves from the initial position and recursively calculating all possible moves from these new positions, until I reached the destination square.</p> <p>[caption width=\"450\" align=\"aligncenter\"] Google challenge 3[/caption]</p> <p>So to do that I start with a structure like this: <pre><code>next_moves = [src]\nmoves_counter = 0\n# Only stops is destination is reached in next moves\nwhile dest not in next_moves:\n    all_moves = []\n    # For each position in next_moves it calculate all possible next positions\n    for position in next_moves:\n        moves = possible_moves(position)    # A function to calculate all possible moves\n        all_moves += moves  # Add to all moves the next moves\n    next_moves = all_moves  # Next moves is reset with the accumulated new moves\n    moves_counter += 1\n</code></pre></p> <p>Instead of creating functions inside the main function, I tried to develop the code along with the previous struture as short and clear as possible. <pre><code>def solution(src, dest):\n    possible_moves = [(2, 1), (1, 2), (-1, 2), (2, -1), (-2, 1), (1, -2), (-1, -2), (-2, -1)]\n    lines = [list(range(8*i, 8*i+8)) for i in range(0, 8)]\n    next_moves = [src]\n    moves_counter = 0\n    while dest not in next_moves:\n        all_moves = []\n        for position in next_moves:\n            # Possible moves\n            idx = int(position/8), (position % 8)\n            solutions = [(idx[0] + move[0], idx[1] + move [1]) for move in possible_moves\\\n                         if 0 &lt;= idx[0] + move[0] &lt; 8 and 0 &lt;= idx[1]+ move[1] &lt; 8]\n            moves = [lines[coor[0]][coor[1]] for coor in solutions]\n            all_moves += moves\n        next_moves = all_moves\n        moves_counter += 1\n    return moves_counter\n</code></pre> <pre><code>solution(0, 7)\n</code></pre> 5</p>"},{"location":"notebooks/coding/google-challenge-part-1/#references-and-links","title":"References and links","text":"<ul> <li>Download the Jupyter notebook from: github.com/charlstown/MyGoogleFooBar</li> <li>Google Foobar official</li> <li>More Notebooks like this here</li> </ul>"},{"location":"notebooks/coding/python-design-patterns/","title":"Python design patterns","text":"<ul> <li>Margaret Hamilton in 1969</li> </ul> <p>I wanted to share a post with some notes from the Jungwoo Ryoo Python Design Patterns course. Dr. Jungwoo Ryoo educates people in the field of Software Security/cybersecurity, Computer networking, Data Science and others.</p>"},{"location":"notebooks/coding/python-design-patterns/#1-creational-design-patterns","title":"1. Creational Design Patterns","text":"<p>Creational patterns provides essential information regarding the Class instantiation or the object instantiation. Class Creational Pattern and the Object Creational pattern is the major categorization of the Creational Design Patterns. While class-creation patterns use inheritance effectively in the instantiation process, object-creation patterns use delegation effectively to get the job done.</p>"},{"location":"notebooks/coding/python-design-patterns/#factory-class-method","title":"Factory class Method","text":"<p>To create an object with a instanciated factory on the run with out knowing how, why or what parameters are you going to pass</p> <p>Allows an interface or a class to create an object, but lets subclasses decide which class or object to instantiate. Using the Factory method, we have the best ways to create an object. Here, objects are created without exposing the logic to the client, and for creating the new type of object, the client uses the same common interface.</p> <pre><code>class SocketH:\n    \"\"\"Class example object connection HTTP\"\"\"\n    def __init__(self, name):\n        self.name = name\n\n    def action(self):\n        return f\"Connection: {self.name}\"\n\nclass SocketF:\n    \"\"\"Class example object connection FTP\"\"\"\n    def __init__(self, name):\n        self.name = name\n\n    def action(self):\n        return f\"Connection: {self.name}\"\n\n\ndef get_connection(connection=\"http\"):\n    \"\"\"The factory method\"\"\"\n    connections = {'http': SocketH(\"http_connection\"),\n                   'ftp': SocketF('ftp_connection')}\n\n    return connections[connection]\n\n\n# Running factory function\nhttp = get_connection('http')\nprint(f\"object -&gt; {http.action()}\")\n\nftp = get_connection('FTP')\nprint(f\"Object -&gt; {ftp.action()}\")\n</code></pre> Output<pre><code>object -&gt; Connection: http_connection\nObject -&gt; Connection: ftp_connection\n</code></pre>"},{"location":"notebooks/coding/python-design-patterns/#abstract-factory-method","title":"Abstract Factory Method","text":"<p>Allows you to produce the families of related objects without specifying their concrete classes. Using the abstract factory method, we have the easiest ways to produce a similar type of many objects.</p> <p>It provides a way to encapsulate a group of individual factories. When the user expectation yields multiple, related objects at a given time but don't need to know which family it is until runtime.</p> <pre><code>class SocketH:\n    \"\"\"One of the objects to be returned\"\"\"\n\n    def action(self):\n        return f\"Connection: http\"\n\n    def __str__(self):\n        return \"Socket_h\"\n\n\nclass SocketHFactory:\n    \"\"\"Concrete factory\"\"\"\n\n    def get_connection(self):\n        \"\"\"Returns a Connection object\"\"\"\n        return SocketH()\n\n    def get_request(self):\n        \"\"\"Returns a Get Request object\"\"\"\n        return \"get request\"\n\nclass Connections:\n    \"\"\"Connections class houses our abstract factory\"\"\"\n\n    def __init__(self, connection_factory = None):\n        \"\"\"Connections class is our abstract factory\"\"\"\n\n        self._connection_factory = connection_factory\n\n\n    def show_connection(self):\n        \"\"\"Utility method to display details of the objects returned\n        by the Connection factory\"\"\"\n\n        connection = self._connection_factory.get_connection()\n        request = self._connection_factory.get_request()\n\n        print(f\"Connection: {connection}\")\n        print(f\"Action {connection.action()}\")\n        print(f\"Request: {request}\")\n\n# Create a Concrete Factory\nfactory_a = SocketHFactory()\nfactory_b = SocketHFactory()\n\n# Create a connections, our Abstract Factory\nconnect = Connections(factory_a)\n\n# Invoke the utility method to show the details\nconnect.show_connection()\n</code></pre> Output<pre><code>Connection: Socket_h\nAction Connection: http\nRequest: get request\n</code></pre>"},{"location":"notebooks/coding/python-design-patterns/#singleton-method","title":"Singleton Method","text":"<p>[caption id=\"attachment_2062\" align=\"aligncenter\" width=\"267\"] Singleton Pattern[/caption]</p> <p>Allows to keep information in a single object there is no need to extract the information every time. When you want to allow only one object to be instantiated from a class.</p> <p>Singleton Design Pattern can be understood by a very simple example of Database connectivity. When each object creates a unique Database Connection to the Database, it will highly affect the cost and expenses of the project. So, it is always better to make a single connection rather than making extra irrelevant connections which can be easily done by Singleton Design Pattern.</p> <pre><code>class Borg:\n    \"\"\"Borg class attributes global\"\"\"\n    _shared_state = {} # Attribute dictionary\n\n    def __init__(self):\n        self.__dict__ = self._shared_state\n\nclass Singleton(Borg): # Inherits from the Borg class\n    \"\"\"This class now shares all its attributes among its various instances\"\"\"\n    # This essentially makes the singleton objects an object-oriented\n    def __init__(self, **kwargs):\n        # Update the attribute dictionary by insrting a new key-value pair\n        self._shared_state.update(kwargs)        \n\n    def __str__(self):\n        return str(self._shared_state)\n\n# Create a singleton object and add our fiest acronym\nsingleton = Singleton(HTTP = \"Hyper Text Transfer Protocol\")\n\n# Print the updated object\nprint(f\"First print: {singleton}\")\n\n# Create another singleton object by adding another acronym\nsingleton = Singleton(SNMP = \"Simple Network Management Protocol\")\n\n# Print the updated object\nprint(f\"Second print: {singleton}\")\n```\n\n```bash title=\"Output\"\nFirst print: {'HTTP': 'Hyper Text Transfer Protocol'}\nSecond print: {'HTTP': 'Hyper Text Transfer Protocol', 'SNMP': 'Simple Network Management Protocol'}\n```\n\n\n### Builder Method\n[caption id=\"attachment_2067\" align=\"aligncenter\" width=\"571\"]&lt;a href=\"https://carlosgrande.me/wp-content/uploads/2021/12/BuilderPattern.jpg\"&gt;&lt;img src=\"https://carlosgrande.me/wp-content/uploads/2021/12/BuilderPattern.jpg\" alt=\"Builder Pattern\" width=\"571\" height=\"174\" class=\"size-full wp-image-2067\" /&gt;&lt;/a&gt; Builder Pattern[/caption]\n\nBuilder Method is a Creation Design Pattern which aims to \u201cSeparate the construction of a complex object from its representation so that the same construction process can create different representations.\u201d It allows you to construct complex objects step by step. Here using the same construction code, we can produce different types and representations of the object easily.\n\nTo avoid telescoping constructor anti-pattern.\n- Director: builds the project\n- Abstract Builder: interfaces\n- Concrete Builder: implements the interfaces\n- Product: object being build\n\n```python\nclass Director:\n    \"\"\"Director class\"\"\"\n    def __init__(self, builder):\n        self._builder = builder\n\n    def construct_obj(self):\n        self._builder.create_new_obj()\n        self._builder.add_user()\n        self._builder.add_connection()\n        self._builder.add_name()\n\n    def get_obj(self):\n        return self._builder.obj\n\nclass Builder:\n    \"\"\"Abstract Builder\"\"\"\n    def __init__(self):\n        self.obj = None\n\n    def create_new_obj(self):\n        self.obj = Obj()\n\nclass ConcreteBuilder(Builder):\n    \"\"\"Concrete Builder --&gt; Provides parts and tools to work on the parts\"\"\"\n\n    def add_user(self):\n        self.obj.user = \"skwh\"\n\n    def add_connection(self):\n        self.obj.connection = \"HTTP\"\n\n    def add_name(self):\n        self.obj.name = \"Skyler White\"\n\nclass Obj:\n    \"\"\"Product\"\"\"\n    def __init__(self):\n        self.user = None\n        self.connection = None\n        self.name = None\n\n    def __str__(self):\n        return f\"{self.user} | {self.connection} | {self.name}\"\n\nbuilder = ConcreteBuilder()\ndirector = Director(builder)\ndirector.construct_obj()\nobj = director.get_obj()\n\nprint(obj)\n</code></pre> Output<pre><code>skwh | HTTP | Skyler White\n</code></pre>"},{"location":"notebooks/coding/python-design-patterns/#prototype-method","title":"Prototype Method","text":"<p>[caption id=\"attachment_2069\" align=\"aligncenter\" width=\"429\"] Prototype Pattern[/caption]</p> <p>Prototype Method is a Creational Design Pattern which aims to reduce the number of classes used for an application. It allows you to copy existing objects independent of the concrete implementation of their classes. Generally, here the object is created by copying a prototypical instance during run-time.</p> <p>It is highly recommended to use Prototype Method when the object creation is an expensive task in terms of time and usage of resources and already there exists a similar object. This method provides a way to copy the original object and then modify it according to our needs.</p> <pre><code>import copy\n\nclass Prototype:\n\n    def __init__(self):\n        self._objects = {}\n\n    def register_object(self, name, obj):\n        \"\"\"Register an object\"\"\"\n        self._objects[name] = obj\n\n    def unregister_object(self, name):\n        \"\"\"Unregister an object\"\"\"\n        del self._objects[name]\n\n    def clone(self, name, **attr):\n        \"\"\"Clone a registered object\"\"\"\n        obj = copy.deepcopy(self._objects[name])\n        obj.__dict__.update(attr)\n        return obj\n\nclass Api:\n    def __init__(self):\n        self.name = \"api_name\"\n        self.ip = \"192.168.23.85\"\n        self.option = True\n\n    def __str__(self):\n        return f\"api -&gt; {self.name} | {self.ip} | {self.option}\"\n\napi = Api()\nprototype = Prototype()\nprototype.register_object(\"api_name\", api)\n\napi_cloned = prototype.clone(\"api_name\", ip = \"232.0.0.1\")\nprint(api_cloned)\n</code></pre> Output<pre><code>api -&gt; api_name | 232.0.0.1 | True\n</code></pre>"},{"location":"notebooks/coding/python-design-patterns/#2-structural-design-patterns","title":"2. Structural Design Patterns","text":"<p>Structural design patterns are about organizing different classes and objects to form larger structures and provide new functionality while keeping these structures flexible and efficient. Mostly they use Inheritance to compose all the interfaces. It also identifies the relationships which led to the simplification of the structure.</p>"},{"location":"notebooks/coding/python-design-patterns/#decorator-method","title":"Decorator Method","text":"<p>Decorator Method is a Structural Design Pattern which allows you to dynamically attach new behaviors to objects without changing their implementation by placing these objects inside the wrapper objects that contains the behaviors.</p> <p>It is much easier to implement Decorator Method in Python because of its built-in feature. It is not equivalent to the Inheritance because the new feature is added only to that particular object, not to the entire subclass.</p> <pre><code>from functools import wraps\n\ndef make_blink(function):\n    \"\"\"Defines the decorator\"\"\"\n\n    # This makes the decoratos transparent in terms of its name and docstring\n    @wraps(function)\n\n    def decorator():\n        # Grab the return value of the function being decorated\n        ret = function()\n\n        # Add new funcionality to the function being decorated\n        return f\"&lt;blink&gt; {ret} &lt;/blink&gt;\"\n\n    return decorator\n\n# Apply the decorator here!\n@make_blink\ndef hello_world():\n    \"\"\"Original function!\"\"\"\n    return \"Hello, World!\"\n# Check the result of decorating\nprint(hello_world())\n\n# Check if the function name is still the same name of the function being decorated.\nprint(hello_world.__name__)\n\n# Check if the docstring is still the same as that of the function being decorated.\nprint(hello_world.__doc__)\n</code></pre> Output<pre><code>&amp;lt;blink&amp;gt; Hello, World! &amp;lt;/blink&amp;gt;\nhello_world\nOriginal function!\n</code></pre>"},{"location":"notebooks/coding/python-design-patterns/#proxy-method","title":"Proxy Method","text":"<p>The Proxy method is Structural design pattern that allows you to provide the replacement for an another object. Here, we use different classes to represent the functionalities of another class. The most important part is that here we create an object having original object functionality to provide to the outer world.</p> <p>The meaning of word Proxy is \u201cin place of\u201d or \u201con behalf of\u201d that directly explains the Proxy Method. Proxies are also called surrogates, handles, and wrappers. They are closely related in structure, but not purpose, to Adapters and Decorators.</p> <pre><code>import time\n\nclass Producer:\n    \"\"\"Define the 'resource-intensive' object to instantiate\"\"\"\n    def produce(self):\n        print(\"Producer is working hard!\")\n\n    def meet(self):\n        print(\"Producer has time to meet you now!\")\n\nclass Proxy:\n    \"\"\"Define the relatively 'less resource-intensive proxy' to instatiate as a middleman\"\"\"\n    def __init__(self):\n        self.occupied = False\n        self.producer = None # Instance of the producer class\n\n    def produce(self):\n        \"\"\"Check if Producer is available\"\"\"\n        print(\"Artist checking if Producer is available...\")\n\n        if self.occupied == False:\n            # If the Producer is available, create a producer object\n            self.producer = Producer()\n            time.sleep(2)\n\n            # Make the producer meet the guest\n            self.producer.meet()\n\n        else:\n            # Otherwise, don't instantiate a producer\n            time.sleep(2)\n            print(\"Producer is busy!\")\n\n# Instantiate a Proxy\nproxy = Proxy()\n\n# Make the Proxy: Artist produce until Producer is available\nproxy.produce()\n\n# Change the state to 'Occupied'\nproxy.occupied = True\n\n# Make the Producer produce\nproxy.produce()\n</code></pre> Output<pre><code>Artist checking if Producer is available...\nProducer has time to meet you now!\nArtist checking if Producer is available...\nProducer is busy!\n</code></pre>"},{"location":"notebooks/coding/python-design-patterns/#adapter-method","title":"Adapter Method","text":"<p>Adapter method is a Structural Design Pattern which helps us in making the incompatible objects adaptable to each other. The Adapter method is one of the easiest methods to understand because we have a lot of real-life examples that show the analogy with it. The main purpose of this method is to create a bridge between two incompatible interfaces. This method provides a different interface for a class. We can more easily understand the concept by thinking about the Cable Adapter that allows us to charge a phone somewhere that has outlets in different shapes. Using this idea, we can integrate the classes that couldn\u2019t be integrated due to interface incompatibility.`</p> <pre><code>class Spanish:\n    \"\"\"Korean speaker\"\"\"\n    def __init__(self):\n        self.name = \"Spanish\"\n\n    def speak_spanish(self):\n        return \"Hola!\"\n\nclass British:\n    \"\"\"British speaker\"\"\"\n    def __init__(self):\n        self.name = \"British\"\n\n    # Make the difference method name here!\n    def speak_english(self):\n        return \"Hello!\"\n\nclass Adapter:\n    \"\"\"This changes the generic method name to individualized method names\"\"\"\n    def __init__(self, object, **adapter_method):\n        \"\"\"Change the name of the method\"\"\"\n        self._object = object\n\n        # add a new dictionary item that establishes the mapping between the generic method name: speak() and the concrete method\n        # for example, speak() will be translated to speak_korean() if the mapping says so\n        self.__dict__.update(adapter_method)\n\n    def __getattr__(self, attr):\n        \"\"\"Simply return the rest of the attributes!\"\"\"\n        return getattr(self._object, attr)\n\n\n# List to store speaker objects\nobjects = []\n\n# Create Spanish object\nspanish = Spanish()\n\n# Create British object\nbritish = British()\n\n# Append the objects to the objects list\nobjects.append(Adapter(spanish, speak = spanish.speak_spanish))\nobjects.append(Adapter(british, speak=british.speak_english))\n\nfor obj in objects:\n    print(f\"{obj.name} says {obj.speak()}\\n\")\n</code></pre> Output<pre><code>Spanish says Hola!\n\nBritish says Hello!\n</code></pre>"},{"location":"notebooks/coding/python-design-patterns/#composite-method","title":"Composite Method","text":"<p>Composite Method is a Structural Design Pattern which describes a group of objects that is treated the same way as a single instance of the same type of the objects. The purpose of the Composite Method is to Compose objects into Tree type structures to represent the whole-partial hierarchies.</p> <p>Composite Method is a Structural Design Pattern which describes a group of objects that is treated the same way as a single instance of the same type of the objects. The purpose of the Composite Method is to Compose objects into Tree type structures to represent the whole-partial hierarchies.</p> <pre><code>class Component(object):\n    \"\"\"Abstract class\"\"\"\n\n    def __init__(self, *args, **kwargs):\n        pass\n\n    def component_function(self):\n        pass\n\nclass Child(Component): # Inherits from the abstract class Component\n    \"\"\"Concrete class\"\"\"\n\n    def __init__(self, *args, **kwargs):\n        Component.__init__(self, *args, **kwargs)\n\n        # This is where we store the name of your child item!\n        self.name = args[0]\n\n    def component_function(self, tabs):\n        \"\"\"Print the name of your child item here\"\"\"\n        tab_spaces = \"\\t\"*tabs\n        print(f\"{tab_spaces}{self.name}\")\n\nclass Composite(Component): # Inherits from the abstract class Component\n    \"\"\"Concrete class and mantains the tree recursive structure\"\"\"\n\n    def __init__(self, *args, **kwargs):\n        Component.__init__(self, *args, **kwargs)\n\n        # This is where we store the name of composite object!\n        self.name = args[0]\n\n        # This is where we keep ur child items\n        self.children = []\n\n    def append_child(self, child):\n        \"\"\"Method to add a new child item\"\"\"\n        self.children.append(child)\n\n    def remove_child(self, child):\n        \"\"\"Method to remove a new child item\"\"\"\n        self.children.remove(child)\n\n    def component_function(self, tabs = 0):\n\n        # Print the name of the composite object\n        tab_spaces = \"\\t\"*tabs\n        print(f\"{tab_spaces}{self.name}\")\n        tabs += 1\n\n        # Iterate through the child objects and invoke theircomponent function printing their names\n        for i in self.children:\n            i.component_function(tabs = tabs)\n\n\n# Build a composite submenu 1\nsub1 = Composite(\"submenu1\")\n\n# Create a new child sub_menu 11\nsub11 = Child(\"sub_menu 11\")\n# Create a new child sub_menu 12\nsub12 = Child(\"sub_menu 12\")\n\n# Add the sub_menu 11 to submenu 1\nsub1.append_child(sub11)\n# Add the sub_menu 12 to submenu 1\nsub1.append_child(sub12)\n\n# Build a top-level composite menu\ntop = Composite(\"top_menu\")\n\n# Build a submenu 2 that is not a composite\nsub2 = Composite(\"submenu2\")\nsub2.append_child(sub11)\n\n# Add the composite submenu 1 to the top-level composite menu\ntop.append_child(sub1)\n\n# Add the plain submenu 2 to the top-level composite menu\ntop.append_child(sub2)\n\n# Let's test if our composite pattern works!\ntop.component_function()\n</code></pre> Output<pre><code>top_menu\n&amp;emsp;submenu1\n&amp;emsp;&amp;emsp;sub_menu 11\n&amp;emsp;&amp;emsp;sub_menu 12\n&amp;emsp;submenu2\n&amp;emsp;&amp;emsp;sub_menu 11\n</code></pre>"},{"location":"notebooks/coding/python-design-patterns/#bridge-method","title":"Bridge Method","text":"<p>Bridge method is a Structural Design Pattern which allows us to separate the Implementation Specific Abstractions and Implementation Independent Abstractions from each other and can be developed considering as the single entities. Bridge Method is always considered as one of the best methods to organize the class hierarchy.</p> <p>Elements of Bridge Design Pattern: - Abstraction: It is the core of the Bridge Design Pattern and it provides the reference to the implementer. - Refined Abstraction: It extends the abstraction to the new level where it takes the finer details one level above and hides the finer element from the implementors. - Implementer: It defines the interface for implementation classes. This interface does not need to correspond directly to the abstraction interface and can be very different. - Concrete Implementation: Through the concrete implementation, it implements the above implementer.</p> <pre><code>class DrawingAPIOne(object):\n    \"\"\"Implementation-specific abstraction: concrete class one\"\"\"\n    def draw_circle(self, x, y, radius):\n        print(f\"API 1 drawing a circle at ({x}, {y} with radius {radius}!)\")\n\nclass DrawingAPITwo(object):\n    \"\"\"Implementation-specific abstraction: concrete class two\"\"\"\n    def draw_circle(self, x, y, radius):\n        print(f\"API 2 drawing a circle at ({x}, {y} with radius {radius}!)\")\n\nclass Circle(object):\n    \"\"\"Implementation-independent abstraction: for example, there could be a rectangle class\"\"\"\n    def __init__(self, x, y, radius, drawing_api):\n        \"\"\"Initialize the necessary attributes\"\"\"\n        self._x = x\n        self._y = y\n        self._radius = radius\n        self._drawing_api = drawing_api\n\n    def draw(self):\n        \"\"\"Implementation-specific abstraction taken care of by another class: DrawingAPI\"\"\"\n        self._drawing_api.draw_circle(self._x, self._y, self._radius)\n\n    def scale(self, percent):\n        \"\"\"Implementation independent\"\"\"\n        self._radius *= radius\n\n\n# Build the first circle object using API One\ncircle1 = Circle(1, 2, 3, DrawingAPIOne())\n\n# Build the \ncircle1.draw()\n\n# Build the second circle object using API One\ncircle2 = Circle(2, 3, 4, DrawingAPITwo())\n\ncircle2.draw()\n</code></pre> Output<pre><code>API 1 drawing a circle at (1, 2 with radius 3!)\nAPI 2 drawing a circle at (2, 3 with radius 4!)\n</code></pre>"},{"location":"notebooks/coding/python-design-patterns/#3-behavioral-design-patterns","title":"3. Behavioral Design Patterns","text":"<p>Behavioral patterns are all about identifying the common communication patterns between objects and realize these patterns. These patterns are concerned with algorithms and the assignment of responsibilities between objects.</p>"},{"location":"notebooks/coding/python-design-patterns/#observer-method","title":"Observer Method","text":"<p>The observer method is a Behavioral design Pattern which allows you to define or create a subscription mechanism to send the notification to the multiple objects about any new event that happens to the object that they are observing. The subject is basically observed by multiple objects. The subject needs to be monitored and whenever there is a change in the subject, the observers are being notified about the change. This pattern defines one to Many dependencies between objects so that one object changes state, all of its dependents are notified and updated automatically.</p> <p>One-two many relationships between a subject and multiple observers. Example: Core temperatures monitored by observers.</p> <ul> <li>Subject: abstract class (Attach Detach Notify)</li> <li>Concrete subjects</li> </ul> <pre><code>class Subject(object):  # Represents what is being 'observed'\n    \"\"\"This is where references to all the observers\n    are being kept. Note that this is a one-to-\n    many relationship: there will be one subject\n    to be observed by multiple observers.\"\"\"\n    def __init__(self):\n        self._observers = []\n\n    def attach(self, observer):\n        \"\"\"If the observer is not already in the observers list\n        append the observer to the list\"\"\"\n        if observer not in self._observers:\n            self._observers.append(observer)     \n\n    def detach(self, modifier=None):\n        \"\"\"Simply remove the observer\"\"\"\n        try:\n            self._observers.remove(observer)\n        except ValueError:\n            pass\n\n    def notify(self, modifier = None):\n        \"\"\"For all the observers in the list. Don't notify the observer\n        who is actually updating the temperature of the core\"\"\"\n        for observer in self._observers:\n            if modifier != observer:\n                observer.update(self)\nclass Core(Subject): # Inherits from the Subject class\n    def __init__(self, name = \"\"):\n        Subject.__init__(self)\n        self._name = name # Set the name of the core\n        self._temp = 0 # Initiaize the temperature of the core\n\n    @property # Getter that gets the core temperature\n    def temp(self):\n        return self._temp\n\n    @temp.setter # Setter that sets the core temperature\n    def temp(self, temp):\n        \"\"\"Notify the observers whenever somebody changes the core temperature\"\"\"\n        self._temp = temp\n        self.notify()\nclass TempViewer:\n    \"\"\"A printer for the temperature\"\"\"\n    def update(self, subject):\n        \"\"\"Alert method that is invoked when the notify()\n        method in a concrete subject is invoked\"\"\"\n        print(f\"Temperature Viewer: {subject._name} has temperature {subject._temp}\")\n\n# CODE\n# Let's create our subjects\nc1 = Core(\"Core 1\")\nc2 = Core(\"Core 2\")\n\n# Let's create our observers\nv1 = TempViewer()\nv2 = TempViewer()\n\n# Let's attach our observers to the first core\nc1.attach(v1)\nc1.attach(v2)\n\n# Let's change the temperature of our first core\nc1.temp = 80\nc1.temp = 90\n</code></pre> Output<pre><code>Temperature Viewer: Core 1 has temperature 80\nTemperature Viewer: Core 1 has temperature 80\nTemperature Viewer: Core 1 has temperature 90\nTemperature Viewer: Core 1 has temperature 90\n</code></pre>"},{"location":"notebooks/coding/python-design-patterns/#visitor-method","title":"Visitor Method","text":"<p>Visitor Method is a Behavioral Design Pattern which allows us to separate the algorithm from an object structure on which it operates. It helps us to add new features to an existing class hierarchy dynamically without changing it. All the behavioral patterns proved as the best methods to handle the communication between the objects. Similarly, it is used when we have to perform an operation on a group of similar kinds of objects.</p> <pre><code>class House(object):\n    \"\"\"The class being visited\"\"\"\n    def accept(self, visitor):\n        \"\"\"Interface to accept a visitor, triggers the visiting operation!\"\"\"\n        visitor.visit(self)\n\n    def work_on_hvac(self, hvac_specialist):\n        \"\"\"Reference to the hvac specialist object in the house object\"\"\"\n        print(f\"{self} worked on by {hvac_specialist}\")\n\n    def work_on_electricity(self, electrician):\n        \"\"\"Reference to the electrician object in the house object\"\"\"\n        print(f\"{self} worked on by {electrician}\")\n\n    def __str__(self):\n        \"\"\"Simply return the class name when the House object is printed\"\"\"\n        return self.__class__.__name__\n\nclass Visitor(object):\n    \"\"\"Abstract visitor\"\"\"\n    def __str__(self):\n        \"\"\"Simply return the class name when the visitor object is printed\"\"\"\n        return self.__class__.__name__\n\nclass HvacSpecialist(Visitor):\n    \"\"\"Concrete visitor: electrician\n    Inherits from the parent class, Visitor\"\"\"\n    def visit(self, house):\n        house.work_on_hvac(self)\n\nclass Electrician(Visitor):\n    \"\"\"Concrete visitor: electrician\n    Inherits from the parent class, Visitor\"\"\"\n    def visit(self, house):\n        house.work_on_electricity(self)\n\n\n# CODE\n# Create an HVAC especialist\nhvac_specialist = HvacSpecialist()\n\n# Create an electritian\nelectrician = Electrician()\n\n# Create a House\nhouse = House()\n\n# Let the house accept the HVAC specialist and work on the house\n# by invokation the visit() method\nhouse.accept(hvac_specialist)\n\n# Let the house accept the HVAC specialist and work on the house\n# by invokation the visit() method\nhouse.accept(electrician)\n</code></pre> Output<pre><code>House worked on by HvacSpecialist\nHouse worked on by Electrician\n</code></pre>"},{"location":"notebooks/coding/python-design-patterns/#iterator-method","title":"Iterator Method","text":"<p>Iterator method is a Behavioral Design Pattern that allows us to traverse the elements of the collections without taking the exposure of in-depth details of the elements. It provides a way to access the elements of complex data structure sequentially without repeating them.</p> <p>According to GangOfFour, Iterator Pattern is used \u201d to access the elements of an aggregate object sequentially without exposing its underlying implementation\u201d.`</p> <pre><code>def count_to(count):\n    \"\"\"Our iterator implementation\"\"\"\n\n    # Our list\n    numbers_in_spanish = ['uno', 'dos', 'tres', 'cuatro', 'cinco',\n                         'seis', 'siete', 'ocho', 'nueve', 'diez']\n\n    # Our build in iterator\n    # Creates a tuple such as (1, \"uno\")\n    iterator = zip(range(count), numbers_in_spanish)\n\n    # Iterate thourgh our iterable list\n    # Extract the Spanish numbers\n    # put them in a generator called number\n    for position, number in iterator:\n        # Returns a 'generator' containing numbers in Spanish\n        yield number\n\n# CODE\n# Lest's test the generator returned by our iterator\nfor i in count_to(8):\n    print(f\"The number is: {i}.\")\n</code></pre> Output<pre><code>The number is: uno.\nThe number is: dos.\nThe number is: tres.\nThe number is: cuatro.\nThe number is: cinco.\nThe number is: seis.\nThe number is: siete.\nThe number is: ocho.\n</code></pre>"},{"location":"notebooks/coding/python-design-patterns/#strategy-method","title":"Strategy Method","text":"<p>The strategy method is Behavioral Design pattern that allows you to define the complete family of algorithms, encapsulates each one and putting each of them into separate classes and also allows to interchange there objects. It is implemented in Python by dynamically replacing the content of a method defined inside a class with the contents of functions defined outside of the class. It enables selecting the algorithm at run-time. This method is also called as Policy Method.</p> <pre><code>import types # Types module\n\nclass Strategy:\n    \"\"\"The Strategy Pattern class\"\"\"\n    def __init__(self, function = None):\n        self.name = \"Default Strategy\"\n\n        # If a reference to a function is provided, replace the execute() method with the given function.\n        if function:\n            self.execute = types.MethodType(function, self)\n\n    def execute(self):\n        \"\"\"The default method that prints the name of the strategy being used\"\"\"\n        print(f\"{self.name} is used!\")\n\n\n# Replacement method 1\ndef strategy_one(self):\n    print(f\"{self.name} is used to execute method 1\")\n\ndef strategy_two(self):\n    print(f\"{self.name} is used to execute method 2\")\n\n# CODE\n# Let's create our default strategy\ns0 = Strategy()\n# Let's execute our default strategy\ns0.execute()\n\n# Let's create our first variation of our default strategy by providing a new behavior\ns1 = Strategy(strategy_one)\n# Let's set its name\ns1.name = \"Strategy_one\"\n# Let's execute the strategy\ns1.execute()\n\n# Let's create our first variation of our default strategy by providing a new behavior\ns2 = Strategy(strategy_two)\n# Let's set its name\ns2.name = \"Strategy_two\"\n# Let's execute the strategy\ns2.execute()  \n</code></pre> Output<pre><code>Default Strategy is used!\nStrategy_one is used to execute method 1\nStrategy_two is used to execute method 2\n</code></pre>"},{"location":"notebooks/coding/python-design-patterns/#chain-of-responsability-method","title":"Chain of Responsability Method","text":"<p>Chain of Responsibility method is Behavioral design pattern and it is the object-oriented version of if \u2026 elif \u2026 elif \u2026 else and make us capable to rearrange the condition-action blocks dynamically at the run-time. It allows us to pass the requests along the chain of handlers. The processing is simple, whenever any handler received the request it has two choices either to process it or pass it to the next handler in the chain.  This pattern aims to decouple the senders of a request from its receivers by allowing the request to move through chained receivers until it is handled. </p> <pre><code>class Handler:\n    \"\"\"Abstract Handler\"\"\"\n    def __init__(self, succesor):\n        \"\"\"Define who is the next handler\"\"\"\n        self._succesor = succesor\n\n    def handle(self, request):\n        handled = self._handle(request)\n\n        if not handled:\n            self._succesor.handle(request)\n\n    def _handle(self, request):\n        raise NotImplementedError(\"Must provide implementation in subclass!\")\n\nclass ConcreteHandler1(Handler):\n    \"\"\"Concrete handler 1\"\"\"\n    def _handle(self, request):\n        if 0 &lt; request &lt;= 10: # Provide a condition for handling\n            print(f\"Request {request} handled in handeler 1\")\n            return True # Indicates the request have been handled\n\nclass DefaultHandler(Handler):\n    \"\"\"Default handler\"\"\"\n    def _handle(self, request):\n        \"\"\"If ther is no handler available. No condition checking since\n        this is a default handler\"\"\"\n        print(f\"End of chain, no handler for {request}\")\n        return True # Indicates that the request has been handled\n\nclass Client:\n    \"\"\"Using handlers\"\"\"\n    def __init__(self):\n        \"\"\"Create handlers and use them in a sequence you want\"\"\"\n        self.handler = ConcreteHandler1(DefaultHandler(None))\n\n    def delegate(self, requests):\n        \"\"\"Send your requestas one at a time for handlers to handle\"\"\"\n        for request in requests:\n            self.handler.handle(request)\n\n# CODE\n# Create a client\nclient = Client()\n\n# Create requests\nrequests = [2, 5, 30]\n\n# Send requests\nclient.delegate(requests)\n</code></pre> Output<pre><code>Request 2 handled in handeler 1\nRequest 5 handled in handeler 1\nEnd of chain, no handler for 30\n</code></pre>"},{"location":"notebooks/coding/python-design-patterns/#command-method","title":"Command Method","text":"<p>Command Method is Behavioral Design Pattern that encapsulates a request as an object, thereby allowing for the parameterization of clients with different requests and the queuing or logging of requests. Parameterizing other objects with different requests in our analogy means that the button used to turn on the lights can later be used to turn on stereo or maybe open the garage door. It helps in promoting the \u201cinvocation of a method on an object\u201d to full object status. Basically, it encapsulates all the information needed to perform an action or trigger an event.</p> <pre><code>#Use built-in abc to implement Abstract classes and methods\nfrom abc import ABC, abstractmethod\n\nclass Command(ABC):\n\"\"\"Class Dedicated to Command\"\"\"\n\n    def __init__(self, receiver):\n    \"\"\"constructor method\"\"\"\n        self.receiver = receiver\n\n\n    def process(self):\n    \"\"\"process method\"\"\"\n        pass\n\nclass CommandImplementation(Command):\n\"\"\"Class dedicated to Command Implementation\"\"\"\n\n\n    def __init__(self, receiver):\n    \"\"\"constructor method\"\"\"\n        self.receiver = receiver\n\n\n    def process(self):\n    \"\"\"process method\"\"\"\n        self.receiver.perform_action()\n\n\nclass Receiver:\n\"\"\"Class dedicated to Receiver\"\"\"      \n    def perform_action(self):\n    \"\"\"perform-action method\"\"\"\n        print('Action performed in receiver.')\n\nclass Invoker:\n\"\"\"Class dedicated to Invoker\"\"\"\n\n    def command(self, cmd):\n    \"\"\"command method\"\"\"\n        self.cmd = cmd\n\n    def execute(self):\n    \"\"\"execute method\"\"\"\n        self.cmd.process()\n\n\nif __name__ == \"__main__\":      \n    # create Receiver object\n    receiver = Receiver()\n    cmd = CommandImplementation(receiver)\n    invoker = Invoker()\n    invoker.command(cmd)\n    invoker.execute()\n</code></pre> Output<pre><code>Action performed in receiver.\n</code></pre>"},{"location":"notebooks/coding/python-design-patterns/#mediator-method","title":"Mediator Method","text":"<p>Mediator Method is a Behavioral Design Pattern that allows us to reduce the unordered dependencies between the objects. In a mediator environment, objects take the help of mediator objects to communicate with each other. It reduces coupling by reducing the dependencies between communicating objects. The mediator works as a router between objects and it can have it\u2019s own logic to provide a way of communication.</p> <p>Design Components: - Mediator: It defines the interface for communication between colleague objects. - Concrete Mediator: It implements the mediator interface and coordinates communication between colleague objects. - Colleague: It defines the interface for communication with other colleagues - Concrete Colleague: It implements the colleague interface and communicates with other colleagues through its mediator.</p> <pre><code>class Course(object):\n    \"\"\"Mediator class.\"\"\"\n\n    def displayCourse(self, user, course_name):\n        print(\"[{}'s course ]: {}\".format(user, course_name))\n\n\nclass User(object):\n    '''A class whose instances want to interact with each other.'''\n\n    def __init__(self, name):\n        self.name = name\n        self.course = Course()\n\n    def sendCourse(self, course_name):\n        self.course.displayCourse(self, course_name)\n\n    def __str__(self):\n        return self.name\n\n# main method\nif __name__ == \"__main__\":\n\n    mayank = User('Mayank')   # user object\n    lakshya = User('Lakshya') # user object\n    krishna = User('Krishna') # user object\n\n    mayank.sendCourse(\"Data Structures and Algorithms\")\n    lakshya.sendCourse(\"Software Development Engineer\")\n    krishna.sendCourse(\"Standard Template Library\")\n</code></pre>"},{"location":"notebooks/coding/python-design-patterns/#memento-method","title":"Memento Method","text":"<p>Memento Method is a Behavioral Design pattern which provides the ability to restore an object to its previous state. Without revealing the details of concrete implementations, it allows you to save and restore the previous version of the object. It tries not to disturb the encapsulation of the code and allows you to capture and externalize an object\u2019s internal state.</p> <pre><code>\"\"\"Memento class for saving the data\"\"\"\n\nclass Memento:\n\n    def __init__(self, file, content):\n    \"\"\"Constructor function\"\"\"\n\n        \"\"\"put all your file content here\"\"\"\n\n        self.file = file\n        self.content = content\n\n\"\"\"It's a File Writing Utility\"\"\"\n\nclass FileWriterUtility:\n\n    def __init__(self, file):\n    \"\"\"Constructor Function\"\"\"\n\n        \"\"\"store the input file data\"\"\"\n        self.file = file\n        self.content = \"\"\n\n    \"\"\"Write the data into the file\"\"\"\n\n    def write(self, string):\n        self.content += string\n\n    \"\"\"save the data into the Memento\"\"\"\n\n    def save(self):\n        return Memento(self.file, self.content)\n\n    \"\"\"UNDO feature provided\"\"\"\n\n    def undo(self, memento):\n        self.file = memento.file\n        self.content = memento.content\n\n\"\"\"CareTaker for FileWriter\"\"\"\n\nclass FileWriterCaretaker:\n\n    \"\"\"saves the data\"\"\"\n\n    def save(self, writer):\n        self.obj = writer.save()\n\n    \"\"\"undo the content\"\"\"\n\n    def undo(self, writer):\n        writer.undo(self.obj)\n\n\nif __name__ == '__main__':\n\n    \"\"\"create the caretaker object\"\"\"\n    caretaker = FileWriterCaretaker()\n\n    \"\"\"create the writer object\"\"\"\n    writer = FileWriterUtility(\"GFG.txt\")\n\n    \"\"\"write data into file using writer object\"\"\"\n    writer.write(\"First vision of GeeksforGeeks\\n\")\n    print(writer.content + \"\\n\\n\")\n\n    \"\"\"save the file\"\"\"\n    caretaker.save(writer)\n\n    \"\"\"again write using the writer \"\"\"\n    writer.write(\"Second vision of GeeksforGeeks\\n\")\n\n    print(writer.content + \"\\n\\n\")\n\n    \"\"\"undo the file\"\"\"\n    caretaker.undo(writer)\n\n    print(writer.content + \"\\n\\n\")\n</code></pre>"},{"location":"notebooks/coding/python-design-patterns/#state-method","title":"State Method","text":"<p>State method is Behavioral Design Pattern that allows an object to change its behavior when there occurs a change in its internal state. It helps in implementing the state as a derived class of the state pattern interface. If we have to change the behavior of an object based on its state, we can have a state variable in the Object and use if-else condition block to perform different actions based on the state. It may be termed as the object-oriented state machine. It implements the state transitions by invoking methods from the pattern\u2019s superclass.</p> <pre><code>class State:\n\"\"\"State class: Base State class\"\"\"\n\n    def scan(self):\n        \"\"\"Scan the dial to the next station\"\"\"\n        self.pos += 1\n\n        \"\"\"check for the last station\"\"\"\n        if self.pos == len(self.stations):\n            self.pos = 0\n        print(\"Visiting... Station is {} {}\".format(self.stations[self.pos], self.name))\n\n\nclass AmState(State):\n\"\"\"Separate Class for AM state of the radio\"\"\"  \n\n    def __init__(self, radio):\n    \"\"\"constructor for AM state class\"\"\"\n        self.radio = radio\n        self.stations = [\"1250\", \"1380\", \"1510\"]\n        self.pos = 0\n        self.name = \"AM\"\n\n\n    def toggle_amfm(self):\n    \"\"\"method for toggling the state\"\"\"\n        print(\"Switching to FM\")\n        self.radio.state = self.radio.fmstate\n\nclass FmState(State):\n\"\"\"Separate class for FM state\"\"\"\n\n    def __init__(self, radio):\n    \"\"\"Constriuctor for FM state\"\"\"\n        self.radio = radio\n        self.stations = [\"81.3\", \"89.1\", \"103.9\"]\n        self.pos = 0\n        self.name = \"FM\"\n\n    def toggle_amfm(self):\n    \"\"\"method for toggling the state\"\"\"\n        print(\"Switching to AM\")\n        self.radio.state = self.radio.amstate\n\nclass Radio:\n\"\"\"Dedicated class Radio\"\"\"\n\n    \"\"\"A radio. It has a scan button, and an AM / FM toggle switch.\"\"\"\n\n    def __init__(self):\n\n        \"\"\"We have an AM state and an FM state\"\"\"\n        self.fmstate = FmState(self)\n        self.amstate = AmState(self)\n        self.state = self.fmstate\n\n    def toggle_amfm(self):\n    \"\"\"method to toggle the switch\"\"\"\n        self.state.toggle_amfm()\n\n    def scan(self):\n    \"\"\"method to scan \"\"\"\n        self.state.scan()\n\n# main method\nif __name__ == \"__main__\":\n\n    # create radio object\n    radio = Radio()\n    actions = [radio.scan] * 3 + [radio.toggle_amfm] + [radio.scan] * 3\n    actions *= 2\n\n    for action in actions:\n        action()\n</code></pre> Output<pre><code>Visiting... Station is 89.1 FM\nVisiting... Station is 103.9 FM\nVisiting... Station is 81.3 FM\nSwitching to AM\nVisiting... Station is 1380 AM\nVisiting... Station is 1510 AM\nVisiting... Station is 1250 AM\nVisiting... Station is 1380 AM\nVisiting... Station is 1510 AM\nVisiting... Station is 1250 AM\nSwitching to FM\nVisiting... Station is 89.1 FM\nVisiting... Station is 103.9 FM\nVisiting... Station is 81.3 FM\n</code></pre>"},{"location":"notebooks/coding/python-design-patterns/#template-method","title":"Template Method","text":"<p>The Template method is a Behavioral Design Pattern that defines the skeleton of the operation and leaves the details to be implemented by the child class. Its subclasses can override the method implementations as per need but the invocation is to be in the same way as defined by an abstract class. It is one of the easiest among the Behavioral design pattern to understand and implements. Such methods are highly used in framework development as they allow us to reuse the single piece of code at different places by making certain changes. This leads to avoiding code duplication also.</p> <pre><code>def get_text():\n    \"\"\" method to get the text of file\"\"\"\n    return \"plain_text\"\n\ndef get_xml():\n    \"\"\" method to get the xml version of file\"\"\"\n\n    return \"xml\"\n\n\ndef get_pdf():\n    \"\"\" method to get the pdf version of file\"\"\"\n\n    return \"pdf\"\n\n\ndef get_csv():\n    \"\"\"method to get the csv version of file\"\"\"\n    return \"csv\"\n\n\ndef convert_to_text(data):\n    \"\"\"method used to convert the data into text format\"\"\"\n\n    print(\"[CONVERT]\")\n    return \"{} as text\".format(data)\n\n\ndef saver():\n    \"\"\"method used to save the data\"\"\"\n\n    print(\"[SAVE]\")\n\n\ndef template_function(getter, converter = False, to_save = False):\n    \"\"\"helper function named as template_function\"\"\"\n\n    \"\"\"input data from getter\"\"\"\n    data = getter()\n    print(\"Got {}\".format(data))\n\n    if len(data) &lt;= 3 and converter:\n        data = converter(data)\n    else:\n        print(\"Skip conversion\")\n\n    \"\"\"saves the data only if user want to save it\"\"\"\n    if to_save:\n        saver()\n\n    print(\"{} was processed\".format(data))\n\n\n\"\"\"main method\"\"\"\nif __name__ == \"__main__\":\n\n    template_function(get_text, to_save = True)\n\n    template_function(get_pdf, converter = convert_to_text)\n\n    template_function(get_csv, to_save = True)\n\n    template_function(get_xml, to_save = True)\n</code></pre> Output<pre><code>Got plain_text\nSkip conversion\n[SAVE]\nplain_text was processed\nGot pdf\n[CONVERT]\npdf as text was processed\nGot csv\nSkip conversion\n[SAVE]\ncsv was processed\n</code></pre>"},{"location":"notebooks/coding/python-design-patterns/#references-and-links","title":"References and links","text":"<ul> <li>Download the Jupyter notebook from: github.com/charlstown/Python-Design-Patterns</li> <li>Python Design Patterns course from Jungwoo Ryoo</li> <li>Jungwoo Ryoo LinkedIn profile</li> <li>More Notebooks like this here</li> </ul>"},{"location":"notebooks/coding/python-generators/","title":"Python generators","text":""},{"location":"notebooks/data-architecture/azure-big-picture/","title":"Azure Big Picture","text":""},{"location":"notebooks/data-architecture/cloud-migration-planning/","title":"Cloud migration planning","text":""},{"location":"notebooks/data-science/dimensionality-reduction-using-pca/","title":"Dimensionality reduction using PCA","text":""},{"location":"notebooks/data-science/machine-learning-basics/","title":"Machine learning basics","text":""},{"location":"notebooks/data-science/reinforcement-learning-basics/","title":"Reinforcement learning basics","text":""},{"location":"notebooks/data-science/sample-size-determination/","title":"Sample size determination","text":""},{"location":"notebooks/data-science/text-mining-sentiment-analysis/","title":"Text mining sentiment analysis","text":""},{"location":"projects/","title":"Projects","text":"<p>This section is where I turn my ideas into reality. It's a personal space where I experiment, create, and push my skills further through the projects that matter most to me.</p>"},{"location":"projects/madrid-airbnb-analysis/","title":"Madrid Airbnb analysis","text":"<p>This research aims to process big data information from the city using data design as an urban analysis tool. For this project, it is proposed to use this tool on big data from tourist rentals in the city of Madrid, to code the information collected and display it under a visual language that allows us to see the relationships and links that occur at the urban level.</p> <p></p>"},{"location":"projects/madrid-airbnb-analysis/#airbnb-growth-data-analysis-in-madrid","title":"Airbnb growth data analysis in Madrid","text":""},{"location":"projects/madrid-airbnb-analysis/#objectives","title":"Objectives","text":"<p>Test case presenting the Data Design as an urban tool and it's application. Applying the opportunistic sensing method where all the data used in the project was generated before for another purpose to study the Airbnb growth data analysis in Madrid.</p>"},{"location":"projects/madrid-airbnb-analysis/#specific-objectives","title":"Specific objectives:","text":"<ul> <li>Graph the tourist renting apartment growth for each district in the Madrid central almond, from 2015 to 2018.</li> <li>Compare the tourist house rent vs. traditional house rent in the Madrid central almond, from 2015 to 2018.</li> <li>Create a digital cartography showing the affection grade of tourist renting.</li> </ul>"},{"location":"projects/madrid-airbnb-analysis/#data-origins","title":"Data origins","text":"<p>For this project, I used data from these different websites:</p> <ul> <li>Airbnb data: http://insideairbnb.com/get-the-data.html</li> <li>Traditional renting: https://www.idealista.com/labs/</li> <li>Cartographies: https://datos.madrid.es/</li> </ul>"},{"location":"projects/madrid-airbnb-analysis/#data-analysis-contents","title":"Data analysis contents","text":"<ol> <li>Digital cartography locating all Airbnb data as points colored by the district. One cartography per year.</li> <li>Graph comparison showing the traditional renting vs. p2p renting evolution from 2015 to 2018.</li> <li>Digital cartography with a lower scale showing the tourist rent affection in the Madrid Central district from 2015 to 2018.</li> <li>Digital cartography showing a gradient map, filtering the affection of each square by the number of Airbnb apartments and the area of the square.</li> </ol>"},{"location":"projects/madrid-airbnb-analysis/#results","title":"Results","text":"<p>After the analysis, you are able to understand the 180 thousand rows from the data frame. Using the data design as a tool, the information can be coded producing a final result that makes possible the understanding of all the data with no explanations needed. Data design makes big data accessible to the reader. At the end of this post, you can see all the results of this study presented in 7 figures.</p> <ul> <li>The gif: shows the superposition of the figures from 1 to 4 presenting the Airbnb growth through the years.</li> <li>Figures 1-4: digital cartographies locating all Airbnb data as points colored by the district.</li> <li>Figure 6: tourist rent evolution in the Madrid Central district from 2015 to 2018.</li> <li>Figure 7: final digital cartography showing the affection of each urban square.</li> <li>Figure 5: graph comparison showing the traditional renting vs. p2p renting evolution from 2015 to 2018.</li> </ul> <p></p> <p></p> <p></p>"},{"location":"projects/madrid-airbnb-analysis/#other-links","title":"Other links","text":"<ul> <li>You can read the whole project here: https://issuu.com/charlstown/docs/carlos_grande-cv-isuue</li> <li>More post like this here: https://carlosgrande.me/category/projects/</li> </ul>"},{"location":"projects/madrid-invisible/","title":"Madrid invisible","text":"<p>Madrid invisible, data analysis of 230.000 complaints called in to 010, the municipal services phone number of Madrid during the year 2018. Here are the most common categories, plotted by time of day.</p> <p></p> <p></p>"},{"location":"projects/madrid-invisible/#references-othe-links","title":"REFERENCES &amp; OTHE LINKS:","text":"<ul> <li>More articles like this here: https://carlosgrande.me/category/projects/</li> </ul>"},{"location":"projects/my-rubiks-cube-model/","title":"My rubiks cube model","text":"<p>I've been thinking for a while about developing a model to simulate a Rubik's cube in Python. I wanted to make use of object-oriented programming, and this project was the perfect excuse to get down to work.</p> <p>Visit this link to access the code repository</p>"},{"location":"projects/my-rubiks-cube-model/#objectives","title":"Objectives","text":"<p>Code a Rubiks Cube model using object-oriented programming. The model should provide a user interface to interact with the cube and a render engine to visualize its process.</p>"},{"location":"projects/my-rubiks-cube-model/#object-oriented-programming-model","title":"Object-oriented programming model","text":"<p>I started by developing a centric architecture pattern with an orchestrator class. Then I added a class to generate the mapped cube as a data frame and a 2D visualizer with the unwrapped cube to render the work in progress. Finally, I developed a \"Drive\" class to apply permutations in the cube.</p> <p>Although it was not enough with a 2D unwrapped model, I started thinking about a 3D render engine using the Matplotlib library. I choose this library for its ease of use. It was hard to figure out how to update the model in real-time but way worth it.</p> <p>To develop the model, I defined these four Python classes:</p> <ol> <li>Class App: This class initiates the whole program and acts as an orchestrator calling other classes</li> <li>Class Cube: This class represents the Rubik's cube model as a Dataframe.</li> <li>Class Drive: This class maps the permutations and applies moves to the cube.</li> <li>Class Viz: This class generates 2D and 3D models and renders each representation.</li> </ol> <p>As a graphic support I drew a pattern with the classes, their methods and the calls between the different components in the infographic below.</p> <p></p>"},{"location":"projects/my-rubiks-cube-model/#cube-notation-model","title":"Cube notation model","text":"<p>Before developing the code, I needed to define how to represent and serialize the data. It was so fun to notate and map the cube.</p>"},{"location":"projects/my-rubiks-cube-model/#naming-convention","title":"Naming convention","text":"<p>First, I established a naming convention naming each face by color. I used the same color scheme I have at my physical Rubik's cube, the BOY scheme (blue-orange-yellow).</p> <ul> <li>W -&gt; white</li> <li>G -&gt; green</li> <li>O -&gt; Orange</li> <li>B -&gt; Blue</li> <li>R -&gt; Red</li> <li>Y -&gt; Yellow</li> </ul>"},{"location":"projects/my-rubiks-cube-model/#faces-notation","title":"Faces notation","text":"<p>I am used to solving my Rubik's cube starting on the white face. Therefore, I decided to take this face as the main reference, becoming the front and the center face when unwrapping the cube.</p> <pre><code>    | R |\n| B | W | G | Y |\n    | O |\n</code></pre>"},{"location":"projects/my-rubiks-cube-model/#faces-parametrization","title":"Faces parametrization","text":"<p>Once I assigned the colors to the faces, I named each face square by the initial of its color and the position occupied in the matrix. After this step, I had my model parametrized and ready to roll.</p> <pre><code>         |R1|R2|R3|\n         |R4|R5|R6|\n         |R7|R8|R9|\n|B1|B2|B3|W1|W2|W3|G1|G2|G3|Y1|Y2|Y3|\n|B4|B5|B6|W4|W5|W6|G4|G5|G6|Y4|Y5|Y6|\n|B7|B8|B9|W7|W8|W9|G7|G8|G9|Y7|Y8|Y9|\n         |O1|O2|O3|\n         |O4|O5|O6|\n         |O7|O8|O9|\n</code></pre>"},{"location":"projects/my-rubiks-cube-model/#serialization-structure","title":"Serialization structure","text":"<p>Last but not least, it was necessary to save the state of the data. To accomplish this, I used a JSON format structure defining keys by their face color and the list of squares of each face as values.</p> <pre><code>{\n  \"w\": [\"w1\", \"w2\", \"w3\", \"w4\", \"w5\", \"w6\", \"w7\", \"w8\", \"w9\"],\n  \"r\": [\"r1\", \"r2\", \"r3\", \"r4\", \"r5\", \"r6\", \"r7\", \"r8\", \"r9\"],\n  \"g\": [\"g1\", \"g2\", \"g3\", \"g4\", \"g5\", \"g6\", \"g7\", \"g8\", \"g9\"],\n  \"o\": [\"o1\", \"o2\", \"o3\", \"o4\", \"o5\", \"o6\", \"o7\", \"o8\", \"o9\"],\n  \"b\": [\"b1\", \"b2\", \"b3\", \"b4\", \"b5\", \"b6\", \"b7\", \"b8\", \"b9\"],\n  \"y\": [\"y1\", \"y2\", \"y3\", \"y4\", \"y5\", \"y6\", \"y7\", \"y8\", \"y9\"]\n}\n</code></pre>"},{"location":"projects/my-rubiks-cube-model/#moves-notation","title":"Moves notation","text":"<p>I used the Singmaster notation developed by David Singmaster. Its relative nature allows algorithms to be written in such a way that they can be applied regardless of which side is designated the top or how the colors are organized on a particular cube.</p> <ul> <li>t: Top side move 1 clockwise</li> <li>f: Front side move 1 clockwise</li> <li>d: Down side move 1 clockwise</li> <li>r: Right side move 1 clockwise</li> <li>l: Left side move 1 clockwise</li> <li>b: Back side move 1 clockwise</li> </ul> <p>So for example we could apply a combined premutation by doing:</p> <pre><code>t2l3b1r2\n</code></pre> <ul> <li>t2: Top side move 2 clockwise</li> <li>l3: Left side move 3 clockwise (same as left side move 1 anticlockwise)</li> <li>b1: Back side move 1 clockwise</li> <li>r2: Right side move 2 clockwise</li> </ul>"},{"location":"projects/my-rubiks-cube-model/#result","title":"Result","text":"<p>The final output displayed was way faster than I expected. The trick of regenerating the whole geometry for each move works like a charm is capable of applying over a hundred permutations in less than 30 seconds. Here is a capture of the final output applying twenty random moves to the cube.</p> <p></p>"},{"location":"projects/my-rubiks-cube-model/#references-and-links","title":"References and links","text":"<ul> <li>Github repository: github.com/charlstown/RubiksCube</li> <li>Rubik's cube color schemes</li> <li>Singmaster notation</li> <li>David Singmaster</li> <li>More Projects like this here</li> </ul>"},{"location":"references/","title":"References","text":"<p>This section is my go-to archive of articles and case studies. It\u2019s a collection of everything I find valuable as my personal source of inspiration.</p>"},{"location":"references/#articles","title":"Articles","text":"<p>A curated collection of articles that explore relevant concepts like the space of flows, the ubiquitous computing, etc.</p> <p> Start reading</p>"},{"location":"references/#case-studies","title":"Case studies","text":"<p>Detailed explorations of geek projects with a focus on social impact and real-world applications.</p> <p> Start reading</p>"},{"location":"references/articles/death-of-distance/","title":"Death of distance","text":"<p>I would like to post a fragment about the concept death of distance I found very interesting in one of my last reads, The City of Tomorrow written by Carlo Ratti and Matthew Claudel.</p>"},{"location":"references/articles/death-of-distance/#the-city-of-tomorrow-by-ratti-claudel","title":"The city of tomorrow by Ratti &amp; Claudel","text":"<p>A prevailing opinion at this crucial moment in humanity's cultural history was that distance would die. Physicality, it seemed, would lose all relevance as it was subsumed by the connective fabric of the Internet.</p> <p>The argument held that if information can be instantaneously transferred anywhere, to anyone, then all places are equivalent. If I am connected, why does it matter where I am? The post-information age will remove the limitations of geography. The digital living will include less and less dependence upon being in a specific place at a specific time, and the transmission of the place itself will start to become possible, wrote the MIT Media Lab founder Nicholas Negroponte. Work is a simple example: why commute to the office when the office will come right to your home?</p> <p>The internet was expected to neuter place in every dimension of human habitation, from entertainment to employment. Many of the tools for interaction, commerce, and information management were digitized and dematerialized. They became efficient, accessible, and aspatial. The economist Frances Cairncross followed his trend to its logical conclusion with an overt hypothesis that she called the death of distance. The Internet would usher in a  communications future... in which distance is irrelevant.</p> <p>These are resounding predictions, but history (so far) has proven them wrong. Over the past two decades, cities have grown as never before. Urban space has flourished across the globe as humanity rushes headlong into an urban era. Some calculations suggest that the urban population is increasing by a quarter million per day, amounting to a new London every month. [...] More than ever, cities are human magnets. Why? it seems that in the collective frenzy of the network, the death-of-distance theorist forgot something crucial to human experience: the importance of the physical interaction between people and with environment. [...]</p> <p>Traditional urban patterns cannot coexist with cyberspace. But long live the new, network-mediated metropolis of the digital era. Today's reality is a powerful collision of physical and digital that augments both, a triumph of atoms and bits. In short, the digital revolution did not kill urban spaces, far from it, but neither did it leave them unaffected. The introduction of the Internet, the space of flows, the connective tissue that theorist from Cairncross to Negroponte expected to kill physical proximity, has indeed had a profound impact on cities. Instead of flows replacing spaces an bits replacing atoms, cities ar now a hybrid space at the intersection of the two. Physical and virtual ar fused through a productive collision, where both propinquity and connectivity play an important role.</p> <p>(Ratti and Claudel, 2016, p. 18-20)</p>"},{"location":"references/articles/death-of-distance/#references-othe-links","title":"REFERENCES &amp; OTHE LINKS:","text":"<ul> <li>Ratti, C., and Claudel, M. (2016). The city of tomorrow: Sensors, networks, hackers, and the future of urban life. New Haven, CT: Yale University Press.</li> <li>Pict from Alex Andrews.</li> <li>More articles like this here: https://carlosgrande.me/category/case-studies/</li> </ul>"},{"location":"references/articles/space-of-flow/","title":"Space of flows","text":"<p>I wanted to post one of my favorite concepts about future cities, the space of flows coined by Manuel Castells. Castells supports that there is a new form of space typical of the social practice that domains and shapes the structure of society, he called it the space of flows. For Castells this space is the new matter setting up how our societies are made, space of flows means that physical space stops being considered absolute, and blends with the digital dimension.</p>"},{"location":"references/articles/space-of-flow/#what-is-the-space-of-flows","title":"What is the Space of Flows?","text":"<p>Age-based on the distinction between the space of places and the space of flows. This conceptualization has been widely discussed although not always understood, probably due to the obscurity of my formulation. My approach simply states that space is not a tangible reality, just as it is not from the natural science perspective. It is a concept constructed based on experience. And so, space in society is not the same as space in astrophysics or quantum mechanics. If we look at space as a social form and a social practice, throughout history space has been the material support of simultaneity in social practice. That is, space defines the time frame of social relationships. This is why cities were born from the concentration of the functions of command and control, of coordination, of exchange of goods and services, of diverse and interactive social life. In fact, cities are, from their onset, communication systems, increasing the chances of communication through physical contiguity. I call space of places the space of contiguity.</p> <p>(Castells, 1988, n/d)</p> <p>On the other hand, social practices as communication practices also took place at a distance through transportation and messaging. With the advent of electrically operated communication technologies, e.g. the telegraph and telephone, some measure of simultaneity were introduced in social relationships at a distance. But it was the development of microelectronics-based digital communication, advanced telecommunication networks, information systems, and computerized transportation that transformed the spatiality of social interaction by introducing simultaneity, or any chosen time frame, in social practices, regardless of the location of the actors engaged in the communication process. This new form of spatiality is what I conceptualized as the space of flows: the material support of simultaneous social practices communicated at a distance. This involves the production, transmission, and processing of flows of information.</p> <p>(Castells, 1988, n/d)</p>"},{"location":"references/articles/space-of-flow/#references-othe-links","title":"REFERENCES &amp; OTHE LINKS:","text":"<ul> <li>Castells, M. (2010). The Rise of the Network Society. Second edition. Retrieved from https://pdfs.semanticscholar.org/3746/ef34fd58d047d973008a0a723f832a83797e.pdf</li> <li>More articles like this here: https://carlosgrande.me/category/articles/</li> </ul>"},{"location":"references/articles/the-chinese-room/","title":"The chinese room","text":"<p>I wanted to post one of my favorites theories based on an impressive premise: that artificial intelligence is the fastest processing but is not intelligent. Jeff Hawking in his book On Intelligence developed a powerful theory about how the human brain works and explained why computers are not intelligent.</p>"},{"location":"references/articles/the-chinese-room/#artificial-intelligence-by-jeff-hawkins","title":"Artificial Intelligence by Jeff Hawkins","text":"<p>John Searle, an influential philosophy professor at the University of California at Berkeley, was at the time saying that computers were not, and could not be, intelligent. To prove it, in 1980 he came up with a thought experiment called the Chinese Room. It goes like this:</p> <p>Suppose you have a room with a slot in one wall, and inside is an English-speaking person sitting at a desk. He has a big book of instructions and all the pencils and scratch paper he could ever need. Flipping through the book, he sees that the instructions, written in English, dictate ways to manipulate, sort, and compare Chinese characteres. Mind you, the directions say nothing about the meanings of the Chinese characters; they only deal with how the characters are to be copied, erased, reordered, transcribed, and so forth.</p> <p>Someone outside the toom slips a piece of paper through the slot. On it is written a story and questions about the story, all in Chinese. The man inside doesn't speak or read a word of Chinese, but he picks up the paper and goes to work with the rulebook. He toils and toils, rotely following the instuctions in the book.</p> <p>At times the instructions tell him to write characters on scrap paper, and at other times to move and erase characters, the man works until the book's instructions tell him he is done. When he is finished at last he has written a new page of characters, which unbeknownst to him are the answers to the questions. The book tells him to pass his paperback through the slot. He does it and wonders what this whole tedious exercise has been about. Outside, a Chinese speaker reads the page. The answers are all correct, she notes-even insightful. If she is asked whether those answers came from an intelligent mind that had understood the story, she will definitely say yes. But can she be right? Who understood the story?</p> <p>Searly's answer is that no understanding did occur; it was just a bunch of mindless page flipping and pencil scratching. And now the bait-and-switch: the Chinese Room is exactly analogous to a digital computer. Thus no matter how cleverly a computer is designed to simulate intelligence by producing the same behavior as a human, it has no understanding and it is not intelligent. (Searle made it clear he didn't know what intelligence is; he was only saying that whatever it is, computers don't have it.)</p> <p>A human doesn't need to do anything to understand a story. I can read a story quitely, and although I have no overt behavior my understanding and comprehension are clear, at least to me. You, on the other hand, cannot tell from my quiet behavior whether I understand the story or not, or even if I know the language the story is written in. You might later ask me questions to see if I did, but my understanding occurred when I read the story, not just when I answer your questions. A thesis of this book is that understanding cannot be measured by external behavior, it is instead an internal metric of how the brain remembers things and uses its memories to make predictions.</p> <p>(Hawkins, 2004.)</p> <p>REFERENCES:</p> <ul> <li>Hawkins, J. with Blakeslee, S. (2004). On Intelligence. St. Martin's Griffin, New York.</li> <li>Post picture from Lenin Estrada. Retreived from https://www.pexels.com/</li> </ul> <p>FURTHER READING: - About Hawkins: Wikipedia Jeff Hawkins - TED talks: TED Talk - Numenta article: Hierarchical Temporal Memory - Buy the book: On Intelligence - More post like this here: https://carlosgrande.me/category/articles/</p>"},{"location":"references/articles/ubiquitous-computing/","title":"Ubiquitous computing","text":"<p>I wanted to post one of my favorite visions about future cities, the ubiquitous computing coined in 1988 by Mark Weiser. Ubicom for me is one of the keys to future technologies and smart cities, it is a total revolution. As we follow the tech progress with the first computers like Zuse or Eniac, the World Wide Web in 1992, the arrival of the smartphones in 2000 and the Internet of things in 2015 we can see how the connectivity is spreading like a virus and surrounding all around us. For Mark Weiser this state where the technology and connectivity start losing the hardware and mimetizing in the environment becoming invisible is called ubiquitous computing.</p>"},{"location":"references/articles/ubiquitous-computing/#ubiquitous-computing-by-mark-weiser","title":"Ubiquitous computing by Mark Weiser","text":"<p>Ubiquitous computing is roughly the opposite of virtual reality. Where virtual reality puts people inside a computer-generated world, ubiquitous computing forces the computer to live out here in the world with people. Virtual reality is primarily a horse power problem; ubiquitous computing is a very difficult integration of human factors, computer science, engineering, and social sciences.</p> <p>(Weiser, 1988, n/d)</p> <p>We believe that people live through their practices and tacit knowledge so that the most powerful things are those that are effectively invisible in use. This is a challenge that affects all of computer science. Our preliminary approach: Activate the world. Provide hundreds of wireless computing devices per person per office, of all scales (from 1 displays to wall sized). This has required new work in operating systems, user interfaces, networks, wireless, displays, and many other areas. We call our work ubiquitous computing. This is different from PDA's, dynabooks, or information at your fingertips. It is invisible, everywhere computing that does not live on a personal device of any sort, but is in the woodwork everywhere.</p> <p>(Weiser, 1994, n/d)</p> <p>A few thousand years ago people of the Fertile Crescent invented the technology of capturing words on flat surfaces using abstract symbols: literacy. The technology of literacy when first invented, and for thousands of years afterwards, was expensive, tightly controlled, precious. Today it effortlessly, unobtrusively, surrounds us. Look around now: how many objects and surfaces do you see with words on them? Computers in the workplace can be as effortless, and ubiquitous, as that. Long-term the PC and workstation will wither because computing access will be everywhere: in the walls, on wrists, and in scrap computers (like scrap paper) lying about to be grabbed as needed. This is called ubiquitous computing, or ubicomp.</p> <p>I first created the idea of ubiquitous computing from contemplation of the place of today's computer in actual activities of everyday life. In particular, anthropological studies of work life [Suchman 1985, Lave 1991] teach us that people primarily work in a world of shared situations and unexamined technological skills. However the computer today is isolated and isolating from the overall situation, and fails to get out of the way of the work. In other words, rather than being a tool through which we work, and so which disappears from our awareness, the computer too often remains the focus of attention. And this is true throughout the domain of personal computing as currently implemented and discussed for the future, whether one thinks of PC's, palmtops, or dynabooks. The characterization of the future computer as the intimate computer [Kay 1991], or rather like a human assistant [Tesler 1991] makes this inappropriate attention to the machine itself particularly apparent.</p> <p>(Weiser, 1993, pp. 74-83)</p> <p>REFERENCES AND OTHER LINKS:</p> <ul> <li>Weiser, M. (1988). What Ubiquitous Computing Isn't. Retrieved from http://www.ubiq.com/weiser/UbiHome.html</li> <li>Weiser, M. (1993). Ubiquitous computing. IEEE Computer. Retrieved from http://www.ubiq.com/weiser/UbiHome.html</li> <li>Weiser, M. (1994). Building Invisible Interfaces, User Interface, Systems, and Technologies (UIST). Retrieved from http://www.ubiq.com/weiser/UbiHome.html</li> <li>Warner Brothers. (1965) Don Adams holding shoe phone in publicity portrait for the television series 'Get Smart'. Retrieved from https://www.gettyimages.es/</li> <li>More post like this here: https://carlosgrande.me/category/articles/</li> </ul>"},{"location":"references/articles/whole-brain-emulation/","title":"Whole brain emulation","text":"<p>I wanted to post some conclusions from a recent book I've been reading. The Technological Singularity (Shanahan, M. 2015). A singularity in human society would occur if the exponential technological progress brought such a dramatic change that human affairs as we understand them today came to an end. In this case, the whole brain emulation theory could make a singularity in our human society.</p>"},{"location":"references/articles/whole-brain-emulation/#the-whole-brain-emulation-theory","title":"The whole brain emulation theory","text":"<p>The idea behind the whole brain emulation theory (WBE) is to make an exact working copy of a particular brain in a nonbiological system. To achieve this idea, we need to assume the hypothesis that human behavior is determined exclusively by physical processes in the brain that mediate between its incoming sensory signals and its outgoing motor signals.</p>"},{"location":"references/articles/whole-brain-emulation/#the-three-stages-behind-the-wbe-theory","title":"The three stages behind the \"WBE\" theory","text":"<p>Shanahan says that there are three stages to achieve the whole brain emulation mapping, simulation, and embodiment.</p> <p>[caption id=\"attachment_1738\" align=\"aligncenter\" width=\"413\"] Success levels for WBE.Sandberg, A. and Bostrom, N. (2008), \"Whole Brain Emulation: A Roadmap\".[/caption]</p> <p>The first stage is to map the brain of the subject at high spatial resolution. The result will be an exquisitely detailed blueprint of a particular brain at a particular time.</p> <p>The second stage of the process is to use this mapped blueprint to build a real-time simulation emulating the electrochemical activity of all those neurons and their connections. This simulation could be built using the standard techniques from the field of computational neuroscience, using mathematical formulations of neural behavior such as the Hodgkin-Huxley model.</p> <p>The third stage of the process is to interface the simulation to an external environment, expecting incoming signals and generating outgoing signals just like those of its biological precursor.</p> <p>If the mapping and simulation stages are successful, then the behavior of the simulated neurons, both individually and as a population, should be indistinguishable from that of the original, biological brain given the same input from the environment. Consequently, small inaccuracies in the mapping process, would cause the behavior of the simulation eventually to diverge from that of its biological prototype. If these microscopic deviations are sufficiently small, the macro-scale outward behavior of the emulation would surely be indistinguishable from that of the original. From the standpoint of an observer, the emulation would seem to make the same decisions and perform the same actions as its prototype under any given set of circumstances.</p>"},{"location":"references/articles/whole-brain-emulation/#the-technology-of-neural-simulation","title":"The technology of Neural simulation","text":"<p>The electrical and chemical properties of the various components of a neuron can be modeled using Hodgkin-Huxley equations, and fortunately, neurons are slow. Even when a neuron is excited it only emits a spike every few milliseconds. In the time it takes for a typical neuron to emit two spikes, a desktop computer running at a modest 3GHz can perform more than ten million operations.</p> <p>However, even the brain of a mouse contains tens of millions of neurons, and to simulate them all accurately and in real-time requires an awful lot of computation. Rather than using a serial processor that carries out one operation at a time, the simulation can be done with multiple processors all running simultaneously, each one simulating many thousands of neurons. And the neuron itself \"computes\" a function that continuously maps its dendritic \"input\" and the current state of its \"memory\" to the \"output\" signal it delivers to its axon.</p> <p>As the number of processors they incorporate has increased, the cost per processor has gone down, following an exponential trend that accords with Moore's law. By 2012 the world's most powerful computer, Cray's Titan, was based on a hybrid architecture that incorporated 18 688 GPUs, each one in itself a powerful parallel computer.</p>"},{"location":"references/articles/whole-brain-emulation/#brain-scale-computation","title":"Brain scale computation","text":"<p>It would be possible to simulate the whole brain of a mouse using the most powerful computers of the mid-2010s if the level of physical detail required for successful emulation were sufficiently low and we had a blueprint at the required level of detail.</p> <p>The engineering challenge here is not merely to achieve the required number of FLOPS (floating-point operations per second) but to do so in a small volume and with low power consumption. The average human brain (female) occupies a mere 1 130 cm3 and consumes just 20 W. By contrast, the Tianhe-2, the world's most powerful supercomputer in 2013, consumes 24M MW and is housed in a complex occupying 720 m2.</p> <p>One promising approach is using neuromorphic hardware. Conventional digital hardware. performs hundreds of binary floating-point arithmetic operations to simulate a few milliseconds of change on a single neuron's membrane potential. This involves thousands of transistors switching events, each of which consumes power. The neuromorphic approach does away with all this digital paraphernalia and uses analog components that behave like the original neuron. The result is far more efficient in terms of power consumption.</p> <p>What we need is hardware, one candidate is quantum-dot cellular automata (QDCA) nano-scale semiconductor device that can act like a transistor, witching states very rapidly but using very little power. The advantage of WDCA over conventional CMOS silicon technology is the enormous scale of integration of the permit, enabling many more switching devices to be placed in the same area.</p> <p>REFERENCES:</p> <ul> <li>Shanahan, M. (2015). The Technological Singularity. (The MIT Press, Cambridge, Massachusetts)</li> <li>Sandberg, A. and Bostrom, N. (2008), \"Whole Brain Emulation: A Roadmap\".</li> </ul> <p>FURTHER READING:</p> <ul> <li>About Hodgkin Huxley model: Hodgkin\u2013Huxley model</li> <li>About Whole-brain functional imaging at cellular resolution using light-sheet microscopy</li> <li>About Whole Brain Emulation Roadmap</li> <li>About Sequencing the Connectome</li> <li>About  An Ultrasonic, Low Power Solution for Chronic Brain-Machine Interfaces</li> <li>About the Human Brain Project</li> <li>About Neuromorphic silicon neuron circuits</li> <li>About Molecular Quantum-Dot Cellular Automata</li> <li>About Ultimate physical limits to computation</li> <li>About Neuromorphic engineering</li> <li>More post like this here: https://carlosgrande.me/category/articles/</li> </ul>"},{"location":"references/case-studies/3000000-kms/","title":"300.000 Km/s studio","text":"<p>In this post, I wanted to introduce 300000 kms. An urban planning agency founded in Barcelona aimed at making cities the most livable places on the planet. They are a team of architects, urban planners, data scientists and programmers who explore the potentials of big data and new computing paradigms to improve urban analysis, strategic planning and decision-making.</p>"},{"location":"references/case-studies/3000000-kms/#atnight-project","title":"AtNight project","text":"<p>The project addresses the mapping of intangible aspects for a better understanding of the city's visible surface and internal flows organization beyond traditional mobility and usage as the guideline topics that allow us to verify earlier hypotheses on nightscapes configuration.</p> <p>We classified the data into three main categories according to the source. First, we used information from Open Data and public geographic services regarding cartographic features and general statistics (demography, land use, street layout, etc). Second, we obtained mobility trends and energy consumption averages through agreements with public and private local agencies. Finally, we collected geo-social data sets by a systematic crawl of several location-based social networks API.</p> <p>The basic technological idea behind the project was to set up a platform that harvests data from geo-social streams and local Open Data providers apply mining functionalities, extracts key elements, and plots them on a series of maps. The research followed several phases, from the design of capture engines to the management of data via a geographic information system, allowing parameters to be set and tuned in accordance with specific data sets</p> <p>We employed intuitive strategies to represent and communicate information from the simple existence of data and non-data, each value has been related to a geometrical feature to the superimposition of several data sets. On the one hand, we assigned color scales to easily distinguish dissimilar situations, mainly day and night perception. We established a hierarchy for certain values within the network by applying thickness and size to geometrical entities. On the other hand, new information can be seen by modifying the opacity of data groups or adopting multiplication or subtraction criteria of RGB values. Santamar\u00eda, and Mart\u00ednez (2015, as cited in Binhanic, 2015).</p>"},{"location":"references/case-studies/3000000-kms/#conclusion","title":"Conclusion","text":"<p>Over the next few years, we will reconfigure our cities. We should adapt urban contexts to new scenarios resulting from urban and technical advancements. The ability to transform society will be key to ensuring a prosperous future and sustainable development. Recent technological breakthroughs have derived in the consolidation of a distinct urban model, the Smart City, which places sensors of all kinds to monitor life in real-time (weather, traffic, the flow of people, contamination, etc.). What if these same digital achievements could work in reverse and could interact with urban data to raise a common imagination from the collective and collaborative contribution? In accordance with cities rearrangement, we should foster urban planning from another perspective-taking advantage of novel representation tools to interpret, intervene, and rebuild.</p> <p>It is crucial to delve into this line of research. Today, urban planning still relies upon traditional cartographic information (topography, plot division, usage) and neighborhood-level statistics, these long-established practices are inadequate in comprehending the interaction of citizenship on the urban skin and the territory. The new and accessible cartographic information will provide an inestimable tool for citizen empowerment, enabling individuals to tale collective decisions about the intangible city we actually inhabit. Santamar\u00eda, and Mart\u00ednez (2015, as cited in Binhanic, 2015).</p>"},{"location":"references/case-studies/3000000-kms/#gallery","title":"Gallery","text":"<p>REFERENCES:</p> <ul> <li>Binhanic, D. (2015). New challenges for Data Design. Springer. Link to the book</li> <li>Figures retrieved from http://www.atnight.ws</li> </ul> <p>FURTHER READING:</p> <ul> <li>300000 Kms personal site: 300000kms.net/</li> <li>LinkedIn: @300-000km-s</li> <li>AtNight project: http://www.atnight.ws/</li> <li>More articles like this here: https://carlosgrande.me/category/case-studies/</li> </ul>"},{"location":"references/case-studies/ben-willers/","title":"Ben Willers","text":"<p>In this post, I wanted to introduce Ben Willers, a freelance graphic and data designer in the UK, who enjoys exploring and creating data-rich visualizations. He has published amazing big data design works full of inspiration capable of giving shape and meaning to Big Data. Ben Willers as a data designer believes stories in data should be built on a solid foundation of facts. As a result allows the audience to arrive at their own conclusions based on the evidence provided.</p>"},{"location":"references/case-studies/ben-willers/#show-dont-tell-by-ben-willers","title":"Show, Don't Tell. By Ben Willers","text":"<p>Too often, I see visual representations of data used as a means to an end, reinforcing or adding creditability to an argument, or worse still an attempt to decorate or distract. Those experienced with handling large quantities of raw data will know of the pleasures that can be had from uncovering hidden truths buried within. These may be unusual and unexpected, sometimes even controversial. They provoke thought, spark conversations, and encourage further exploration. By carefully selecting data and display methods, we are able to communicate these discoveries without explicit exposition, allowing the reader to explore the rich data landscape that lies before us.</p> <p>[...]</p> <p>One of the greatest challenges I face in my work is finding a balance that does not undermine the richness of the data, yet is still accessible and engaging to the reader. Many of us have been conditioned to read in a particular way, along a linear path in a one-dimensional fashion. Un familiar territory is inevitably going to cause many readers, and perhaps some designers with a degree of anxiety, wich may explain why I see so many displays of data that simply mimic the traditional reading style. I always asume my reader is highly capable of judgment and reasoning, possessing a desire to learn but lacking knowledge within a particular area. My heart always sinks whenever a client asks me to simplify the scope of a design and turn up the visual 'cool factor' when a miltidimensional approach would be my preference. It is my hope that as audiences become more accustomed to these alternative methods of reading, designers will grow in confidence and deliver thought-provokin visualizations on a more regular basis.</p> <p>(Willers, 2015, quoted in Bihanic, 2015.)</p>"},{"location":"references/case-studies/ben-willers/#gallery","title":"Gallery","text":"<p>REFERENCES &amp; OTHE LINKS:</p> <ul> <li>Bihanic, D. (2015). New challenges for Data Design. London: Springer-Verlag.</li> <li>Figures retrieved from http://www.benwillers.com/</li> <li>You can by the book here: https://www.springer.com/gp/book/9781447165958</li> <li>More articles like this here: https://carlosgrande.me/category/case-studies/</li> </ul>"},{"location":"references/case-studies/eric-fischer/","title":"Eric Fischer","text":"<p>In this post, I wanted to introduce Eric Fischer, a data designer and software developer at San Francisco. His works have been published in the Modern Art Museum, in Wired magazine, and other online magazine publications. Fischer is particularly interested in using geographic data to understand and improve pedestrian traffic and movements within the cities. Fischer never expected his private reflections to become in national art, but his data maps uploaded to your account Flickr, have been praised by art blogs, imitated by the New York Times, and exhibited at the Museum of Modern Art. Fisher creates maps by extracting resources such as the US data census UU., Flickr photographs, and tweets.</p> <p>Fischer manages to perfectly represent all hidden information from the cities, he has published around 100 city maps in his investigations. He wrote a software that allowed him to represent all these maps and he developed it to such a level where It has become a payment tool known as MapBox. This tool provides an opportunity for data scientists and engineers leaving them a powerful tool capable of displaying dynamic maps in real-time. ---</p>"},{"location":"references/case-studies/eric-fischer/#the-washington-post-eric-fischer-uses-maps","title":"The Washington Post: Eric Fischer uses maps","text":"<p>Fischer represents data and geotags by colors and points using a code he wrote by himself. The result of the patterns produces visual galaxies of information. His work caught the attention in 2010 when he published some maps showing the list of photos taken by tourists and by locals around the city. (...) His next job focused him on racial divisions in 109 cities in the United States, showing the divisions still present in the population. Their recent works, called Look at something or say something, compare where people write about their social life and where people photograph. (...) \u201cI want people to look at the different areas of the maps that they know well and to discover what some popular places do and other not-so-popular places, \u201dsays Fischer. \u201cIf you live in a street that nobody goes to, what causes people to not want to go there? One of his observations is that people go on vacation to sites where they can walk to the sites, but on the contrary, they live in places where they cannot do so (Bell, 2011).</p>"},{"location":"references/case-studies/eric-fischer/#gallery","title":"Gallery","text":"<p>REFERENCES &amp; OTHE LINKS:</p> <ul> <li>Bell, M. (2011) Eric Fischer uses maps, Twitter and Flickr to explore the contours of cities, Washington, USA, The Washington Post. Retrieved from https://www.washingtonpost.com</li> <li>Figures retrieved from https://www.flickr.com/photos/walkingsf/albums</li> </ul> <p>FURTHER READING:</p> <ul> <li>Fischer Word Atlas article: https://blog.mapbox.com/linking-the-most-interesting-places-in-the-world-b421f35b35a0</li> <li>Word Atlas: https://www.flickr.com/photos/walkingsf/sets/72157623971287575/</li> <li>MapBox blog: https://blog.mapbox.com/</li> <li>More articles like this here: https://carlosgrande.me/category/case-studies/</li> </ul>"},{"location":"references/case-studies/giorgia-lupi/","title":"Giorgia Lupi","text":"<p>In this post, I wanted to introduce Giorgia Lupi, Co-Founder and Design Director at Accurat, she is an information designer and researcher. Her work in information visualization frequently crosses the divide between digital and print, exploring visual models and metaphors to represent dense and rich data-driven stories.</p>"},{"location":"references/case-studies/giorgia-lupi/#visual-models-and-metaphors-by-giorgia-lupi","title":"Visual Models and Metaphors. By Giorgia Lupi.","text":"<p>At Accurat, we publish compound and complex stories that each time are not told through an article, but through a data visualization. We here try to think like a journalist, rather than data analysts: Understanding in which contexts we should interpret the data we gather and analyze, and questioning ourselves about what is interesting in these numbers and what possible correlations with other information we might experiment to unveil hidden stories. We aim at delivering rich visual narratives able to maintain the complexity of the data but still making this complexity more accessible and understandable through the visualization.</p> <p>[...]</p> <p>Whenever the main purpose of visualizations is to open readers' eyes yo new knowledge and to reveal something new about the world or to engage and entertain the audience about a topic, it is impractical to avoid a certain level of visual and complexity indeed. The world is complex, compound, rich in information that can be combined in endless ways, therefore catching new points of view or discovering something that you did not know before often cannot happen at a glance: This process of \"revelation\" often needs and require an in-depth investigation of the context. Consequently, we like to think at these kinds of data visualizations we presented as visual ways to convey the richness, the involvement, and feelings that we experience in our everyday lives rather than a simplification of the world.</p> <p>(Giorgia Lupi, 2015, quoted in Bihanic, 2015.)</p>"},{"location":"references/case-studies/giorgia-lupi/#gallery","title":"Gallery","text":"<p>REFERENCES:</p> <ul> <li>Bihanic, D. (2015). New challenges for Data Design. London: Springer-Verlag.</li> <li>Figures retrieved from https://www.behance.net/accurat</li> <li>You can buy the book here: https://www.springer.com/gp/book/9781447165958</li> </ul> <p>FURTHER READING:</p> <ul> <li>Giorgia Lupi personal site: http://giorgialupi.com/</li> <li>@giorgialupi: https://twitter.com/giorgialupi</li> <li>More articles like this here: https://carlosgrande.me/category/case-studies/</li> </ul>"},{"location":"references/case-studies/moritz-stefaner/","title":"Moritz Stefaner","text":"<p>In this post, I wanted to introduce Moritz Stefaner, a designer dedicated to data visualization and big data. He works as a designer, consultant, and researcher, collaborating and helping organizations and companies to find the real meaning of the data he works with.</p> <p>Stefaner has a quite diverse background. After school, he applied for art schools, but he didn't get accepted. He finally proceeds to do a one-year 'multimedia producer' crash course, and worked with agencies for a few years, mostly doing flash websites. In the mid-twenties, he decided to go back to University to study Cognitive Science. After these years the went to Postdam to study Interface Design, working as a part-time researcher for a few years. In the last years, he has been working as an independent consultant and designer of data visualization.</p>"},{"location":"references/case-studies/moritz-stefaner/#truth-and-beauty-moritz-stefaner","title":"Truth and Beauty. (Moritz Stefaner)","text":"<p>To me, truth and beauty are equally important. In a visualization project, if you have only one without the other, you are not done yet. Buckminister Fuller, the famous designer and systems theorist, said once, in essence, that he didn't think about beauty when he started a design, engineering, or architectural project. He was just concerned with its function. He wanted to find the right way to devise the product. But then, in the end, if the solution he came up with was not beautiful, he knew something was wrong. For Buckminister Fuller, in some sense, beauty was an indicator of functionality and truth.</p> <p>Design is much more than mere decoration. Often, people think of design as is the wrong approach. Good design is tightly intertwined with the content it presents. It consists of thinking about what to show; what to leave out; what to highlight; how to structure information; what rhythm, visual flow, and pace, you want your story to have. That's what design is all about. (Stefaner, 2011) (Stefaner, 2015, quoted in Bihanic, 2015.)</p>"},{"location":"references/case-studies/moritz-stefaner/#gallery","title":"Gallery","text":"<p>REFERENCES &amp; OTHE LINKS:</p> <ul> <li>Bihanic, D. (2015). New challenges for Data Design. London: Springer-Verlag.</li> <li>Figures retrieved from https://truth-and-beauty.net/](https://truth-and-beauty.net/)</li> <li>You can by the book here: https://www.springer.com/gp/book/9781447165958</li> <li>More articles like this here: https://carlosgrande.me/category/case-studies/</li> </ul> <p>FURTHER READING:</p> <ul> <li>Follow @Moritz_Stefaner: https://twitter.com/moritz_stefaner?lang=es</li> <li>About him: https://en.wikipedia.org/wiki/Moritz_Stefaner</li> <li>More articles like this here: https://carlosgrande.me/category/case-studies/</li> </ul>"},{"location":"references/case-studies/nicholas-felton/","title":"Nicholas Felton","text":"<p>In this post, I wanted to introduce Nicholas Felton, who is the author of several Personal Annual Reports that weave numerous measurements into a tapestry of graphs, maps, and statistics to reflect the year's activities. He is the co-founder of www.Reporter-App.com, www.Daytum.com, and is a former member of the product design team at Facebook.</p> <p>His work has been profiled in publications including The New York Times, Wall Street Journal, Wired and Good Magazine and has been recognized as one of the 50 most influential designers in America by Fast Company. He is credited with influencing the design of Facebook's timeline.</p>"},{"location":"references/case-studies/nicholas-felton/#tracing-my-life-by-nicholas-felton","title":"Tracing My Life. By Nicholas Felton","text":"<p>The Feltron Annual Report was first published in 2005, It was intended as a quick project in which to share my interests and habits from the previous year. I had assumed that the Report would be interesting to only friends and family, and was surprised to find it picked up by bloggers and passed around the internet. As a result of the encouragement I received this in this first year, I have continued the project and increased its scope each year since.</p> <p>[...]</p> <p>The challenge each year is to define an approach that will illuminate new aspects of my behavior. After a few years of straightforward accountings, I started exploring new lenses for recording the year. In 2008, I concentrate on the distances I traveled. In 2009, I asked those around me to report back on my behaviors. In 2010, I lost my father and used this project to describe his life through the fragments of data he left behind. In 2011, I combined two years of data to look at the ways in which my behavior changed depending on who I was with. Most recently I developed an iPhone app for 2012 to ping me at random intervals and record my behavior. I have already planned the methodology for 2014 and will continue the project as long as I think there are new challenges to be tackled.</p> <p>The project initially served as a self-promotional piece, but has evolved fat beyond that point. It is now more of a research &amp; development platform for exploring new dimensions of data collection and representation.</p> <p>(Felton, 2015, quoted in Bihanic, 2015.)</p>"},{"location":"references/case-studies/nicholas-felton/#gallery","title":"Gallery","text":"<p>REFERENCES:</p> <ul> <li>Bihanic, D. (2015). New challenges for Data Design. London: Springer-Verlag.</li> <li>Figures retrieved from http://feltron.com/index.html</li> <li>You can buy the book here: https://www.springer.com/gp/book/9781447165958</li> </ul> <p>FURTHER READING:</p> <ul> <li>Pitch Interactive site: http://feltron.com/index.html</li> <li>@feltron: https://twitter.com/feltron</li> <li>More articles like this here: https://carlosgrande.me/category/case-studies/</li> </ul>"},{"location":"references/case-studies/refik-anadol/","title":"Refik Anadol","text":"<p>In this post, I wanted to introduce Refik Anadol,  is a media artist, director, and pioneer in the aesthetics of machine intelligence. His main work locates creativity at the intersection of humans and machines. He takes the data that flows around us as his primary material and uses Neural Networks to design with a thinking brush. Refik offers us radical visualizations of our digitized memories and expanding the possibilities of architecture, narrative, and the body in motion.</p> <p></p>"},{"location":"references/case-studies/refik-anadol/#a-data-universe-made-of-memories-ai-and-architecture-interview-with-refik-anadol","title":"A data universe made of memories, AI and architecture. Interview with Refik anadol.","text":"<p>The idea of the project started in 2012 while having my second Master\u2019s degree at UCLA. I was taking a highly inspiring class from a legendary professor Jennifer Steinkamp where she had brilliant questions. Brainstorming with her helped me a lot on finding ways to implement my dreams into projects and create perfect sequences. At the time I couldn\u2019t make a project that big as a student. Even though Infinity Room is just a room, it\u2019s still a place that needs construction, four projections, complex drawings and engineering. I was able to realize the project only in 2015. It was first launched in Istanbul. Later it travelled to 29 cities. It was transformed into a global project with almost two million viewers and has been to every single continent in the world. This is mind boggling. The beauty of the possibility that an idea can reach so many people with all kinds of backgrounds and ages. I don\u2019t have any other projects that reached people in such extent.</p> <p>[...]</p> <p>Just like everyone else, I love to remember and talk about my memories. The concept of time is closely related to memory. We\u2019re all aware that the concept of time has a strong correlation to memory. I\u2019ve always had an obsession about memory, remembering and lifelogging. Data are also a form of memory. Our likes, posts, comments, the cars we use and GPS are also a memory. Interestingly, the concept of memory in the 21st century doesn\u2019t boil down to the cognitive and neurological system of the humans. We\u2019re in a position to interact with machines. I found this topic interesting and we began to work on it with the outstanding professor and a neuroscientist, Adam Gazzaley. Being a part the University of California as both an alumni and a researcher, I had the chance to access a large data set and work with a sample whilst doing the research. In these data sets, there were extended researches on brain and memory shared by groups which have accelerated the speed of my work. I can\u2019t give many names working with John Does. Instead of concerning ourselves with what memory belonged to whom, we detached the ego from the data and delved into the energy signals that are the representation of the moment of reminiscence. As a result, I transformed these outputs into sculptures. In terms of exhibiting the project, I wasn\u2019t sure sure if the public space was a fit for an intimate idea like this one. I thought it would be better understood and monitored if it was placed in a gallery space rather than a public space.</p> <p>(Anadol, 2019)</p>"},{"location":"references/case-studies/refik-anadol/#gallery","title":"Gallery","text":"<p>REFERENCES:</p> <ul> <li>Anadol, R. (2019). Personal interview. Digicult. Link to interview</li> <li>Figures retrieved from https://www.behance.net/refikanadol</li> </ul> <p>FURTHER READING:</p> <ul> <li>Refik Anadol personal site: refikanadol.com</li> <li>@refikanadol: https://twitter.com/refikanadol</li> <li>More articles like this here: https://carlosgrande.me/category/case-studies/</li> </ul>"},{"location":"references/case-studies/wesley-grubbs/","title":"Wesley Grubbs","text":"<p>In this post, I wanted to introduce Wesley Grubbs, is an artist, data visualizer and provocateur based in Berkeley. Grubbs founded in 2007 the data visualization studio Pitch Interactive, he is in charge of technical and creative direction and managing all crucial aspects that define the project scope, client expectations, deliverables and story telling.</p> <p>Built upon his life experiences, education and his innate interest in the brain and cognition, Grubbs work focuses on revealing patterns about human behavior and how our actions impact our surroundings.</p>"},{"location":"references/case-studies/wesley-grubbs/#a-process-dedicated-to-cognition-and-memory-by-wesley-grubbs","title":"A process Dedicated to Cognition and Memory. By Wesley Grubbs","text":"<p>The most effective form of communication is a visual one. Even when we speak we use visual metaphors to help our communication (i.e., to bark up the wrong tree or to push one's buttons). Focusing on a visual that helps bring attention to the story can also help establish a memory of the story, and this is the key aspect we take into consideration in every visualization we build.</p> <p>[...]</p> <p>It never ceases to amaze me how frequent the importance of good design and esthetics is disregarded as unnecessary fluff or a distraction from the actual data. Design is critical in the work we do, and we spend a significant amount of time during the production and postproduction focusing on colors, type, aligment, and other visuall elements because they are a crucial part to the communication aspect of the visualization. The human brain is wired to not only process but to attract to intrincate imagery, for example, looking at a mountain range or watching waves on a beach. These are inmense complex images of terrain and motion, yet we often find ourselves mesmerized and amazed by them even if we cannot appreciate fine art. When intrincate imagery has a story embedded within, it only simulates our interest more.</p> <p>(Grubbs, 2015, quoted in Bihanic, 2015.)</p>"},{"location":"references/case-studies/wesley-grubbs/#gallery","title":"Gallery","text":"<p>REFERENCES:</p> <ul> <li>Bihanic, D. (2015). New challenges for Data Design. London: Springer-Verlag.</li> <li>Figures retrieved from https://pitchinteractive.com/work/Wired311/</li> <li>You can by the book here: https://www.springer.com/gp/book/9781447165958</li> </ul> <p>FURTHER READING:</p> <ul> <li>Pitch Interactive site: https://pitchinteractive.com/</li> <li>@wesleygrubbs: https://twitter.com/wesleygrubbs</li> <li>Article in Substratum: Substratum Article</li> <li>More articles like this here: https://carlosgrande.me/category/case-studies/</li> </ul>"},{"location":"resources/","title":"Resources","text":"<p>This section houses my essential resources, including my thesis on data concepts, various cheat sheets, coding templates, and favorite tools. It\u2019s a comprehensive collection that supports my work and projects.</p>"},{"location":"resources/#thesis","title":"Thesis","text":"<p>In-depth explorations of complex tech concepts like Data Mesh, Data Products, and Data Fabric.</p> <p> Start reading</p>"},{"location":"resources/#cheatsheets","title":"Cheatsheets","text":"<p>Concise, one-page references for coding languages, tools, and essential commands.</p> <p> Start reading</p>"},{"location":"resources/#coding-templates","title":"Coding templates","text":"<p>Ready-to-use Cookiecutter templates for efficient project setup and development.</p> <p> Start reading</p>"},{"location":"resources/#tools","title":"Tools","text":"<p>Software tools that I find highly effective and useful for various tasks.</p> <p> Start reading</p>"},{"location":"resources/cheatsheets/my-business-model-canvas/","title":"Business model canvas","text":"<p>With this post, I wanted to share my latest Business Model Canvas Design and explain all the sections that you should fill to organize and develop your project.</p>"},{"location":"resources/cheatsheets/my-business-model-canvas/#lean-startup-by-definition","title":"Lean startup by definition","text":"<p>Lean startup is a methodology for developing businesses and products that aim to shorten product development cycles and rapidly discover if a proposed business model is viable; this is achieved by adopting a combination of business-hypothesis-driven experimentation, iterative product releases, and validated learning. (Ries, 2011)</p>"},{"location":"resources/cheatsheets/my-business-model-canvas/#business-model-canvas_1","title":"Business model canvas","text":"<p>A Business Model Canvas is a strategic management and lean startup template for developing new or documenting existing business models. It helps visualize what is important and forces users to address key areas. It can also be used by a team (employees and/or advisors) to understand relationships and reach agreements.</p> <p></p> <p>Download the Canvas in PDF</p>"},{"location":"resources/cheatsheets/my-business-model-canvas/#1-customer-segments","title":"1. Customer segments","text":"<p>Customer segments are the community of customers or businesses that you are aiming to sell your product or services to. Customer segments is one of the most important building blocks in the business model canvas for your business, so getting this building block right is key to your success.</p> <p>Define your client/s type: Try to summarize your client features like age, hobbies, routines, environment...</p> <p>Estimate and define your TAM, SAM and SOM:</p> <ul> <li>TAM: total addresssable market</li> <li>SAM: serviceable available market</li> <li>SOM: serviceable obtainable market</li> </ul> <p>Define the problem or need solved with your business model: Focus and describes how you solve your client\u2019s need.</p>"},{"location":"resources/cheatsheets/my-business-model-canvas/#2-value-propositions","title":"2. Value propositions","text":"<p>A value proposition is designed to convince a potential customer that your particular product or service will add more value or better solve a problem than your competition. It should answer the fundamental question of \u201cWhy should I buy your product instead of your competitor\u2019s product?\u201d</p> <ul> <li>Describe your product or service: Describe and list your value proposition, focus on the problem you are solving.</li> <li>List the benefits that your idea provides to your client segment: List all the advantages that your idea provide to the market you are serving to.</li> <li>Describe the reason why your potential client is choosing you: Describe the reason and link it your client\u2019s type.</li> </ul>"},{"location":"resources/cheatsheets/my-business-model-canvas/#3-customer-relationships","title":"3. Customer relationships","text":"<p>Customer relationships describes the type of relationship a company establishes with it\u2019s specific customer segments. Customer relationships are driven by customer acquisition, customer retention, and boosting sales \u2013 in other words you need to get, keep, and grow your customer relationships.</p> <ul> <li>Capture: List at least three the activities to gain clients</li> <li>Retention: list the activities to mantain clients</li> <li>Growth: list the activities to offer more servicies to your clients</li> </ul> <p>In this section, it would be helpful to estimate the conversion rates from these activities.</p>"},{"location":"resources/cheatsheets/my-business-model-canvas/#4-channels","title":"4. Channels","text":"<p>Channels are ways for you to reach your Customer Segments. And remember that in the initial stages it\u2019s important not to think about scale but to focus on learning. With that in mind try to think which channels will give you enough access to your Customer Segments at the same time give you enough learning. Channels can be email, social, CPC ads, blogs, articles, trade shows, radio &amp; TV, webinars etc. and BTW you don\u2019t have to be on all of them, just where your Customer Segments are.</p> <p>Channels: Describe 2-3 channels to provide your value proposition.</p> <ul> <li>By form: directs and indirects</li> <li>By type: producer, wholesaler, retailer, and consumer.</li> </ul> <p>Remember that one more channel implies one more cost too.</p>"},{"location":"resources/cheatsheets/my-business-model-canvas/#5-revenue-streams","title":"5. Revenue streams","text":"<p>How you price your business will depend on the type of model it is, however, it\u2019s quite common for startups to lower their cost, even offer it for free to gain traction, however, this can pose a few problems. The key being it actually delays/avoids validation. Getting people to sign up for something for free is a lot different than asking them to pay. There is also the idea of perceived value.</p> <ul> <li> <p>Revenue model: Describe your revenue model type like pay per use, license, renting, asset sale, advertising, subscription...</p> </li> <li> <p>Price estimation: 3 alternatives</p> <ol> <li>By cost-benefit</li> <li>By competitors</li> <li>By demand curve</li> </ol> </li> </ul>"},{"location":"resources/cheatsheets/my-business-model-canvas/#6-key-activities","title":"6. Key activities","text":"<p>These are really the activities that you do on a daily basis and what you need to understand is your strengths and weaknesses to which you can play on. These may include sales and marketing, research and development, manufacturing or distribution. Perhaps if you were an app development or software company your core focus should be to program and develop.</p> <p>Define this section with action verbs. Find the key activities to provide your value proposition to your customers. Ex: Develop an app, Website construction, customer service, software development, etc.</p> <p>Focus on those essential activities that enable your value proposition.</p>"},{"location":"resources/cheatsheets/my-business-model-canvas/#7-key-resources","title":"7. Key resources","text":"<p>This is really the part that explains how you will create your value proposition. Have a think about what types of products, services, assets you may need to create your value proposition. This often includes intellectual property, talent, and infrastructure. Perhaps you have a website that is unique, perhaps you have offices or property that are very unique.</p> <p>Elements needed to provide your value proposition. 4 types:</p> <ul> <li>Physical resources: Computers, offices, stores, etc.</li> <li>Intellectual resources: Licenses, contracts, patents, etc.</li> <li>Human resources: Customer service, designers, etc.</li> <li>Liquid assets</li> </ul>"},{"location":"resources/cheatsheets/my-business-model-canvas/#8-key-partners","title":"8. Key partners","text":"<p>Define your strategic allies essential for your business model. Ex: distribution, shipping, hosting provider, software development, marketing, customer service, etc.</p> <p>4 Different partner types:\u00e7</p> <ul> <li>Buyer-Supplier partnership</li> <li>Coopetition and partnering</li> <li>Strategic Alliance</li> <li>Complementary partnering</li> </ul>"},{"location":"resources/cheatsheets/my-business-model-canvas/#9-cost-structure","title":"9. Cost structure","text":"<p>Here you should list all the operational costs for taking this business to market. How much will it cost to build your idea? What is your burn rate \u2014 your total monthly running costs? How much will it cost to interview your customer segment? How much do market research papers cost? etc. You can then use these costs and potential revenue streams to calculate a rough break-even point.</p> <ul> <li>Key activities costs (internal): App development, design, customer service, shiping etc.</li> <li>Key resources costs (internal): Office space, warehouse, computers, licenses, designers, etc.</li> <li>Key partners costs (external): Shipping, providers, distribution, customer services.</li> </ul>"},{"location":"resources/cheatsheets/my-business-model-canvas/#references-and-links","title":"References and links","text":"<ul> <li>steve_mullen, An Introduction to Lean Canvas.</li> <li>Ries, E. (2011) personal interview. 2011</li> <li>More articles like this here: Resources</li> </ul>"},{"location":"resources/cheatsheets/my-docker-cheatsheet/","title":"Docker cheatsheet","text":"<p>With this post, I wanted to share my latest Docker cheatsheet and explain some basics I founded so useful.</p>"},{"location":"resources/cheatsheets/my-docker-cheatsheet/#the-docker-cheatsheet","title":"The Docker cheatsheet","text":"<p>Download the cheatsheet</p>"},{"location":"resources/cheatsheets/my-docker-cheatsheet/#what-is-docker","title":"What is docker?","text":"<p>\"Docker is a set of platform as a service (PaaS) products that use OS-level virtualization to deliver software in packages called containers. Containers are isolated from one another and bundle their own software, libraries and configuration files; they can communicate with each other through well-defined channels. All containers are run by a single operating system kernel and therefore use fewer resources than virtual machines.\" wikipedia)</p> <p>All this sounds a lot to a virtual machine, so what is the main difference? Containers and virtual machines have similar resource isolation and allocation benefits, but function differently because containers virtualize the operating system instead of hardware. Containers are more portable and efficient.</p> <ul> <li> <p>CONTAINERS</p> <p></p> <p>Containers are an abstraction at the app layer that packages code and dependencies together. Multiple containers can run on the same machine and share the OS kernel with other containers, each running as isolated processes in user space. Containers take up less space than VMs (container images are typically tens of MBs in size), can handle more applications and require fewer VMs and Operating systems.</p> </li> <li> <p>VIRTUAL MACHINES</p> <p></p> <p>Virtual machines (VMs) are an abstraction of physical hardware turning one server into many servers. The hypervisor allows multiple VMs to run on a single machine. Each VM includes a full copy of an operating system, the application, necessary binaries and libraries - taking up tens of GBs.</p> </li> </ul> <p>Pictures from Docker official website.</p>"},{"location":"resources/cheatsheets/my-docker-cheatsheet/#how-to-create-a-dockerfile","title":"How to create a Dockerfile","text":"<p>A Dockerfile is a text document that contains all the commands a user could call on the command line to assemble an image. Using docker build users can create an automated build that executes several command-line instructions in succession. </p> <p>Here you can see an example of a real and working Dockerfile. In this example, we create a container with a small Debian package and Python installed.</p> <pre><code># Select a base image\nFROM python:3.8-slim\n\n# Install python &amp; git\nRUN apt-get update -y &amp;&amp; apt-get -y install git-core\\\n    python3-pip \\\n    nano\n\n# Copy files needed to the container\nCOPY code /app_folder/code\nCOPY data /app_folder/data\nCOPY requirements.txt /app_folder\n\n# Install requirements\nRUN pip3 install -r /app_folder/requirements.txt\n\n# Select working directory inside the container\nWORKDIR /app_folder\n\n# Running the app\nCMD [ \"python\", \"./code/app.py\" ]\n</code></pre>"},{"location":"resources/cheatsheets/my-docker-cheatsheet/#how-to-build-and-play-with-the-container","title":"How to build and play with the container","text":"<p>Once you have a Dockerfile in the main folder, containerize an application is really easy.</p> <ul> <li> <p>Build an image</p> <p>The first thing we need to do is to create the image, you can do with this command:</p> <pre><code>docker build --tag myImage .\n</code></pre> <p>In this case, we use --tag in options to assign a name to the image, once done that docker starts building the image from the Dockerfile. To make sure the image was created you can do:</p> <pre><code>docker images\n</code></pre> <p>And you will see a list with all the images created.</p> </li> <li> <p>Run a container</p> <p>To run a container the first thing to do is create it from the image, to do so we can use this command.</p> <pre><code>docker run -it --name myContainer myimage\n</code></pre> <p>In this case, we use the option -it to run the container on interactive mode with tty active, and we assign a name to this container.</p> </li> <li> <p>Accessing to a running container</p> <p>Sometimes, we lunch a container but it's running in detached mode so we are not able to interact with it. We can access again to that container by the command exec:</p> <pre><code>docker exec -it myContainer bin/bash\n</code></pre> <p>With this command, we are running the process \"bin/bash\" inside the running container.</p> </li> </ul>"},{"location":"resources/cheatsheets/my-docker-cheatsheet/#other-references-and-links","title":"Other references and links","text":"<ul> <li>Docker documentation</li> <li>More articles like this here: Resources</li> <li>Github repository: cheatsheets</li> </ul>"},{"location":"resources/cheatsheets/my-git-cheatsheet/","title":"Git cheatsheet","text":"<p>With this post, I wanted to share my latest Git cheatsheet, with my most used commands and common workflows used at my daily projects.</p>"},{"location":"resources/cheatsheets/my-git-cheatsheet/#the-git-cheat-sheet","title":"The Git cheat sheet","text":"<p>Download the cheatsheet</p>"},{"location":"resources/cheatsheets/my-git-cheatsheet/#what-is-git","title":"What is Git?","text":"<p>\"Git is software for tracking changes in any set of files, usually used for coordinating work among programmers collaboratively developing source code during software development. Its goals include speed, data integrity, and support for distributed, non-linear workflows (thousands of parallel branches running on different systems).</p> <p>Git was created by Linus Torvalds in 2005 for development of the Linux kernel, with other kernel developers contributing to its initial development. Since 2005, Junio Hamano has been the core maintainer. As with most other distributed version control systems, and unlike most client\u2013server systems, every Git directory on every computer is a full-fledged repository with complete history and full version-tracking abilities, independent of network access or a central server. Git is free and open-source software distributed under the GPL-2.0-only license.\" wikipedia</p>"},{"location":"resources/cheatsheets/my-git-cheatsheet/#common-git-workflows","title":"Common Git workflows","text":"<p>In this section I wanted to post common workflow actions, I usually do in my daily work.</p>"},{"location":"resources/cheatsheets/my-git-cheatsheet/#bring-changes-from-other-branch-onto-current","title":"Bring changes from other branch onto current","text":"<p>Here is how you would transplant a topic branch to another, to pretend that you forked the topic branch from the latter branch, using rebase --onto.</p> <p></p> <p>Steps:</p> <ul> <li> <p>Run the git branch command to make sure you are at the branch that you want to update.</p> <pre><code>git branch\n</code></pre> Output<pre><code>develop \n* master \n</code></pre> </li> <li> <p>Run a git Fetch to update changes available in the remote repository.</p> <pre><code>git fetch\n</code></pre> </li> <li> <p>Rebase the master branch.</p> <pre><code>git rebase master --onto develop\n</code></pre> Output<pre><code>Successfully rebased and updated refs/heads/master.\n</code></pre> </li> <li> <p>Push the new changes on the master branch.</p> <pre><code>git push\n</code></pre> </li> </ul>"},{"location":"resources/cheatsheets/my-git-cheatsheet/#discard-local-changes","title":"Discard local changes","text":"<p>When you change code in one or more files locally, and you realize you want to go back to the latest repository state, the remote state.</p> <ul> <li> <p>First we confirm there aren't unstaged files.</p> <pre><code>git status\n</code></pre> output<pre><code>On branch main\nYour branch is up to date with 'origin/main'.\n\nnothing to commit, working tree clean\n</code></pre> </li> <li> <p>[Optional] If there are unstaged files we run the next command.</p> <pre><code>git reset\n</code></pre> output<pre><code>Unstaged changes after reset:\nM       PythonGenerators.ipynb\n</code></pre> </li> <li> <p>Next, we overwrite the local changes. We can add --FileName to select a specific file.</p> <pre><code>git checkout\n</code></pre> output<pre><code>M       PythonGenerators.ipynb\nYour branch is up to date with 'origin/main'.\n</code></pre> </li> <li> <p>Finally we discard the local changes to all files.</p> <pre><code>git reset --hard\n</code></pre> output<pre><code>HEAD is now at 9fbe555 all commits except first\n</code></pre> </li> </ul>"},{"location":"resources/cheatsheets/my-git-cheatsheet/#unify-commits-into-one","title":"Unify commits into one","text":"<p>For this case I pushed 3 commits due to some mistakes until I finlly had the correct files. To clean the last three commits and make them into one you can do the following.</p>"},{"location":"resources/cheatsheets/my-git-cheatsheet/#the-easy-way","title":"The easy way","text":"<ul> <li> <p>Get the lastest commit logs.</p> <pre><code>git log --oneline --graph --decorate\n</code></pre> output<pre><code>* be501b6 (HEAD -&gt; main, origin/main) Files ready  \n* 3f12dce Ups! Readme Updated  \n* ae11087 License and Readme added  \n* 2017231 Init files  \n</code></pre> </li> <li> <p>Squash until commit hash 2017231 Init files (not included).</p> <pre><code>git reset --soft 2017231\n</code></pre> output<pre><code>No output\n</code></pre> </li> <li> <p>Overwrite local changes</p> <pre><code>git commit -m \"License &amp; Readme added\"\n</code></pre> output<pre><code>[main 92c87b4] License &amp; Readme added\n41 files changed, 816 insertions(+)\n...\n</code></pre> </li> <li> <p>Push the local changes</p> <pre><code>git push --force\n</code></pre> output<pre><code>4 files changed, 121 insertions(+), 53 deletions(-)\n</code></pre> </li> </ul>"},{"location":"resources/cheatsheets/my-git-cheatsheet/#the-manual-way","title":"The manual way","text":"<ul> <li> <p>Get the lastest commit logs.</p> <pre><code>git log --oneline --graph --decorate\n</code></pre> output<pre><code>* be501b6 (HEAD -&gt; main, origin/main) Files ready  \n* 3f12dce Ups! Readme updated  \n* ae11087 License and Readme added  \n* 2017231 Init files  \n</code></pre> </li> <li> <p>Squash the commits by editor using git rebase.</p> <pre><code>git rebase -i 2017231\n</code></pre> </li> <li> <p>After git rebase, you'll see an editor like the output shown. With the selected commits and a list of commands. *Press \"i\" to start editing the file</p> <pre><code>pick 92c87b4 License &amp; Readme added\npick f96f4e5 Ups! License updated\npick 954518c Files ready\n\n# Rebase 2017231..954518c onto 2017231 (3 commands)\n...\n</code></pre> </li> <li> <p>Change the pick commits to stash commits you can add an \"s\" at the begining</p> <pre><code>pick 92c87b4 License &amp; Readme added\ns f96f4e5 Ups! License updated\ns 954518c Files ready\n\n# Rebase 2017231..954518c onto 2017231 (3 commands)\n...\n</code></pre> </li> <li> <p>At the next screen you can type the new commit message for the combination of the three commits. Delete the old comments and type the new one.</p> <pre><code># This is a combination of 3 commits.\n# My final commit message\nLicense &amp; Readme added\n\n# Please enter the commit message for your changes. Lines starting\n# with '#' will be ignored, and an empty message aborts the commit.\n</code></pre> </li> <li> <p>To check your new commit combination, run a git log.</p> <pre><code>git log --oneline --graph --decorate\n</code></pre> output<pre><code>* ec8cc95 (HEAD -&gt; main) License &amp; Readme added \n* 2017231 Init files  \n</code></pre> </li> </ul>"},{"location":"resources/cheatsheets/my-git-cheatsheet/#other-references-and-links","title":"Other references and links","text":"<ul> <li>Git Project</li> <li>More articles like this here: Resources</li> </ul>"},{"location":"resources/cheatsheets/my-letter-cases-reminder/","title":"Letter cases reminder","text":"<p>With this post, I would like to clarify something that we use every day when we name files, folders, emails, etc. The Letter Cases. Letter case (or just case) is the distinction between the letters that are in larger uppercase or capitals and smaller lowercase in the written representation of certain languages. In computer programming are a set of rules to be used for identifiers. That denote variables, types, functions, files, directories, and other entities.</p>"},{"location":"resources/cheatsheets/my-letter-cases-reminder/#most-popular-letter-cases","title":"Most popular letter cases","text":"Case Example Description Flat case - lettercase- flatcase Spaces and punctuation are removed and all the letters are lowercase. Camel case - letterCase- camelCase Spaces and punctuation are removed and the first letter of each word after the first one is capitalized. Pascal case - LetterCase- PascalCase Spaces and punctuation are removed and the first letter of each word after the first one is capitalized. Snake case - letter_case- snake_case Punctuation is removed and spaces are replaced by single underscores. Normally the letters share the same case. Kebab case - letter-case- kebab-case Similar to snake case, except hyphens rather than underscores are used to replace spaces. Doner case - letter|case- doner|case Punctuation is removed and spaces are replaced by pipes. Very common as an OR symbol. Train case - Letter-Case- Train-Case Like the kebab case but every word is capitalized. Upper flat case - LETTERCASE- UPPERFLATCASE Spaces and punctuation are removed and all the letters are capitalized. Macro case - LETTER_CASE- MACRO_CASE Spaces and punctuation are replaced by single underscores, and all letters are capitalized. Cobol case - LETTER-CASE- COBOL-CASE Spaces and punctuation are replaced by hyphens and all letters are capitalized. Studly caps - LeTTeRCaSe- stUdLYcApS Mixed case with no significance to the use of the capitals. Sometimes only vowels are upper case, but often it is simply random."},{"location":"resources/cheatsheets/my-letter-cases-reminder/#my-favorite-letter-case","title":"My favorite letter case","text":""},{"location":"resources/cheatsheets/my-letter-cases-reminder/#pascalcase-snake_case","title":"PascalCase + snake_case","text":"<p>A mix between Pascal case and Snake case make the best combination for my daily tasks (coding, naming files, naming conventions for systems, etc.) The great advantage of using this mix allows the creation of multiple hierarchies in the same string.</p> <p>For example, you can create a naming convention using underscores for each level and Capitals two distinguish between words.</p> Structure  Index_FirstLevel_SecondLevel Example  01_ProjectName_TaskName"},{"location":"resources/cheatsheets/my-letter-cases-reminder/#main-letter-cases","title":"Main letter cases","text":""},{"location":"resources/cheatsheets/my-letter-cases-reminder/#flat-case","title":"Flat case","text":"<p>Example  lettercase</p> <p>Spaces and punctuation are removed, and all letters are lowercase. This format is straightforward, with no capitalization or special characters, making it useful for simple, uniform naming conventions. Commonly used in contexts where minimalism and simplicity are key, such as in certain coding environments or database fields.</p>"},{"location":"resources/cheatsheets/my-letter-cases-reminder/#camel-case-or-dromedary-case","title":"Camel case or Dromedary case","text":"<p>Example  letterCase</p> <p>Spaces and punctuation are removed and the first letter of each word is capitalised. When the first letter of the first word is lowercase (\"iPod\", \"eBay\"...), the case is usually known as lower camel case or dromedary case (illustratively: dromedaryCase). This format has become popular in the branding of information technology products and services.</p>"},{"location":"resources/cheatsheets/my-letter-cases-reminder/#pascal-case","title":"Pascal case","text":"<p>Example  LetterCase</p> <p>Spaces and punctuation are removed and the first letter of each word after the first one is capitalised. If this includes the first letter of the first word, the case is sometimes called upper camel case (or, illustratively, CamelCase), Pascal case, or bumpy case.</p>"},{"location":"resources/cheatsheets/my-letter-cases-reminder/#snake-case","title":"Snake case","text":"<p>Example  letter_case</p> <p>Punctuation is removed and spaces are replaced by single underscores. Normally the letters share the same case (e.g. \"UPPER_CASE_EMBEDDED_UNDERSCORE\" or \"lower_case_embedded_underscore\") but the case can be mixed, as in OCaml modules. The style may also be called pothole case, especially in Python programming, in which this convention is often used for naming variables. Illustratively, it may be rendered snake_case, pothole_case, etc.</p>"},{"location":"resources/cheatsheets/my-letter-cases-reminder/#kebab-case","title":"Kebab case","text":"<p>Example  letter-case</p> <p>Similar to snake case, above, except hyphens rather than underscores are used to replace spaces. It is also known as spinal case, param case, Lisp case, and dash case (or illustratively as kebab-case). If every word is capitalised, the style is known as train case (TRAIN-CASE)</p>"},{"location":"resources/cheatsheets/my-letter-cases-reminder/#doner-case","title":"Doner case","text":"<p>Example  letter|case</p> <p>Spaces and punctuation are removed and replaced by pipes (|). This case is often used to denote alternatives or to visually separate words in a manner similar to an OR symbol. It provides a unique and less common alternative to other delimiter-based cases, offering a clear visual distinction in contexts where options or separations need to be highlighted.</p>"},{"location":"resources/cheatsheets/my-letter-cases-reminder/#train-case","title":"Train case","text":"<p>Example  Letter-Case</p> <p>A mix between Camel case and Kebab case. spaces are replaced by single hyphens, punctuation are removed and the first letter of each word is capitalised.</p>"},{"location":"resources/cheatsheets/my-letter-cases-reminder/#upper-flat-case","title":"Upper flat case","text":"<p>Example  LETTERCASE</p> <p>Spaces and punctuation are removed, and all letters are capitalized. This format is bold and prominent, often used in scenarios where emphasis and visibility are crucial, such as in acronyms or constant values in programming. It is similar to flat case but with the added impact of uppercase letters, making it stand out more in textual content.</p>"},{"location":"resources/cheatsheets/my-letter-cases-reminder/#macro-case","title":"Macro case","text":"<p>Example  LETTER_CASE</p> <p>Punctuation is removed, all the letters are capitalised and spaces are replaced by single underscores. It may be referred to as screaming snake case (or SCREAMING_SNAKE_CASE) or hazard case.</p>"},{"location":"resources/cheatsheets/my-letter-cases-reminder/#cobol-case","title":"Cobol case","text":"<p>Example  LETTER-CASE</p> <p>Punctuation is removed, every word is capitalised and spaces are replaced by single hyphens. Named from the COBOL programming language. COBOL is primarily used in business, finance, and administrative systems for companies and governments.</p>"},{"location":"resources/cheatsheets/my-letter-cases-reminder/#studly-caps","title":"Studly caps","text":"<p>Example  LeTTeR CaSe</p> <p>Mixed case with no semantic or syntactic significance to the use of the capitals. Sometimes only vowels are upper case, at other times upper and lower case are alternated, but often it is simply random. The name comes from the sarcastic or ironic implication that it was used in an attempt by the writer to convey their own coolness. It is also used to mock the violation of standard English case conventions by marketers in the naming of computer software packages, even when there is no technical requirement to do so \u2013 e.g., Sun Microsystems' naming of a windowing system NeWS. Illustrative naming of the style is, naturally, random: stUdlY cAps, StUdLy CaPs, etc.</p>"},{"location":"resources/cheatsheets/my-letter-cases-reminder/#references-and-links","title":"References and links","text":"<ul> <li>Patrick Divine, Case Styles: Camel, Pascal, Snake, and Kebab Case.</li> <li>WikiProject Computer science, Wikipedia. Letter case.</li> <li>WikiProject Computer science, Wikipedia. Naming convention (programming)</li> <li>My Resources</li> </ul>"},{"location":"resources/cheatsheets/my-linux-cheatsheet/","title":"Linux cheatsheet","text":"<p>With this post, I wanted to share my latest GNU Bash cheatsheet, with my most used commands at my daily projects.</p> <p></p>"},{"location":"resources/cheatsheets/my-linux-cheatsheet/#download-the-cheat-sheet","title":"DOWNLOAD THE CHEAT SHEET:","text":"<p>Download the cheatsheet</p>"},{"location":"resources/cheatsheets/my-linux-cheatsheet/#what-is-linux-bash","title":"What is Linux-Bash?","text":"<p>\"Bash is a Unix shell and command language written by Brian Fox for the GNU Project as a free software replacement for the Bourne shell. First released in 1989, it has been used as the default login shell for most Linux distributions. Bash was also the default shell in all versions of Apple macOS prior to the 2019 release of macOS Catalina, which changed the default shell to zsh, although Bash currently remains available as an alternative shell.</p> <p>Bash is a command processor that typically runs in a text window where the user types commands that cause actions. Bash can also read and execute commands from a file, called a shell script.\" wikipedia</p>"},{"location":"resources/cheatsheets/my-linux-cheatsheet/#other-references-and-links","title":"Other references and links","text":"<ul> <li>GNU Project</li> <li>More articles like this here: Resources</li> <li>Github repository: cheatsheets</li> </ul>"},{"location":"resources/cheatsheets/my-python-3-cheatsheet/","title":"Python 3 cheatsheet","text":"<p>I wanted to post my Python3 cheat sheet and some useful functions I use almost every day. These functions are no libraries needed making them easier to implement. As far as I learn more programming languages I'll create more cheat sheets, hope they'll be helpful.</p> <p></p>"},{"location":"resources/cheatsheets/my-python-3-cheatsheet/#download-the-cheat-sheet","title":"DOWNLOAD THE CHEAT SHEET:","text":"<p>Download the cheatsheet</p>"},{"location":"resources/cheatsheets/my-python-3-cheatsheet/#usefull-scripts","title":"Usefull scripts:","text":""},{"location":"resources/cheatsheets/my-python-3-cheatsheet/#method-sort-by-key-list","title":"Method: sort by key list","text":"<p>This function sort two list of the same lenght taking the first one as the key-list and making the second one dependent. This codes works only with two same size lists.</p>"},{"location":"resources/cheatsheets/my-python-3-cheatsheet/#method","title":"Method","text":"<pre><code>def sorter(lst, elements):\n    xy = zip(lst, elements)\n    lst_sorted = [x for x, y in sorted(xy)]\n    xy = zip(lst, elements)\n    elements_sorted = [y for x, y in sorted(xy)]\n    return lst_sorted, elements_sorted\n</code></pre>"},{"location":"resources/cheatsheets/my-python-3-cheatsheet/#example","title":"Example","text":"<pre><code>keys = [1, 3, 2, 6, 4, 8, 5, 10]\nelements = ['dog', 'cat', 'monkey', 'lizard', 'eagle', 'spyder', 'snake', 'wolf']\n\n# Calling the function\nsorter(keys, elements)\n</code></pre> output<pre><code>([1, 2, 3, 4, 5, 6, 8, 10],\n['dog', 'monkey', 'cat', 'eagle', 'snake', 'lizard', 'spyder', 'wolf'])\n</code></pre>"},{"location":"resources/cheatsheets/my-python-3-cheatsheet/#method-entwine-elements-of-two-lists","title":"Method: entwine elements of two lists","text":"<p>This function takes two lists and interspersing all elements, starting by the first list element. For this function the same size lists are no needed.</p>"},{"location":"resources/cheatsheets/my-python-3-cheatsheet/#method_1","title":"Method:","text":"<pre><code>def entwine(lst1, lst2):\n    zipped = []\n    a = max([len(lst1), len(lst2)])\n    for i in range(a):\n        try:\n            zipped.append(lst1[i])\n        except:\n            pass\n        try:\n            zipped.append(lst2[i])\n        except:\n            pass\n    return zipped\n</code></pre>"},{"location":"resources/cheatsheets/my-python-3-cheatsheet/#example_1","title":"Example","text":"<pre><code>lst_1 = ['alpha', 'bravo', 'charlie', 'delta', 'echo']\nlst_2 = [1, 2, 5, 12, 18, 0]\n\nentwine(lst_1, lst_2)\n</code></pre> output<pre><code>    ['alpha', 1, 'bravo', 2, 'charlie', 5, 'delta', 12, 'echo', 18, 0]\n</code></pre>"},{"location":"resources/cheatsheets/my-python-3-cheatsheet/#method-counter-items","title":"Method: counter items","text":"<p>This function counts each item of a list and outputs providing the frequency by item.</p>"},{"location":"resources/cheatsheets/my-python-3-cheatsheet/#method_2","title":"Method","text":"<pre><code>def counter(lst):\n    output = {}\n    for value in lst:\n        count = len([i for i in lst if i == value])\n        output[value] = count\n    return output\n</code></pre>"},{"location":"resources/cheatsheets/my-python-3-cheatsheet/#example_2","title":"Example","text":"<pre><code>lst = ['A', 'C', 'D', 'A', 'B', 'C', 'D', 'A']\n\ngrooping(lst)\n</code></pre> Output<pre><code>    {'A': 3, 'C': 2, 'D': 2, 'B': 1}\n</code></pre>"},{"location":"resources/cheatsheets/my-python-3-cheatsheet/#method-permutate-elements-of-a-list","title":"Method: permutate elements of a list","text":"<p>This function resolve all the possible permutations from a list with no elements repeated.</p>"},{"location":"resources/cheatsheets/my-python-3-cheatsheet/#method_3","title":"Method","text":"<pre><code>def permute(lst):\n    combi = []\n    for i in range(len(lst)):\n        for x in range(i, len(lst)):\n            if lst[i] != lst[x]:\n                combi.append([lst[i],lst[x]])\n    return combi\n</code></pre>"},{"location":"resources/cheatsheets/my-python-3-cheatsheet/#example_3","title":"Example","text":"<pre><code>words = ['alpha', 'bravo', 'charlie']\n\npermute(words)\n</code></pre> Output<pre><code>    [['alpha', 'bravo'], ['alpha', 'charlie'], ['bravo', 'charlie']]\n</code></pre>"},{"location":"resources/cheatsheets/my-python-3-cheatsheet/#method-invert-dictionary","title":"Method: invert dictionary","text":"<p>This function invert a dictionary switching keys with values.</p>"},{"location":"resources/cheatsheets/my-python-3-cheatsheet/#method_4","title":"Method","text":"<pre><code>def invert_dictionary(dictionary):\n    all_dcts = []\n    for k, v in dictionary.items():\n        temp_dct = {}\n        for token in v:\n            temp_dct[token] = [k]\n        all_dcts.append(temp_dct)\n    inverted_dictionary = all_dcts[0]\n    for dct in all_dcts[1:]:\n        for k, v in dct.items():\n            if k in inverted_dictionary.keys():\n                inverted_dictionary[k] = inverted_dictionary[k] + v\n            else:\n                inverted_dictionary[k] = v\n    return inverted_dictionary\n</code></pre>"},{"location":"resources/cheatsheets/my-python-3-cheatsheet/#example_4","title":"Example","text":"<pre><code>dictionary = {'a' : [1, 5, 9, 8], 'b' : [2, 3, 8, 0], 'c' : [8, 9, 0, 8]}\n\ninvert_dictionary(dictionary)\n</code></pre> output<pre><code>{1: ['a'],\n 5: ['a'],\n 9: ['a', 'c'],\n 8: ['a', 'b', 'c'],\n 2: ['b'],\n 3: ['b'],\n 0: ['b', 'c']}\n</code></pre>"},{"location":"resources/cheatsheets/my-python-3-cheatsheet/#other-links","title":"OTHER LINKS","text":"<ul> <li>Github repository: cheatsheets</li> <li>More resources like this here: https://carlosgrande.me/category/resources</li> </ul>"},{"location":"resources/cheatsheets/my-python-project-cheatsheet/","title":"Python project cheatsheet","text":"<p>There is no one \"right\" way to structure a Python project, as the appropriate structure can depend on the specific needs and goals of the project. However, here are some general guidelines that you might find helpful when organizing your Python project files.</p>"},{"location":"resources/cheatsheets/my-python-project-cheatsheet/#download-the-cheatsheet","title":"DOWNLOAD THE CHEATSHEET:","text":"<p>Download the cheatsheet</p> <p>Visit my post PyTemplate to use my template for general python developers. Recommended for DevOps &amp; Data Science projects. PyTemplate follows a customizable project structure using cookiecutter as the template generator.</p> <p>When it comes to organizing a Python project, \"structure\" refers to the decisions you make about how to best achieve the goals of your project. Proper project structure is important because it helps to ensure that your code is clean, well-organized, and easy to understand. In order to create an effective project structure, it is important to consider how to leverage Python's features to create clean, efficient code, as well as how to organize your files and folders in the filesystem. By taking the time to carefully plan and structure your project, you can save time and effort in the long run and make it easier for you and others to work on and maintain your code.</p>"},{"location":"resources/cheatsheets/my-python-project-cheatsheet/#1-the-directory-layout","title":"1. The directory layout","text":"<p>The appropriate structure for a particular project will depend on the specific needs and goals of that project. However, here are some common tree file structures that you might find useful when organizing your Python project:</p> <ol> <li>Single module structure. This is a simple structure that is suitable for small projects that only have one module (i.e. a single .py file). The structure might look something like this:</li> </ol> <pre><code>my_project/\n\u251c\u2500\u2500 code_of_conduct.md\n\u251c\u2500\u2500 LICENSE\n\u251c\u2500\u2500 my_module.py\n\u251c\u2500\u2500 README.md\n\u2514\u2500\u2500 requirements.txt\n\u2514\u2500\u2500 ...\n</code></pre> <ol> <li>Package structure. This structure is suitable for larger projects that consist of multiple modules and packages. It might look something like this:</li> </ol> <pre><code>my_project/\n\u251c\u2500\u2500 my_package/\n\u2502      \u251c\u2500\u2500 config/\n\u2502      \u2502       \u251c\u2500\u2500 settings.yaml\n\u2502      \u2502       \u2514\u2500\u2500 additional_settings.yaml\n\u2502      \u251c\u2500\u2500 __init__.py\n\u2502      \u251c\u2500\u2500 main.py\n\u2502      \u251c\u2500\u2500 module_1.py\n\u2502      \u251c\u2500\u2500 module_2.py\n\u2502      \u251c\u2500\u2500 ...\n\u251c\u2500\u2500 tests/\n\u2502      \u251c\u2500\u2500 test_module_1.py\n\u2502      \u251c\u2500\u2500 test_module_2.py\n\u2502      \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 data/\n\u2502      \u251c\u2500\u2500 input.csv\n\u2502      \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 docs/\n\u2502      \u251c\u2500\u2500 index.md\n\u2502      \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 requirements.txt\n\u251c\u2500\u2500 LICENSE\n\u2514\u2500\u2500 ...\n</code></pre> <ol> <li>Monolithic structure. This is a more complex structure that is suitable for large projects that have multiple packages and modules, as well as a variety of supporting files and resources. It might look something like this:</li> </ol> <pre><code>my_project/\n\u251c\u2500\u2500 src/\n\u2502      \u251c\u2500\u2500 package_1/\n\u2502      \u2502      \u251c\u2500\u2500 config/\n\u2502      \u2502      \u2502        \u2514\u2500\u2500 settings.yaml\n\u2502      \u2502      \u251c\u2500\u2500 __init__.py\n\u2502      \u2502      \u251c\u2500\u2500 module_1.py\n\u2502      \u2502      \u251c\u2500\u2500 module_2.py\n\u2502      \u2502      \u2514\u2500\u2500 ...\n\u2502      \u251c\u2500\u2500 package_2/\n\u2502      \u2502      \u251c\u2500\u2500 config/\n\u2502      \u2502      \u2502        \u2514\u2500\u2500 settings.yaml\n\u2502      \u2502      \u251c\u2500\u2500 __init__.py\n\u2502      \u2502      \u251c\u2500\u2500 module_3.py\n\u2502      \u2502      \u251c\u2500\u2500 module_4.py\n\u2502      \u2502      \u2514\u2500\u2500 ...\n\u2502      \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 tests/\n\u2502      \u251c\u2500\u2500 tests_package_1/\n\u2502      \u2502      \u251c\u2500\u2500 test_module_1.py\n\u2502      \u2502      \u251c\u2500\u2500 test_module_2.py\n\u2502      \u2502      \u2514\u2500\u2500 ...\n\u2502      \u251c\u2500\u2500 tests_package_2/\n\u2502      \u2502      \u251c\u2500\u2500 test_module_3.py\n\u2502      \u2502      \u251c\u2500\u2500 test_module_4.py\n\u2502      \u2502      \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 data/\n\u2502      \u251c\u2500\u2500 input.csv\n\u2502      \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 docs/\n\u2502      \u251c\u2500\u2500 index.md\n\u2502      \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 requirements.txt\n\u251c\u2500\u2500 LICENSE\n\u2514\u2500\u2500 ...\n</code></pre> <p>Remember, the key is to choose a structure that makes sense for your project and is easy for you and other users to understand. You can always refactor your project's structure as it evolves to better suit its needs.</p>"},{"location":"resources/cheatsheets/my-python-project-cheatsheet/#2-naming-the-project-and-the-package","title":"2. Naming the project and the package","text":"<p>There are no strict rules for naming a Python project or package, but there are some best practices that you can follow to choose meaningful and descriptive names.</p>"},{"location":"resources/cheatsheets/my-python-project-cheatsheet/#21-the-project-name","title":"2.1 The project name","text":"<p>There are no strict rules for naming a coding project, but there are some best practices that you can follow to choose a descriptive and meaningful name:</p> <ul> <li>Choose a name that is descriptive and reflects the purpose of your project. Avoid using vague or generic names, as they may not be meaningful to other users.</li> <li>Use lowercase letters and separate words with hyphens. For example, a project called \"my-project\" would be easier to read and understand than \"MyProject\" or \"my_project\".</li> <li>Avoid using reserved words or names that are already used by GitHub or other projects. You can check the list of reserved words on GitHub to see which names are not allowed.</li> <li>Consider using a name that is unique within the GitHub ecosystem. This can help prevent conflicts with other projects that might have the same name.</li> </ul> <p>Remember, the key is to choose a name that is descriptive, meaningful, and easy to understand. This will make it easier for other users to find and understand your project on GitHub.</p>"},{"location":"resources/cheatsheets/my-python-project-cheatsheet/#22-the-package-name","title":"2.2 The package name","text":"<p>In Python, the term \"package\" refers to a directory of Python modules, while the term \"project\" refers to a collection of related packages and modules. The package name is the name of the directory that contains the Python modules, while the project name is the overall name of the collection of packages and modules.</p> <p>It is not required for the package name and the project name to be the same, although it is common for them to be similar. For example, you might have a project called \"my-project\" that consists of several packages, one of which is called \"my_package\". In this case, the project name is \"my-project\" and the package name is \"my_package\".</p> <p>One thing to keep in mind is that the package name should be unique within the Python ecosystem, since it will be used to import the package's modules. For this reason, it is a good idea to choose a package name that is descriptive and specific to your project, rather than a generic name that might be used by other packages.</p>"},{"location":"resources/cheatsheets/my-python-project-cheatsheet/#3-the-configuration-files","title":"3. The configuration files","text":"<p>Configuration files can be useful for storing data that is specific to your application or environment, such as database credentials or API keys, and can help you avoid hardcoding this information in your code. This can make it easier to deploy your application in different environments or to share your code with others.</p>"},{"location":"resources/cheatsheets/my-python-project-cheatsheet/#31-the-structure-layout","title":"3.1 The structure layout","text":"<p>It is generally a good practice to save configuration files in a separate folder within your Python package. One common convention is to create a \"config\" folder at the root of your package and store all configuration files there. For example, if your package is structured like this:</p> <p><pre><code>my_project/\n\u251c\u2500\u2500 my_package/\n\u2502      \u251c\u2500\u2500 ...\n\u2502      \u2514\u2500\u2500 config/\n\u2502              \u251c\u2500\u2500 settings.yaml\n\u2502              \u2514\u2500\u2500 additional_settings.yaml\n\u2514\u2500\u2500 ...\n</code></pre> Then you can access the configuration files from within your code using the following path: my_package/config/settings.yaml. This way, you can keep your configuration files separate from your code and easily import them when needed.</p> <p>Alternatively, you could also consider using a configuration management library such as PyYAML or configparser to manage your configuration files and easily access their contents from within your code.</p>"},{"location":"resources/cheatsheets/my-python-project-cheatsheet/#32-the-file-format","title":"3.2 The file format","text":"<p>The file format for storing configuration settings in a Python project, as the appropriate choice will depend on the specific requirements and goals of your project. Some common file formats that can be used for storing configuration data in Python include:</p> <ul> <li>.cfg: This is a simple and easy-to-understand format that is commonly used for storing configuration data. It consists of key-value pairs, with each line in the file representing a single setting.</li> <li>.yaml: This is a more flexible format that can represent a wider range of data types, such as lists and dictionaries. It is generally more human-readable than .cfg files, but requires an additional library (e.g. PyYAML) to parse and read the data.</li> <li>.json: This is a widely used format that is well-suited for storing structured data. It can represent a variety of data types, including lists and dictionaries, and is easy to parse using the built-in json library in Python.</li> </ul> <p>Ultimately, the choice of which format to use will depend on the specific needs of your project. If you need to store simple configuration data (e.g. strings, integers, and floating point values) and don't need any advanced features, then .cfg files may be sufficient. On the other hand, if you need to store more complex data structures or want more flexibility in your configuration format, then .yaml or .json files may be a better choice.</p>"},{"location":"resources/cheatsheets/my-python-project-cheatsheet/#4-the-project-documentation","title":"4. The project documentation","text":"<p>There are several ways to write documentation for a Python project, and the specific approach you choose will depend on the needs and goals of your project. Here are some general guidelines for writing and saving documentation in a Python project:</p> <ul> <li>Write documentation that is clear, concise, and easy to understand. This can include an overview of the project, instructions for installation and usage, and details about the project's architecture and design.</li> <li>Use a documentation generator tool to automatically create documentation based on the code and comments in your project. Popular documentation generators for Python include Sphinx, MkDocs, and Doxygen.</li> <li>Save your documentation in a dedicated folder within your project. A common convention is to use a folder called \"docs\" for this purpose.</li> <li>Consider including a README file at the root level of your project folder. This file should provide a high-level overview of your project and include any information that users might need to get started with your project, such as installation instructions and dependencies.</li> </ul> <p>Remember, the key is to provide clear and concise documentation that helps users understand and use your project. By taking the time to write and maintain good documentation, you can make it easier for others to understand and use your code.</p>"},{"location":"resources/cheatsheets/my-python-project-cheatsheet/#41-where-to-add-your-documentation","title":"4.1 Where to add your documentation","text":"<p>It is common to save documentation for a Python project in a dedicated folder within the project directory tree. A common convention is to use a folder called \"docs\" for this purpose. For example, your project directory tree might look something like this:</p> <pre><code>my_project/\n\u251c\u2500\u2500 my_package/\n\u2502      \u2514\u2500\u2500...\n\u251c\u2500\u2500 docs/\n\u2502      \u251c\u2500\u2500 index.md\n\u2502      \u2514\u2500\u2500 ...\n\u2514\u2500\u2500 ...\n</code></pre> <p>In this example, the \"docs\" folder contains the documentation for the project, which might include files such as a user manual, API reference, or installation instructions.</p> <p>It is also a good idea to include a README file at the root level of your project folder. This file should provide a high-level overview of your project and include any information that users might need to get started with your project, such as installation instructions and dependencies.</p> <p>Remember, the key is to keep your documentation organized and easy to find. By saving your documentation in a dedicated folder, you can make it easier for users to access and understand your project.</p>"},{"location":"resources/cheatsheets/my-python-project-cheatsheet/#5-additional-files","title":"5. Additional files","text":"<p>In addition to the code files, there are a number of other files and resources that you might want to include in a Python project. The specific files and resources that you include in your project will depend on the needs and goals of your project. The key is to include any necessary files and resources that will help users understand and use your project.</p>"},{"location":"resources/cheatsheets/my-python-project-cheatsheet/#51-the-readme-file","title":"5.1 The readme file","text":"<p>A README file is a text file that provides a high-level overview of a project and includes any information that users might need to get started with the project. It is typically located at the root level of a project directory and is used to introduce the project to users and provide them with the information they need to understand and use the project.</p> <p>The contents of a README file will vary depending on the specific needs and goals of the project, but it might include information such as:</p> <ul> <li>A description of the project and its purpose</li> <li>Installation instructions</li> <li>Dependencies and requirements</li> <li>Usage instructions</li> <li>Troubleshooting</li> <li>A disclaimer section</li> <li>Possible help wanted</li> <li>Links to documentation and other resources</li> </ul> <p>The README file is often the first thing that users will see when they encounter your project, so it is important to make it clear, concise, and easy to understand. By including all of the necessary information in the README file, you can make it easier for users to get started with your project and understand how to use it.</p>"},{"location":"resources/cheatsheets/my-python-project-cheatsheet/#52-the-license-file","title":"5.2 The license file","text":"<p>A LICENSE file is a text file that includes information about the terms and conditions under which a project is licensed. It specifies how users are allowed to use, modify, and distribute the code and resources of the project.</p> <p>Having a LICENSE file is important because it allows users to understand the legal terms that apply to the use of your project, and helps to protect your rights as the creator of the project. Without a LICENSE file, users may not be clear on what they are allowed to do with your code, and you may not have legal recourse if your code is used in ways that you did not intend.</p> <p>There are many different types of open source licenses available, each with its own specific terms and conditions. Some of the most common open source licenses include:</p> <ul> <li>MIT License: This is a permissive license that allows users to use, modify, and distribute the code and resources of the project as long as they include a copy of the license and attribute the original authors.</li> <li>Apache License: This is a permissive license that allows users to use, modify, and distribute the code and resources of the project as long as they include a copy of the license and attribute the original authors. It also includes a patent grant to protect users from patent lawsuits.</li> <li>GNU General Public License (GPL): This is a copyleft license that requires users to distribute any modifications they make to the code and resources of the project under the same license. This helps to ensure that the code remains freely available and open source.</li> </ul> <p>Ultimately, the choice of which license to use will depend on the specific needs and goals of your project. I recommend the Choose an open source license site to decide witch license fits best to your project. You can also learn more about open licenses at the open source initiative.</p>"},{"location":"resources/cheatsheets/my-python-project-cheatsheet/#53-the-code-of-conduct-file","title":"5.3 The code of conduct file","text":"<p>A code of conduct is a set of guidelines that outline the behavior that is expected of users and contributors to a project. It is often included in a CODE_OF_CONDUCT file within a project's repository.</p> <p>Having a code of conduct is important because it helps to create a positive and inclusive community around a project. It can help to establish clear expectations for behavior and ensure that all users and contributors feel welcome and respected.</p> <p>There are many different codes of conduct available, and the specific code that is right for your project will depend on the needs and goals of your community. Some common codes of conduct include:</p> <ul> <li>Contributor Covenant: This is a widely-used code of conduct that outlines expected behaviors and provides guidance on how to report and address violations.</li> <li>Open Code of Conduct: This is a simple and straightforward code of conduct that outlines basic expectations for behavior and provides guidance on how to report and address violations.</li> <li>Geek Feminism code fo conduct: This code of conduct from the Geek Feminism community prioritizes marginalized people\u2019s safety over privileged people\u2019s comfort. It is dedicated to providing a harassment-free experience for everyone, regardless of gender, gender identity and expression, sexual orientation, disability, physical appearance, body size, race, or religion. We do not tolerate harassment of participants in any form.</li> <li>Citizen Code of Conduct: This is a code of conduct that is specifically designed for use in open source projects and communities. It includes guidelines on appropriate behavior and provides a framework for addressing violations.</li> </ul> <p>It is a good idea to carefully consider the different codes of conduct available and choose one that aligns with the values and goals of your community.</p>"},{"location":"resources/cheatsheets/my-python-project-cheatsheet/#other-references-and-links","title":"Other references and links","text":"<ul> <li>Template with the Python structure on github</li> <li>Open Sourcing a Pthon Project the Right way</li> <li>Filesystem structure of a Python project</li> <li>Github repository: cheatsheets</li> </ul>"},{"location":"resources/cheatsheets/my-r-cheatsheet/","title":"R cheatsheet","text":"<p>I wanted to post my R cheat sheet and some useful functions I use almost every day. As far as I learn more programming languages I'll create more cheat sheets, hope they'll be helpful.</p>"},{"location":"resources/cheatsheets/my-r-cheatsheet/#the-r-cheatsheet","title":"The R cheatsheet","text":"<p>Download the cheatsheet</p>"},{"location":"resources/cheatsheets/my-r-cheatsheet/#other-links","title":"OTHER LINKS","text":"<ul> <li>Github repository: cheatsheets</li> <li>More resources like this here: https://carlosgrande.me/category/resources</li> </ul>"},{"location":"resources/cheatsheets/my-sql-cheatsheet/","title":"SQL cheatsheet","text":"<p>In this post I wanted to share some helpful codes I learned from SQL and my own SQL cheat sheet. As far as I learn more programming languages I'll create more cheat sheets, hope they'll be helpful.</p>"},{"location":"resources/cheatsheets/my-sql-cheatsheet/#the-sql-cheatsheet","title":"The SQL cheatsheet","text":"<p>Download the cheatsheet</p>"},{"location":"resources/cheatsheets/my-sql-cheatsheet/#example-scripts","title":"Example scripts","text":"<p>Here are some SQL scripts to get a better understanding of the cheat sheet and its application.</p>"},{"location":"resources/cheatsheets/my-sql-cheatsheet/#python-library-sqlite-3","title":"Python library sqlite 3","text":"<p>This way shows how to read SQL from python with the library sqlite3.</p> <pre><code>import sqlite3\n# creates de connection\nconn = sqlite3.connect(\"factbook.db\")\n\n# you can create a Cursor object and call its execute() \nc = conn.cursor()\n\n# write the query and call it\nq = '''SELECT * FROM sqlite_master LIMIT 5;'''\nc.execute(q)\n</code></pre> <p>This  other way shows how to call a query using pandas library and save it as a data frame.</p> <pre><code>import sqlite3\nimport pandas\nconn = sqlite3.connect(\"factbook.db\")\n\nq = \"SELECT * FROM sqlite_master WHERE type='table';\"\npd.read_sql_query(q, conn)\n</code></pre>"},{"location":"resources/cheatsheets/my-sql-cheatsheet/#subqueries","title":"Subqueries","text":"<p>This code shows how to use a subquery as a condition.</p> <pre><code>SELECT Major, Unemployment_rate FROM recent_grads\nWHERE Unemployment_rate &lt; (SELECT AVG(Unemployment_rate) FROM recent_grads)\nORDER BY Unemployment_rate;\n</code></pre> <p>In this code it's shown how to make multiples subqueries in different situations.</p> <pre><code>SELECT \nCOUNT(CAST(ShareWomen AS float)) / (SELECT CAST(COUNT(ShareWomen) AS float) FROM recent_grads) AS proportion_abv_avg \nFROM recent_grads\nWHERE ShareWomen &gt; (SELECT AVG(ShareWomen) FROM recent_grads);\n</code></pre>"},{"location":"resources/cheatsheets/my-sql-cheatsheet/#multiple-tables-joins","title":"Multiple tables Joins","text":"<p>This example shows how to join multiple tables by different keys applying some conditions.</p> <pre><code>SELECT t.track_id, t.name track_name, m.name track_type, t.unit_price, i.quantity FROM track t\nJOIN invoice_line i ON i.track_id = t.track_id\nJOIN media_type m ON t.media_type_id = m.media_type_id\nWHERE i.invoice_id = 4;\n</code></pre> <p>Picture retrieved from the SQL course www.dataquest.io</p> <p></p> <p>The next code below shows a multi-join operation using subqueries and group by among others.</p> <pre><code>SELECT am.title album, a.name artist, i.tracks_purchased  FROM album am\nJOIN (SELECT album_id, sum(quantity) as tracks_purchased FROM track t\nJOIN invoice_line i ON t.track_id = i.track_id\nGROUP BY album_id) i ON i.album_id = am.album_id\nJOIN artist a ON a.artist_id = am.artist_id\nORDER BY i.tracks_purchased DESC\nLIMIT 5;\n</code></pre>"},{"location":"resources/cheatsheets/my-sql-cheatsheet/#case-with-and-view-clauses","title":"CASE, WITH and VIEW clauses","text":"<p>The CASE clause allows to create a temporal column modifying its content using conditionals. This example shows how to use CASE clause and join operations with subqueries.</p> <pre><code>SELECT\n    (c.first_name || ' ' || c.last_name) customer_name,\n    i.number_of_purchases,\n    i.total_spent,\n    CASE\n        WHEN total_spent &lt; 40 THEN 'small spender'\n        WHEN total_spent &gt;= 40 and total_spent &lt;= 100 THEN 'regular'\n        ELSE 'big spender'\n    END customer_category\nFROM customer c\nJOIN (SELECT customer_id, SUM(total) total_spent, COUNT(invoice_id) number_of_purchases FROM invoice\n      GROUP BY customer_id\n</code></pre> <p>The WITH clause creates a temporal subquery that can be used as another table. This example shows how to use WITH clause in a multi-join operation.</p> <pre><code>WITH customers_usa AS \n    (\n        SELECT * FROM customer_usa\n        INTERSECT\n        SELECT * FROM customer_gt_90_dollars\n    )\nSELECT\n    em.first_name || ' ' || em.last_name AS employee_name,\n    COUNT(cusa.customer_id) customers_usa_gt_90\n    FROM employee em\nLEFT JOIN customers_usa cusa ON cusa.support_rep_id = em.employee_id\nWHERE em.title = 'Sales Support Agent'\nGROUP BY 1 ORDER BY 1;\n</code></pre> <p>The VIEW clause works as the WITH clause allowing to save the new table for other queries. The next example shows how to create a VIEW and call it after.</p> <pre><code>/* view creation */\nCREATE VIEW chinook.customer_gt_90_dollars AS\n    SELECT \n        c.*\n    FROM customer c\n    JOIN\n        (\n        SELECT customer_id, SUM(total) as total FROM invoice\n        GROUP BY customer_id\n        ) t ON t.customer_id = c.customer_id\n    WHERE t.total &gt; 90;\n\n/* calling the view table */\nSELECT * FROM chinook.customer_gt_90_dollars;\n</code></pre>"},{"location":"resources/cheatsheets/my-sql-cheatsheet/#other-links","title":"OTHER LINKS","text":"<ul> <li>Github repository: cheatsheets</li> <li>A nice website to learn SQL: https://www.codecademy.com/</li> <li>Another nice website to learn SQL: https://www.dataquest.io/</li> <li>More resources like this here: https://carlosgrande.me/category/resources</li> </ul>"},{"location":"resources/templates/my-python-template/","title":"My Python template","text":"<p>Every time I have started a new Python project, I have spent a lot of time setting up the initial files like the README, Code of Conduct, licenses, and the base code itself to start with. Even having a template ready, the process of cloning it and starting to customize it has taken me a lot of time.</p> <p>This is where PyTemplate comes into play. The template follows a customizable project structure using cookiecutter as the project generator. PyTemplate contains the usual files that a Python project needs, base code based on classes with an integrated logger and a Docker file to containerize your application. It also allows you to choose between multiple licenses and customize the documentation, deploying it on GitHub pages with GitHub workflows included in the template.</p> <ul> <li>Documentation: https://charlstown.github.io/py-template/</li> <li>Github Repo: https://github.com/charlstown/py-template</li> </ul> <p>The easiest way to understand what PyTemplate does is to create a simple project from this template and see how it works.</p>"},{"location":"resources/templates/my-python-template/#what-is-cookiecutter","title":"What is Cookiecutter","text":"<p>Cookiecutter is a Python package, easily installable with pip or other package managers, that enables you to create and use templates for microservices and software projects. It is a command-line tool that requires no knowledge of Python to use. Cookiecutter is widely used among software engineers, researchers, data scientists, and other technical roles.</p> <p></p> <p>Visit these links to learn more about cookiecutter:</p> <ul> <li>Documentation: https://cookiecutter.readthedocs.io/</li> <li>GitHub repo: https://github.com/cookiecutter/cookiecutter</li> </ul>"},{"location":"resources/templates/my-python-template/#what-is-pytemplate","title":"What is PyTemplate","text":"<p>A Cookiecutter template for general python developers. Recommended for DevOps &amp; Data Science projects. The template follows a customizable project structure using cookiecutter as the template generator.</p> <ul> <li>Documentation: https://charlstown.github.io/py-template/</li> <li>Github Repo: https://github.com/charlstown/py-template</li> </ul>"},{"location":"resources/templates/my-python-template/#the-project-structure","title":"The project structure","text":"<p>There is no one \"right\" way to structure a Python project, as the appropriate structure can depend on the specific needs and goals of the project. However, PyTemplate applies some general guidelines and good practices when organizing the project files. A structure suitable for larger projects that consist of multiple modules and packages.</p> <pre><code>my_project/\n\u251c\u2500\u2500 my_package/\n\u2502      \u2514\u2500\u2500 config/\n\u2502      \u2502      \u251c\u2500\u2500 settings.yaml\n\u2502      \u2502      \u2514\u2500\u2500 additional_settings.yaml\n\u2502      \u251c\u2500\u2500 __init__.py\n\u2502      \u251c\u2500\u2500 main.py\n\u2502      \u251c\u2500\u2500 module_1.py\n\u2502      \u251c\u2500\u2500 module_2.py\n\u2502      \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 tests/\n\u2502      \u251c\u2500\u2500 test_module_1.py\n\u2502      \u251c\u2500\u2500 test_module_2.py\n\u2502      \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 data/\n\u2502      \u251c\u2500\u2500 input.csv\n\u2502      \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 docs/\n\u2502      \u251c\u2500\u2500 index.md\n\u2502      \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 requirements.txt\n\u251c\u2500\u2500 LICENSE\n\u2514\u2500\u2500 ...\n</code></pre> <p>You can visit my post about My Python project cheatsheet to learn more about this structure.</p>"},{"location":"resources/templates/my-python-template/#pytemplate-demo-in-action","title":"PyTemplate demo in action","text":"<p>To create a new project from the template you need to install cookiecutter and follow these instructions.</p>"},{"location":"resources/templates/my-python-template/#1-cookiecutter-installation","title":"1. Cookiecutter installation","text":"<p>Installing cookiecutter package is very easy, you can simply run the next command to install it.</p> <pre><code>pip install cookiecutter\n</code></pre> <p>Visit the link to the cookiecutter documentation to learn more about the installation.</p> <p>Tip</p> <p>It is recommended to create a virtual environment and install the cookiecutter library inside this environment.</p>"},{"location":"resources/templates/my-python-template/#2-running-the-template","title":"2. Running the template","text":"<p>Run the cookiecutter command followed by the template repository URL. When this command is finished, it generates a folder with your project name and all the template files customized.</p> <pre><code>python -m cookiecutter https://github.com/charlstown/py-template.git\n</code></pre>"},{"location":"resources/templates/my-python-template/#3-customizing-the-template","title":"3. Customizing the template","text":"<p>Fill out the form in the console to customize the template and the project will be generated at the end.</p> <p></p> <p>You should see a new folder with the repository name you gave as input (my-python-project).</p> <p>Congrats! \ud83c\udf87 You finally installed your template to start coding your project!</p>"},{"location":"resources/templates/my-python-template/#4-pushing-the-project-to-github","title":"4. Pushing the project to Github","text":"<p>In this step we want to push the repository to our github account.</p> <ul> <li> <p>Create a new repository in github:</p> <p>Create the new repository and make sure you give it the same name of the project folder, in our case my-python-project.</p> <p>Don't add any predefined file from github in the new repository, all the files are included in the local project folder.</p> </li> <li> <p>Push the local repository:</p> <pre><code>git remote add origin https://github.com/charlstown/my-python-project.git\ngit branch -M main\ngit push -u origin main\n</code></pre> </li> </ul> <p>After pushing the project the github workflow On Push Deploy Documentation will create a new branch called gh-pages.</p> <p>This branch contains the documentation files in html ready to be published in Github Pages.</p>"},{"location":"resources/templates/my-python-template/#5-deploying-the-documentation-in-github","title":"5. Deploying the documentation in Github","text":"<p>Deploy your documentation in Github pages is so easy, you only need to set the pages configuration pointing at the gh-pages branch generated by our github action On Push Deploy Documentation.</p> <ol> <li>Go to the repository settings.</li> <li>Select Pages at the settings menu under the code and automation section.</li> <li>Make sure pages is enable and select gh-pages as the site branch under Build and deployment section.</li> </ol> <p></p> <p>Your site should be live at: https://user.github.io/my-python-project/</p> <p>Remember, to deploy the documentation in Github Pages the repository must be a public repository.</p>"},{"location":"resources/templates/my-python-template/#references-and-links","title":"References and links","text":"<ul> <li>Pytemplate Documentation</li> <li>Pytemplate repository</li> <li>Cookiecutter Documentation</li> <li>Cookiecutter repository</li> <li>Mkdocs main page</li> <li>Mkdocs documentation</li> <li>Mkdocs introductory tutorial</li> <li>Material for Mkdocs</li> <li>Github Actions documentation</li> <li>Github Actions marketplace</li> <li>Github Pages documentation</li> <li>More Resources like this here</li> </ul>"},{"location":"resources/thesis/my-data-fabric-thesis/","title":"My data fabric thesis","text":"<p>After Posting my thesis about Data Mesh, I realized Data fabric is often mistaken for Data Mesh. There is a lot of confusion between these two models, so I decided to do some research about the Data Fabric architectural approach.</p> <p>The goal of this article is to try and bring some clarity to all concepts associated with a Data Fabric, including an end to end storytelling with a thesis structure. This is just my public personal attempt to make sense of all the information I have found about this matter.</p>"},{"location":"resources/thesis/my-data-fabric-thesis/#1-context","title":"1. Context","text":"<p>Current methods of managing data attempt to meet all the objectives using data warehouses and data lakes, but those legacy architectures cannot include all the data that is needed. Although, they remain important components in a larger distributed data landscape.</p> <p>Data sharing isn\u2019t too hard at a small scale but rapidly becomes challenging as variables like the number of independent projects, the variety of database technologies, variation of computing platforms, etc. increase. Satisfying these complex scenarios becomes prohibitively difficult and expensive to accomplish through independent bespoke processes.</p> <p>In this context, Data Fabric emerges as a design concept that acts as an integrated layer (fabric) of data and connecting processes. It represents a single environment having a combination of architecture and technology designed to ease the complications of managing dynamic, distributed, and diverse data.</p> <p></p> <p>So what is a Data Fabric? Data fabric is an architectural approach and a set of technologies to break down data silos and get data into the hands of data users. if I would like to say what is a Data Fabric in a short definition, I would pick the following:</p> <p>A Data Fabric is a metadata-driven unified platform that connects multiple technologies, deployment platforms, and automation services, with a single and consistent data management framework. It provides seamless data access and processing by design across siloed storage.</p> <p></p>"},{"location":"resources/thesis/my-data-fabric-thesis/#2-principles","title":"2. Principles","text":"<p>Basically, data fabric is an architecture that improves data management by reducing data complexity and enabling agility. The effectiveness of data fabric approach, however depends on its abilities to meet numbers of principles. These principles that define data fabric can be divided into: data access, data storage, data management, and data expose.</p> <p></p>"},{"location":"resources/thesis/my-data-fabric-thesis/#21-unified-data-access","title":"2.1 Unified data access","text":"<p>Providing a single and seamless point of access to all data regardless of structure, database technology, and deployment platform creates a cohesive analytics experience working across data storage silos.</p> <ul> <li> <p>Data fabric needs to support different data types and access protocols to allow organizations to easily access data from everywhere, anywhere at any time.</p> </li> <li> <p>Multitenancy is essential in data fabric such that it gives organizations the ability to create logical separations for administration, access, update, and execution.</p> </li> <li>Data fabric should provide support for integrating data streaming to easily integrate data in transit with data at rest.</li> </ul>"},{"location":"resources/thesis/my-data-fabric-thesis/#22-infrastructure-resilience","title":"2.2 Infrastructure resilience","text":"<p>Decoupling data management processes and practices from specific deployment technologies makes for a more resilient infrastructure. Whether adopting edge computing, GPU databases, or technology innovations not yet known, the data fabric\u2019s management framework offers a degree of \u201cfuture-proofing\u201d that reduces the disruptions of new technologies. New infrastructure end-points are connected to the data fabric without impact to existing infrastructure and deployments.</p> <p>Supports the full variety of data storage options, automatically applying the best mix of storage technologies depending on use cases.</p> <ul> <li>Data fabric should give linear scalability where fabric is not limited by the number of files, concurrent client access, and increase in data volumes.</li> <li>Data fabric is designed to ensure data consistency so that data is manageable across multiple environments regardless of its location. A layer built on top of many data sources. This layer presents information as a single virtual (or logical) view regardless of its type and model. It all happens on-demand and in real-time.</li> <li>Distributed metadata across all data storing nodes should be supported in the fabric to prevent bottlenecks.</li> <li>It is necessary for a data fabric to give distributed location support where it can be installed and deployed in any location (any environment) across the entire infrastructure.</li> </ul>"},{"location":"resources/thesis/my-data-fabric-thesis/#23-unified-data-management","title":"2.3 Unified data management","text":"<p>Providing a single framework to manage data across disparate deployments reduce the complexity of data management.</p> <ul> <li>Data fabric providesGovernance and privacy ensuring the right users in our platform have access to the right data by using active metadata, to automate a lot of the enforcement of the policies we define (Mask datasets, RBAC, data lineage, and data quality).</li> <li>Data fabric helps us define compliance policies like GDPR, CCPA, HIPAA, FCRA, etc.</li> <li>Metadata management is another ubiquitous component with touch points throughout the data fabric. Metadata reduces friction throughout the processes of working with data. It is needed to search and understand data, assess data quality, prepare and provision data, protect and govern data, trace data lineage, and trust data and analysis results.</li> </ul>"},{"location":"resources/thesis/my-data-fabric-thesis/#24-consolidated-data-protection","title":"2.4 Consolidated data protection","text":"<p>Data security, backup, and disaster recovery methods are built into the data fabric framework. They are applied consistently across the infrastructure for all data whether deployed in cloud, multi-cloud, hybrid, or on premises.</p> <p>A distinct security and protection component is needed to provide cohesion and continuity across all security and protection touch points. Smart data security interoperates with existing authentication and authorization infrastructure. Smart data protection guards against risks from intrusion, corruption, and loss by automating detection and tagging of sensitive data assets and by providing data recovery capabilities.</p>"},{"location":"resources/thesis/my-data-fabric-thesis/#25-cloud-mobility-and-portability","title":"2.5 Cloud mobility and portability","text":"<p>Minimizing the technical differences that lead to cloud service lock-in and enabling quick migration from one cloud platform to another supports the goal of a true cloud-hybrid environment.</p> <p>Data stored across multiple environments should not be isolated or siloed. Processing can\u2019t be confined to a single execution environment when data resides on multiple platforms. With data fabric orchestration, a single control platform can execute a sequence of services, built with diverse technologies and distributed across multiple execution environments. When data is distributed across multiple platforms, it makes sense to push the processing to the data location. Smart data fabric automates the coordination of multi-services workflows across all execution environments for comprehensive cloud, multi-cloud, and cloud-hybrid support.</p>"},{"location":"resources/thesis/my-data-fabric-thesis/#3-data-fabric-architecture","title":"3. Data Fabric Architecture","text":"<p>A well designed data fabric architecture is modular and supports massive scale, distributed multi-cloud, onpremise, and hybrid deployment.</p> <p></p>"},{"location":"resources/thesis/my-data-fabric-thesis/#31-connection-layer","title":"3.1 Connection layer","text":"<p>In order to provide data across \u201cotherwise siloed storage\u201d a connection layer governs access to data assets and metadata through standard mechanisms whilst preserving (where possible) the native asset access APIs.  It provides interfaces and factories for named connectors that access distributed data resources. These data resources may be data stores (databases, files, etc.) or APIs for application data, transformations, and analytical functions.</p> <ul> <li>SQL and NoSQL databases like MySQL, Oracle, and MongoDB</li> <li>CRMs and ERPs</li> <li>Cloud data warehouses like Amazon Redshift or Google Big Query</li> <li>Data lakes and enterprise data warehouses</li> <li>Streaming sources like IoT and IoMT devices</li> <li>SaaS (Software-as-a-Service) applications like Salesforce</li> <li>Social media platforms and websites</li> <li>Spreadsheets and flat files like CSV, JSON, and XML</li> <li>Big data systems like Hadoop, and many more.</li> </ul> <p>The Connection Layer acts as a secure factory for connectors to data stores.  The application supplies the name of the connection it needs and assuming it is authorized, the Connection layer returns the connector.</p> <p></p> <p>The main components of a Connection layer are:</p> <ul> <li>Connection: The connection is a metadata entity that defines the set of parameters needed to access a specific data resource.  Each connection has a unique name.  An application can request a connector instance from the OCF using the name of a connection.</li> <li>Connector Broker: A connector instance is an object that implements the Connector API.   It provides access to a data resource, along with its related metadata stored.  The connector instance is responsible for calling the governance action framework when it is initialized and before and after every access request to the data resource.</li> <li>Connector Provider: A connector provider is the factory for a particular type of connector.  The connection stored in the Metadata repository will identify the connector provider. The connector provider is responsible for the overall management of the physical data resources that is it using. It may therefore implement capabilities such as connector pooling and limit the number of active connectors to the data resource if appropriate.</li> </ul>"},{"location":"resources/thesis/my-data-fabric-thesis/#32-virtualization-layer","title":"3.2 Virtualization layer","text":"<p>In a nutshell, data virtualization happens via middleware that is nothing but a unified, virtual data access layer built on top of many data sources. This layer presents information as a single virtual (or logical) view regardless of its type and model. It all happens on-demand and in real-time.</p> <p></p>"},{"location":"resources/thesis/my-data-fabric-thesis/#321-storage-mapping","title":"3.2.1 Storage &amp; Mapping","text":"<p>The layer where the virtual and the physical data resides. The virtual store only contains logical views and metadata needed to access the sources, during the physical store.</p> <p>The SQL engine is responsible for processing SQL queries in SAP HANA. The SQL Parser (part of SQL Engine) first checks the syntactic and semantic correctness of the SQL query. The SQL query is then parsed through the SQL optimizer to create an acyclic tree-structure of operations and estimated cost is assigned to each operation in the tree structure. Depending on the complexity involved, the SQL Optimizer may generate multiple execution plan and the most cost-effective execution plan will be forwarded to the SQL Executor.</p> <p>The SQL executor will then forward the request to the appropriate engines i.e. Row store engines to deal with row tables and Column store engines (Calc. Engine, Join Engine and OLAP engine) for the column tables.</p> <p>The calculation engine is used to perform field level calculations and union/rank operations on the dataset, unfolding the definitions, meaning, origin, and rules of the information used by its metadata. There are three types of metadata:</p> <ul> <li>Business Metadata: stores business-critical definitions of information</li> <li>Technical metadata: stores data mapping and transformations from source systems to the target and is mostly used by data integration experts and ETL developers.</li> <li>Operational metadata: describes information from operational systems and runtime environments.</li> </ul>"},{"location":"resources/thesis/my-data-fabric-thesis/#322-virtual-data-models","title":"3.2.2 Virtual data models","text":"<p>It is a structured representation of the virtual and local data with consistent modeling rules applied and providing direct access.</p> <ul> <li>Calculation views fully support set based operations enabling real-time analytics and on the fly calculations without the use of persistent aggregates. Calculation views are un-folded to the table/database objects and any table/database objects not contributing to the SQL execution are eliminated to build a single acyclic data-flow graph for the SQL query.</li> </ul>"},{"location":"resources/thesis/my-data-fabric-thesis/#33-data-access-layer","title":"3.3 Data access layer","text":"<p>A single point of access to data kept in the underlying sources. The delivery of abstracted data views happens through various protocols and connectors depending on the type of the consumer. They may communicate with the virtual layer via SQL and all sorts of APIs, including access standards like JDBC and ODBC, REST and SOAP APIs, and many others.</p> <ul> <li>Data Catalog: To provide a service which allows one to search for data and datasets across an enterprise. Sounds like a search index on data and metadata right? That\u2019s the basis but the key is not to just return data records but help one find the topics in the data fabric that they came from.</li> <li>Query services: provides a user interface and a RESTful API from which you can create SQL queries to better analyze your data. With the user interface, you can write and execute queries, view previously executed queries, and access queries saved by users within your Organization.</li> <li>APIs: offering an access to your business logic through an Interface (the API), with full control on what you want to show or not.</li> <li>Data provisioning: the process of making data available in an orderly and secure way to users, application developers, and applications that need it.</li> </ul>"},{"location":"resources/thesis/my-data-fabric-thesis/#34-data-governance","title":"3.4 Data Governance","text":"<p>When consuming from a data fabric it can be very useful to understand some details about what you are getting. What is the quality? Where did it come from? What is its lineage? The necessity of data governance services is highly dependent on the organization building the data fabric.</p> <p>Governance, data privacy, and data sovereignty must be aligned. Because data stored in binary or digital form is subject to the conditions of the country in which the data resides, it is imperative that organizations understand the nature of their cloud architecture and interactions. Moreover, it is important to understand the rules, legislation, and regulations that apply to various countries as they apply to personal information, data privacy, and data protection.</p>"},{"location":"resources/thesis/my-data-fabric-thesis/#35-data-security-policies","title":"3.5 Data security &amp; policies","text":"<p>The data fabric must accommodate governance and data privacy in a way that is appropriate for the service models (that is, IaaS, PaaS, SaaS) being provided. Security capabilities must be considered through every architectural layer of the data fabric. It is also imperative that organizations know how their data is being secured.</p> <ul> <li>Secure Multitenancy (SMT): Organizations must understand how their service providers segment and isolate each customer in their cloud infrastructure and how that manifests itself across a data fabric.</li> <li>Data in Use, in Motion, and at Rest: refers to any data being processed by a cloud service provider. It is key to maintaining security throughout the lifecycle because it is crucial to maintaining an organization\u2019s security posture. The fact that cloud solutions rely on shared processes and resources introduces the requirement for more diligence. The security criteria that are maintained within an organization must be maintained and possibly enhanced across the data fabric.</li> <li>Key Management: refers to safeguarding and managing keys in order to manage a secure environment that effectively safeguards an organization\u2019s data. A key aspect of the key management solution across a data fabric is maintaining proper access to the keys while securely storing them.</li> </ul>"},{"location":"resources/thesis/my-data-fabric-thesis/#4-market-state-use-cases","title":"4. Market State &amp; Use Cases","text":"<p>Data fabric is a relatively new concept and the technology market is best characterized as emerging and evolving. Today no single vendor provides a complete data fabric solution in a single product. Data fabric vendors fit into the following three main categories:</p> <ul> <li>Single Product: A few vendors offer a single product that provides much, but not all, data fabric functionality. These are good choices if they offer the functions that are most essential for your organization and have a strong roadmap for the future.</li> <li>Single Vendor: Some vendors offer multiple products, each providing a subset of data fabric functionality. They emphasize interoperability among the products as the approach to delivering some (again, not all) data fabric functionality. These may be good choices if the product suite provides the most needed functions, especially for those who already use some of their products.</li> <li>Multiple Vendors: Many vendors offer products that deliver a subset of data fabric functionality. These can be good choices for those organizations whose technology infrastructure includes diverse products that each provide some data fabric functionality, and who have the technical ability to resolve interoperability among disparate products.</li> </ul> <p>In enterprise operations, there are scores of use cases that require a high-scale, high-speed data architecture capable of supporting thousands of simultaneous transactions. Examples include:</p> <ul> <li>Delivering a 360 customer view: delivering a single view of the customer to a selfservice IVR, customer service agents (CRM), customer self-service portal (web or mobile), chat service bots, and field-service technicians</li> <li>Complying with data privacy laws: with a flexible workflow and data automation solution that orchestrates compliance across people, systems, and data designed to address current and future regulations.</li> <li>Pipelining enterprise data into data lakes and warehouses: enabling data engineers to prepare and deliver fresh, trusted data from all sources, to all targets quickly and at scale.</li> <li>Provisioning test data on demand: creating a test data warehouse, and delivering anonymized test data to testers and CI/CD pipelines automatically, and in minutes with complete data integrity.</li> <li>Modernizing legacy systems: safely migrating data from legacy systems into data fabric, and then using the fabric as the database of record for newly developed applications.</li> <li>Securing credit card transactions: Protecting sensitive cardholder information by encrypting and tokenizing the original data to avoid data breaches.</li> <li>Predicting churn, detecting customer, and others.</li> </ul>"},{"location":"resources/thesis/my-data-fabric-thesis/#5-data-fabric-implementation","title":"5. Data Fabric Implementation","text":"<p>As we have seen, data fabric is a high-level data architecture, and each company and provider offer it with different patterns and technologies. In this way, it is essential to remember that the Data Fabric is not static. It is a journey that iterates over the development of the platform, looking to abstract the silos of physical information into a logical layer of data.</p>"},{"location":"resources/thesis/my-data-fabric-thesis/#51-as-is-analysis","title":"5.1 AS-IS analysis","text":"<ul> <li>Identify your data sources: decide what and how much data there is to be virtualized. For this purpose, make a comprehensive list of all datasets, applications, services, and systems producing information. Along with that, determine their locations, management demands, and connectivity requirements to enable them to easily communicate with the virtualization layer. Some systems lay on the surface as they are used in your day-to-day operations while others may be in the depths of IoT devices and social media sites. It is a good idea to include all sources that can enhance business analytics.</li> <li>Identify your consumers: Similar to the previous step, you may want to list all the tools and applications that will reside on the consumer side. Specify what connectors and protocols each of the consumers require for it to have access to the virtual views. Which of the operations in your company will benefit from the virtualization the most? Start with the tools supporting these operations.</li> <li>Decide on resources and people involved:  While it is easier and less costly to implement compared to traditional ETL, you will still have to determine your budget and available resources \u2014 both tech and human. Along with business analysts, you may need such specialists as data engineers and SQL developers to model data, build transformations, design data services, optimize queries, and manage resources.</li> </ul>"},{"location":"resources/thesis/my-data-fabric-thesis/#52-developing-a-technical-reference-model-trm","title":"5.2 Developing a technical reference model (TRM)","text":"<p>Data fabric can be implemented with multiple technologies and patterns. It is mandatory to  provide a reference of generic platform services, technology elements and patterns for each layer (connection, virtualization, and data access layers). The TRM provides a set of architectural and solution building blocks that will ultimately provide the platform for business and infrastructure applications that will deliver the application and infrastructure services. The Technical Reference Model ensures that architectures are created consistently and repeatedly based on a standard set of elements. The model should be created as part of the set up of the architecture programs but will typically require extending as technology standards are introduced and retired.</p>"},{"location":"resources/thesis/my-data-fabric-thesis/#53-trm-implementation-and-tech-stack","title":"5.3 TRM implementation and Tech Stack","text":"<p>Once defined, the Technical Reference Model can be used as the basis for all Infrastructure Architecture models by creating instances of the Infrastructure elements. We also need to define/update our company tech stack (a tech stack typically consists of programming languages, frameworks, a database, front-end tools, back-end tools, and applications connected via APIs). For example, in the Connection layer, we could use Apache Atlas as a Connector directory providing a list of related connections.</p>"},{"location":"resources/thesis/my-data-fabric-thesis/#6-conclussion","title":"6. Conclussion","text":"<p>A data fabric is not an off the shelf sort of thing. It is currently more of a design pattern for something that has been built in different ways across many different organizations. That being said, It\u2019s also not something that needs to be built from scratch. There are many off the shelf components with a little integration work that will get you to the promised land.</p>"},{"location":"resources/thesis/my-data-fabric-thesis/#7-terminology","title":"7. Terminology","text":"<p>Data Fabric isn't a static architecture although it has some related terms. At the next table, I tried to extract the main vocabulary around this topic.</p> Word Definition Data Fabric A metadata-driven unified platform that connects multiple technologies, deployment platforms, and automation services, with a single and consistent data management framework. It provides seamless data access and processing by design across siloed storage. Connection layer A layer to provide data across siloed storage, governing the access to data assets and metadata through standard mechanisms and acting as a secure factory for connectors to data stores. Virtualization layer A layer to present information as a single virtual (or logical) view on-demand and in real-time, regardless of its type and model. Data access layer A single point of access to data kept in the underlying sources. The delivery of abstracted data views happens through various protocols and connectors depending on the type of the consumer. Connector An object to provide access to a data resource, along with its related metadata stored. Metadata Metadata unfolds the definitions, meaning, origin, and rules of the information used in the platform. Virtual data model It is a structured representation of the virtual and local data with consistent modeling rules applied and providing direct access. Calculated View stores data mapping and transformations from source systems enabling real-time analytics and on the fly calculations without the use of persistent aggregates. Calculation engine An engine used to perform field level calculations and union/rank operations on the dataset. Data mapping A process to map the data path from the source system that contains different data fields from the target, appliying on the fly calculations."},{"location":"resources/thesis/my-data-fabric-thesis/#8-discovery-roadmap","title":"8. Discovery Roadmap","text":"<p>To elaborate this Thesis, I have been recollecting links and resources about the Data Fabric architecture. I wanted to share the discovery path I've followed to understand the Data Fabric implications.</p>"},{"location":"resources/thesis/my-data-fabric-thesis/#81-introductory-content","title":"8.1 Introductory content","text":"<ul> <li>What is a data fabric and why should you care?</li> <li>Data Fabric Architecture is Key to Modernizing Data Management and Integration</li> <li>What is Data Fabric? Trifacta</li> <li>Data Fabric Explained: Concepts, Capabilities &amp; Value Props</li> <li>What are the principles that define data fabric?</li> <li>An introduction to the concept of Data Fabric</li> <li>ATSCALE: What is a Data Fabric?</li> <li>Data Fabric Explained by Ellen Fridman and Ted Dunning</li> <li>Managing Data Fabric architecture for data-driven business challenges</li> </ul>"},{"location":"resources/thesis/my-data-fabric-thesis/#82-architecture","title":"8.2 Architecture","text":"<ul> <li>Data Fabric Architecture: Advantages and Disadvantages</li> <li>Comparing Data Fabric vs Data Virtualization</li> <li>What's a data fabric and how does it work?</li> <li>Metadata and Data Virtualization Explained</li> </ul>"},{"location":"resources/thesis/my-data-fabric-thesis/#83-deep-dive","title":"8.3 Deep Dive","text":"<ul> <li>Data Mesh Vs. Data Fabric: Understanding the Differences</li> <li>The top 6 use cases for a data fabric architecture</li> <li>What is Data Fabric</li> <li>Data Fabric TechTarget</li> <li>Data Fabric: What is it and Why Do You Need it?</li> </ul>"},{"location":"resources/thesis/my-data-fabric-thesis/#references-and-links","title":"References and links","text":"<ul> <li>CaraDonna, J. and Lent, A. NetApp Data Fabric Architecture Fundamentals. White Paper. Infoworks.</li> <li>D.Tupper, C. Data Architecture: From Zen to Reality (1st ed.). ELSEVIER.</li> <li>LaPlante, A. Data Fabric as Modern Data Architecture. Report. (1st ed.). O'Reilly.</li> <li>Wells, D. Data Fabric: Smart Data Engineering, Operations and Orchestration Infoworks.</li> <li>Pattanayak, A. High Performance Analytics with SAP HANA Virtual Models. Scientific Research Publishing Inc.</li> <li>Freeman, L. and C. Miller, L. Hybrid Cloud &amp; Data Fabric For Dummies, NetApp Special Edition John Wiley &amp; Sons, Inc. </li> <li>More Resources like this here</li> </ul>"},{"location":"resources/thesis/my-data-mesh-thesis/","title":"My data mesh thesis","text":"<p>I wanted to write a post with my thesis about the  Data Mesh paradigm coined by Zhamak Dehghani. To be honest, I\u2019m still wondering what a Data Mesh is.</p> <p>Looking at different articles, videos and talking to other people wondering the same thing, you realize some ideas differ considerably. It seems that the Data Mesh paradigm has some abstract ideas, and hence it has different interpretations. Although if I would like to say what a Data Mesh is in a short definition, I would pick the following:</p> <p>Data meshes are a decentralization technique of the ownership, transformation &amp; serving of data. It is proposed as a solution for centralized architectures, where growth is limited by its dependencies and complexity.</p> <p></p> <p>The goal of this article is to try and bring some clarity to all concepts associated with a Data Mesh, including an end to end storytelling with a thesis structure. This is just my public personal attempt to make sense of all the information I have found about this matter.</p>"},{"location":"resources/thesis/my-data-mesh-thesis/#1-context","title":"1. Context","text":""},{"location":"resources/thesis/my-data-mesh-thesis/#11-first-generation-data-warehouse-architecture","title":"1.1 First Generation: Data Warehouse Architecture","text":"<p>Data warehousing architecture today is influenced by early concepts such as facts and dimensions formulated in the 1960s. The architecture intends to flow data from operational systems to business intelligence systems that traditionally have served the management with operations and planning of an organization. While data warehousing solutions have greatly evolved, many of the original characteristics and assumptions of their architectural model remain the same. (Dehghani, 2022)</p> <p></p>"},{"location":"resources/thesis/my-data-mesh-thesis/#12-second-generation-data-lake-architecture","title":"1.2 Second Generation: Data Lake Architecture","text":"<p>Data lake architecture was introduced in 2010 in response to challenges of data warehousing architecture in satisfying the new uses of data; access to data based on data science and machine learning model training workflows, and supporting massively parallelized access to data. </p> <p>Unlike data warehousing, data lake assumes no or very little transformation and modeling of the data upfront; it attempts to retain the data close to its original form. Once the data becomes available in the lake, the architecture gets extended with elaborate transformation pipelines to model the higher value data and store it in lakeshore marts.</p> <p>This evolution to data architecture aims to improve ineffectiveness and friction introduced by extensive upfront modeling that data warehousing demands. The upfront transformation is a blocker and leads to slower iterations of model training. Additionally, it alters the nature of the operational system\u2019s data and mutates the data in a way that models trained with transformed data fail to perform against the real production queries. (Dehghani, 2022)</p> <p></p>"},{"location":"resources/thesis/my-data-mesh-thesis/#13-third-generation-multimodal-cloud-architecture","title":"1.3 Third Generation: Multimodal Cloud Architecture","text":"<p>The third and current generation data architectures are more or less similar to the previous generations, with a few modern twists:</p> <ul> <li>Streaming for real-time data availability with architectures such as Kappa</li> <li>Attempting to unify the batch and stream processing for data transformation with frameworks such as Apache Beam</li> <li>Fully embracing cloud based managed services with modern cloud-native implementations with isolated compute and storage</li> <li>Convergence of warehouse and lake, either extending data warehouse to include embedded ML training, e.g. Google BigQuery ML, or alternatively build data warehouse integrity, transactionality and querying systems into data lake solutions, e.g., Databricks Lakehouse</li> </ul> <p>The third generation data platform is addressing some of the gaps of the previous generations such as real-time data analytics, as well as reducing the cost of managing big data infrastructure. However it suffers from many of the underlying characteristics that have led to the limitations of the previous generations. (Dehghani, 2022)</p> <p></p>"},{"location":"resources/thesis/my-data-mesh-thesis/#2-principles","title":"2. Principles","text":"<p>There are four principles that can capture what supports the logical architecture and operating model of Data Mesh.</p> <p></p>"},{"location":"resources/thesis/my-data-mesh-thesis/#21-domain-oriented-ownership","title":"2.1 Domain-oriented ownership","text":"<p>This principle aim to decentralize the ownership of sharing analytical data to business domains who are closest to the data \u2014 either are the source of the data or its main consumers. Decompose the data artefacts (data, code, metadata, policies) - logically - based on the business domain they represent and manage their life cycle independently. (Dehghani, 2022)</p>"},{"location":"resources/thesis/my-data-mesh-thesis/#22-data-as-a-product","title":"2.2 Data as a Product","text":"<p>This simply means applying widely used product thinking to data and, in doing so, making data a first-class citizen: supporting operations with its owner and development team behind it.</p> <p>Existing or new business domains become accountable to share their data as a product served to data users \u2013 data analysts and data scientists. Data as a product introduces a new unit of logical architecture called, data product quantum, controlling and encapsulating all the structural components \u2014 data, code, policy and infrastructure dependencies \u2014 needed to share data as a product autonomously. (Dehghani, 2022)</p>"},{"location":"resources/thesis/my-data-mesh-thesis/#23-self-serve-data-platform","title":"2.3 Self-serve Data Platform","text":"<p>The principle of creating a self-serve infrastructure is to provide tools and user-friendly interfaces so that generalist developers can develop analytical data products where, previously, the sheer range of operational platforms made this incredibly difficult.</p> <p>A new generation of self-serve data platform to empower domain-oriented teams to manage the end-to-end life cycle of their data products, to manage a reliable mesh of interconnected data products and share the mesh\u2019s emergent knowledge graph and lineage, and to streamline the experience of data consumers to discover, access, and use the data products. (Dehghani, 2022)</p>"},{"location":"resources/thesis/my-data-mesh-thesis/#24-federated-computational-governance","title":"2.4 Federated Computational Governance:","text":"<p>This is an inevitable consequence of the first principle. Wherever you deploy decentralised services\u2014microservices, for example\u2014it\u2019s essential to introduce overarching rules and regulations to govern their operation. As Dehghani puts it, it\u2019s crucial to \"maintain an equilibrium between centralisation and decentralisation\".</p> <p>A data governance operational model that is based on a federated decision making and accountability structure, with a team made up of domains, data platform, and subject matter experts \u2014 legal, compliance, security, etc. It creates an incentive and accountability structure that balances the autonomy and agility of domains, while respecting the global conformance, interoperability and security of the mesh. The governance model heavily relies on codifying and automated execution of policies at a fine-grained level, for each and every data product. (Dehghani, 2022)</p>"},{"location":"resources/thesis/my-data-mesh-thesis/#3-data-mesh-architecture","title":"3. Data Mesh Architecture","text":""},{"location":"resources/thesis/my-data-mesh-thesis/#31-data-product","title":"3.1 Data Product","text":"<p>A data product consists of the code including pipelines, the data itself including metadata and the infrastructure required to run the pipelines. The goal is to have application code and data pipelines code under the same domain owned by the same team. As you can see, we are shifting responsibility to the people who actually understand the domain and create the data, instead of \u201cdata\u201d owners in the data plane that usually struggle to understand the data and create friction between teams. This means that the people who change the application and within the data, are in charge of owning that change using schema versioning and documentation to broadcast the data evolution to the different stakeholders. This ensures that data schema changes can be implemented easily by the data creators instead of data analysts trying to adapt to changes after the fact.</p> <p>Data as a product inverts the model of responsibility compared to the past paradigms. In data lake or data warehousing architectures the accountability to create data with expected quality and integrity resides downstream from the source; in the case of data warehouse the accountability remains with the warehouse team, and in the case of the lake it\u2019s left with the consumers.</p> <p></p>"},{"location":"resources/thesis/my-data-mesh-thesis/#32-roles","title":"3.2 Roles","text":"<p>The idea behind the Domain Ownership principle is to use Domain Driven Design in the data plane alongside the operational plane to close the gap between the two planes. The goal is to split teams around business domains being each team fully cross functional, but not only on the operational level (DevOps) but also on the analytical level. Each team should have a data owner, data engineers and QA teams that not only validate mircroservices but also data quality.</p> <p>Following the Zhamak Dehghani book we find two specific key roles related to a data product domain:</p> <ul> <li>Data product developer roles: A group of roles responsible for developing, serving and maintaining the domain\u2019s data products as long as the  data products remain to exist and serve its consumers. Data product developers will be working alongside regular developers in the domain. Each domain team may serve one or multiple data products.</li> </ul> <p>Key domain specific objects: </p> <ul> <li>Transformation code, the domain-specific logic that generates and maintains the     data</li> <li>Tests to verify and maintain the domain\u2019s data integrity.</li> <li>Tests to continuously monitor that the data product meets its quality guarantees.</li> <li> <p>Generating data products meta-data such as its schema, documentation, etc.</p> </li> <li> <p>Data product owner: A role accountable for the success of domain\u2019s data products in delivering value, satisfying and growing the data consumers, and defining the lifecycle of the data products.</p> </li> </ul> <p>Domain data product owners must have a deep understanding of who the data users are, how they use the data,and what are the native methods that they are comfortable with consuming the data. The conversation between users of the data and product owners is a necessary piece for establishing the interfaces of data products.</p> <ul> <li>Data Mesh Architect: A role responsible for the infrastructure team, with the big picture of the Data Mesh, ensuring the data is self-served and acting as the link between the infrastructure layer and the Federated Governance team.</li> </ul> <p>Following the Data Mesh layer Architecture we would have three team types:</p> <ul> <li>Data infrastructure teams (located on the infrastructure plane): Providing the underliying infrastructure required to build, run and monitor data products.</li> <li>Data Product teams (located on the Data Product developer plane): Supporting the common data product developer journey. They are conformed by the Data Product Developer roles and a Data Product Owner.</li> <li>Federated Computatinal Governance Team (located on the mesh supervision plane): maintaining an equilibrium between centralization and decentralization; what decisions need to be localized to each domain and what decisions should be made globally for all domains.</li> </ul> <p>In a data mesh, each data product team is in charge of dealing with the data related to the team domain. They must gather the data and move it to the right storage so it can be easily consume by data users. Data engineers will be part of the team and they may use stream engines to move the data and perform ETL or run data pipelines in a batch or micro batch fashion. The key is that a data pipeline is simply an internal complexity and implementation of the data domain and is handled internally within the domain instead of having separate data engineering teams.</p> <p>On this next visualization, I tried to reflect the teams and roles involved in a Data Mesh. These roles are not exclusive, they may be conditioned by the company, the organizational structure, and the platform differing considerably, but it's important to highlight the different layers and the teams to which they belong.</p> <p></p>"},{"location":"resources/thesis/my-data-mesh-thesis/#33-self-serve-platform","title":"3.3 Self-Serve Platform","text":"<p>Data Mesh\u2019s fourth principle, self-serve data infrastructure as a platform, exists. It is not that we have any shortage of data and analytics platforms and technologies, but we need to make changes to them so that they can scale out sharing, accessing and using analytical data, in a decentralized manner, for a new population of generalist technologists. This is the key differentiation of data platforms that enable a Data Mesh implementation.</p> <p>We need self-service data platforms that can easily scale and can be self provisioned. Standard tooling is required to provide a good experience to data users. This includes monitoring, observability, security, etc. This is something indispensable in the OLTP world, but somehow it is often missing in the data world where lack of observability creates data quality issues and security vulnerabilities. Although tools were created to overcome these issues, they are focused on solving a specific problem using a new tool instead of reusing the concepts and tooling from the OLTP world. We are used to implement static code analysis, container image scanners, metrics, logs etc for our code, but checking the data and metadata for quality issues, generating change logs, lineage, etc is often missing in the OLAP world.</p> <p></p>"},{"location":"resources/thesis/my-data-mesh-thesis/#34-data-governance","title":"3.4 Data Governance","text":"<p>A data mesh implementation requires a governance model that embraces decentralization, interoperability through global standardization, and an automated execution of decisions in the platform. The idea is to create a team to maintain an equilibrium between centralization and decentralization; that is what decisions need to be localized to each domain and what decisions should be made globally for all domains.</p> <p>Data mesh\u2019s federated governance, embraces change and multiple bounded contexts. A supportive organizational structure, incentive model and architecture is necessary for the federated governance model to function in order to arrive at global decisions and standards for interoperability, while respecting autonomy of local domains, and implement global policies effectively.</p> <p>The idea is to localize decisions as close to the source as possible while keeping interoperability and integration standards at a global level, so the mesh components can be easily integrated. In a data mesh, tools can be used to enforce global policies such as GDPR enforcement or access management and also local policies where each domain sets its own policies for their data products such as access control or data retention.</p> Pre Data Mesh Governance Data Mesh Governance Centralized team Federated team Responsible for data quality Responsible for defining how to model what constitutes quality Responsible for data security Responsible for defining aspects of data security i.e. data sensivity levels for the platform to build in and monitor automatically Responsible for complying with regulation Responsible for defining the regulation requirements for the platform to build in and monitor automatically Centralized custodianship of data Federated custodianship of data by domains Responsible for global canonical data modeling Responsible for modeling polysemes, data elements that cross the boundaries of multiple domains Team is independent from domains Team is made of domains representatives Aiming for a well defined static structure of data Aiming for enabling effective mesh operation embracing a continuously changing and dynamic topology of the mesh Centralized technology used by monolithic lake/warehouse Self-serve platform technologies used by each domain Measure success based on number or volume of governed data (tables) Measure success based on the network effect, the connections representing the consumption of data on the mesh Manual process with human intervention Automated processes implemented by the platform Prevent error Detect error and recover through platform's automated processing"},{"location":"resources/thesis/my-data-mesh-thesis/#35-data-mesh-architecture","title":"3.5 Data Mesh Architecture","text":"<p>There are many applied Data Mesh architectures, some are more decentralized than others, and we have different tools and services. Even though there isn't a Data Mesh architecture itself, we can define and relate their main components.</p>"},{"location":"resources/thesis/my-data-mesh-thesis/#storage","title":"Storage","text":"<p>Deep Store: a Repository store to make data addressable using URLs, access control, versioning, encryption, metadata, and observability. Where you can easily monitor and govern the data stored in a data lake.</p> <p>New modern engines have been created to be able to unify real time and batch data and perform OLAP queries with very low latency. As an example, Apache Druid can ingest and store massive amounts of data in a cost efficient way, minimizing the needs for data lakes.</p> <p>Data Warehouse / Data Virtualization: A fast data layer in a relational database used for data analysis, particularly of historical data.</p> <p>The main advantage of Data Virtualization is speed-to-market, where we can build a solution in a fraction of the time it takes to build a data warehouse.  This is because you don\u2019t need to design and build the data warehouse and the ETL to copy the data into it, and also don\u2019t need to spend as much time testing.</p>"},{"location":"resources/thesis/my-data-mesh-thesis/#stream-engine-platform","title":"Stream engine platform","text":"<p>A stream engine platform such as Kafka or Pulsar to migrate to microservices and unify batch and streaming. This is the first step in order to close the gap between OLTP and OLAP workloads, both can use the streaming platform either to develop event driven microservices or to move data.</p> <p>These platforms will allow you to duplicate the data in different formats or databases in a reliable way. This way you can start serving the same data in different shapes to match the needs of the downstream consumers.</p>"},{"location":"resources/thesis/my-data-mesh-thesis/#metadata-and-data-catalogs","title":"Metadata and Data Catalogs","text":"<p>A collection of metadata, combined with data management and search tools, that helps analysts and other data users to find the data that they need, serves as an inventory of available data, and provides information to evaluate fitness data for intended uses.</p>"},{"location":"resources/thesis/my-data-mesh-thesis/#query-engines","title":"Query Engines","text":"<p>This type of tools focus on querying different data sources and formats in an unified way. The idea is to query your data lake using SQL queries like if it was a relational database, although it has some limitations. Some of these tools can also query NoSQL databases and much more. Query engines are the slowest option but provide the maximum flexibility.</p> <p>Some query engines can integrate with data catalogs and join data from different data sources.</p> <p></p>"},{"location":"resources/thesis/my-data-mesh-thesis/#4-data-mesh-implementation","title":"4. Data Mesh Implementation","text":"<p>Currently, we have many kinds of Data Meshes, some highly centralized and others more de-centralized. Migrating to a decentralized architecture is difficult and time consuming, it takes a long time to reach the right level of maturity to be able to run it at a scale. Although there are technical challenges, the main difficulty is trying to change the organization mindset.</p>"},{"location":"resources/thesis/my-data-mesh-thesis/#41-data-mesh-investment-factors","title":"4.1 Data Mesh Investment Factors","text":"<p>Data Mesh is difficult to implement because of the de-centralized nature, but at the end, it is required in order to solve the scalability issues that companies are currently facing. Centralized architectures work better for small companies or companies which are not data driven.</p> <p>Only implement a data mesh if you have difficulties scaling, team friction, data quality issues, bottlenecks and governance/securities problems. You must also have a Big Data problem with huge amounts of structure and unstructured data.</p> <p>In my opinion, to be able to decide if your company needs the Data Mesh paradigm, there are so many factors you should analyze before taking the step. That said, after reading different articles, there are three main factors to keep in mind:</p> <ul> <li>The number of data sources your company has to feed the analytical Data Platform.</li> <li>The size of your data team, how many data analysts, data engineers, and product managers.</li> <li>The number of data domains your company has. How many functional teams (marketing, sales, operations, etc.) rely on your data sources to drive decision-making, and how many products does your company have?</li> </ul> <p></p> <p>There is an excellent article from Barr Moses about Data Mesh. She proposes a simple calculation to determine if it makes sense for your organization to invest in a data mesh.</p>"},{"location":"resources/thesis/my-data-mesh-thesis/#42-building-a-data-mesh","title":"4.2 Building a Data Mesh","text":"<p>In their blogs, Javier Ramos and Sven Balnojan have done an excellent job explaining the different steps required to build a data mesh. I really recommend checking their articles to get more details.</p> <ul> <li>Data Mesh Applied by Sven Balnojan</li> <li>Building a Data Mesh: A beginners guide by Javier Ramos</li> </ul> <p>I have tried to summarize the different steps to decentralize your architecture and, start building a Data Mesh.</p> <ul> <li> <p>Addressable data</p> <p>Adress your data by standardizing path names and using the REST approach to name the data products using resource names, and add SLAs to the end points and monitor them to make sure the data is always available.</p> <p>Example: <pre><code>s3://my-domain/data-service-a/resource/date.\n</code></pre></p> <p>Re route your query engines and BI tools to use the new data products which are independent and addressable.</p> <p>The data infrastructure team will be in charge of this step, still using a centralized approach.  </p> </li> <li> <p>Discoverability (Metadata and Data Catalog)</p> <p>Create a space to find the new data source with the following capabilities:</p> <ul> <li>Search, discover and \u201cadd to the cart\u201d for data within your enterprise. </li> <li>Request access and grant access to data products in a way that is usable to data owners and consumers without the involvement of a central team.</li> </ul> <p>In this step, work on the data product features adding tests for data quality, lineage, monitoring, etc.</p> </li> <li> <p>Step 3 : Decentralize and implement DDD</p> <p>Now we can start adding nodes to our data mesh. </p> <ul> <li>Migrate the ownership into the domain team creating the data moving towards a de centralized architecture. Each team must own their data assets, ETL pipelines, quality, testing, etc.</li> <li>Introduce the federated governance for data standardization,security and interoperability, by introducing DataOps practices and improving observability and self services capabilities. This way you can unify your OLTP and OLAP processes and tooling.</li> </ul> </li> </ul> <p>Once you have created your first \u201cdata microservice\u201d, repeat the above process breaking the legacy data monolith into more decentralized services.</p>"},{"location":"resources/thesis/my-data-mesh-thesis/#5-case-studies","title":"5. Case Studies","text":""},{"location":"resources/thesis/my-data-mesh-thesis/#51-zalando","title":"5.1 Zalando","text":"<p>An excellent presentation by Max Schultze and Arif Wider, about the Zalando analytics cloud journey. The presentation begins with their legacy analytics and how they manage to evolve this legacy from the ingestion, storage, and serving layer.</p> <p> Data Mesh in Practice: How Europe's Leading Online Platform for Fashion Goes Beyond the Data Lake by Max Schultze (Zalando)</p>"},{"location":"resources/thesis/my-data-mesh-thesis/#52-intuit","title":"5.2 Intuit","text":"<p>A great post from Tristan Baker about the followed strategy at Intuit. They have migrated from an on-premise architecture of centrally-managed analytics data sets and data infrastructure tools to a fully cloud-native set of data and tools. Tristan will take you through a full articulation of their vision, inherent challenges, and strategy for building better data-driven systems at Intuit.</p> <p> Intuit\u2019s Data Mesh Strategy</p>"},{"location":"resources/thesis/my-data-mesh-thesis/#53-saxo-bank","title":"5.3 Saxo Bank","text":"<p>An outstanding post by Sheetal Pratik about the Saxo Journey. They show how they implement the Data Mesh paradigm and focus on the Governance Framework and Data Mesh architecture. The post has very clear diagrams to explain their points.</p> <p> Enabling Data Discovery in a Data Mesh: The Saxo Journey</p>"},{"location":"resources/thesis/my-data-mesh-thesis/#54-jp-morgan-chase","title":"5.4 JP Morgan Chase","text":"<p>An AWS blog co-authored with Anu Jain, Graham Person, and Paul Conroy from JP Morgan Chase.</p> <p>They provide a blueprint for instantiating data lakes that implements the mesh architecture in a standardized way using a defined set of cloud services, we enable data sharing across the enterprise while giving data owners the control and visibility they need to manage their data effectively.</p> <p> How JPMorgan Chase built a data mesh architecture to drive significant value to enhance their enterprise data platform</p>"},{"location":"resources/thesis/my-data-mesh-thesis/#55-kolibri-games","title":"5.5 Kolibri Games","text":"<p>A presentation by Ant\u00f3nio Fitas, Barr Moses and, Scorr O'Leary. They talks about the evolution of teams on the data/engineering side as well as the pain points of their setup at that point, and discuss how to evaluate whether data mesh is right for a company and how to measure the return on investment of data mesh, especially the increase in agility and the increase in the number of decisions you deem \"data driven\".</p> <p> Kolibri Games' Data Mesh Journey and Measuring Data Mesh ROI</p>"},{"location":"resources/thesis/my-data-mesh-thesis/#56-netflix","title":"5.6 Netflix","text":"<p>A presentation by Justin Cunningham about the Keystone Platform\u2019s unique approach to declarative configuration and schema evolution, as well as our approach to unifying batch and streaming data and processing covered in depth.</p> <p> Malla de datos de Netflix: procesamiento de datos componibles - Justin Cunningham</p>"},{"location":"resources/thesis/my-data-mesh-thesis/#57-adevinta","title":"5.7 Adevinta","text":"<p>An excellent post by Xavier Gumara Rigol, about how Adevinta evolved from a centralised approach to data for analytics to a data mesh by setting some working agreements.</p> <p> Building a data mesh to support an ecosystem of data products at Adevinta</p>"},{"location":"resources/thesis/my-data-mesh-thesis/#58-hellofresh","title":"5.8 HelloFresh","text":"<p>In this blog, Clemence W. Chee describes their journey of implementing the Data Mesh principles by showing the different phases they have faced.</p> <p> HelloFresh Journey to the Data Mesh</p>"},{"location":"resources/thesis/my-data-mesh-thesis/#6-conclusion","title":"6. Conclusion","text":"<p>Data Mesh isn't a static platform nor an architecture. Data Mesh is a product continuously evolving, and it may have different interpretations, but the core principles always remain. Domain-driven design, decentralization, data ownership, automation, observability, and federated governance.</p> <p>The main important aspect, as Zhamak Dehghani mentions, is to stop thinking about data as an asset, like something we want to keep and collect. The new way of imaging the data should shift from an asset to a product. The moment we think about data as a product, we start delighting the experience of the consumers, shifting the perspective from the producer collecting data to the producer serving the data.</p> <p>Start building a Data Mesh can be overwhelming. First, we need to understand this is an evolutionary path starting from your own company vision and introduce the Mesh principles slowly. We may start by select two or three source-aligned use cases, locating the domain working backwards from the use case to the sources and empower those teams to start serving those data products. Then, think about the platform capabilities we need and put the platform team in place to start building this first generation Data Mesh. Finally, we iterate with new use cases moving towards the Data Mesh vision.</p>"},{"location":"resources/thesis/my-data-mesh-thesis/#7-data-mesh-vocabulary","title":"7. Data Mesh Vocabulary","text":"<p>Data Mesh is a complex paradigm with many abstract terms. At the next table, I tried to extract the main vocabulary around this topic.</p> Word Definition Data Mesh A data mesh is a set of read-only products, designed for sharing the data on the outside for non-real-time consumption/analytics.  They are shared in an interoperable way so you can combine data from multiple domains owned by each domain team. Data Domain A business domain, where experts analyze data and build reports themselves, with minimal IT support. A data domain should create and publish their data as a product for the rest of the business to consume as well. Data Product A Data Product is a collection of datasets concerning a certain topic which has risen to fulfill a certain purpose, yet which can support multiple purposes or be used as building block for multiple other data products. Data Product Owner A role accountable for the success of domain\u2019s data products in delivering value, satisfying and growing the data consumers, and defining the lifecycle of the data products. Data Mesh Architect A role responsible for the infrastructure team, with the big picture of the Data Mesh, ensuring the data is self-served and acting as the link between the infrastructure layer and the Federated Governance team. Source-aligned domain Analytical data reflecting the business facts generatedby the operational systems. This is also called native data product. Aggregated domain Analytical data that is an aggregate of multiple upstream domains. Consumer-aligned domain Analytical data transformed to fit the needs of one or multiple specific use cases and consuming applications. This is also called fit-for-purpose domain data. Inside data Refers to the encapsulated private data contained within the service itself. As a sweeping statement, this is the data that has always been considered \"normal\"\u2014at least in your database class in college. The classic data contained in a SQL database and manipulated by a typical application is inside data. Outside data Data on the outside refers to the information that flows between these independent services. This includes messages, files, and events. It's not your classic SQL data. Architectural Quantum Smallest unit of architecture that can be independently deployed with high functional cohesion, and includes all the structural elements required for its function. IDP Input data ports to the data product. ODP Output data ports from the data product."},{"location":"resources/thesis/my-data-mesh-thesis/#8-discovery-roadmap","title":"8. Discovery Roadmap","text":"<p>To elaborate this Thesis, I have been recollecting links and resources about the Data Mesh paradigm. I wanted to share the discovery path I've followed to understand the Data Mesh implications.</p>"},{"location":"resources/thesis/my-data-mesh-thesis/#81-zhamak-dehghani","title":"8.1 Zhamak Dehghani","text":"<ul> <li>How to Move Beyond a Monolithic Data Lake to a Distributed Data Mesh</li> <li>Data Mesh Principles and Logical Architecture</li> <li>Introduction to Data Mesh: A Paradigm Shift in Analytical Data Management by Zhamak Dehghani (Part I)</li> <li>How to Build a Foundation for Data Mesh: A Principled Approach by Zhamak Dehghani (Part II) </li> </ul>"},{"location":"resources/thesis/my-data-mesh-thesis/#82-introductory-content","title":"8.2 Introductory Content","text":"<ul> <li>Data Mesh Score calculator</li> <li>Decentralizing Data: From Data Monolith to Data Mesh with Zhamak Dehghani, Creator of Data Mesh</li> <li>Data Mesh: The Four Principles of the Distributed Architecture by Eugene Berko</li> <li>How to achieve Data Mesh teams and culture? </li> <li>Decentralizing Data: From Data Monolith to Data Mesh with Zhamak Dehghani, Creator of Data Mesh</li> </ul>"},{"location":"resources/thesis/my-data-mesh-thesis/#83-deep-dive","title":"8.3 Deep Dive","text":"<ul> <li>Anatomy of Data Products in Data Mesh</li> <li>Building a Data Mesh: A beginners guide</li> <li>Data Mesh Applied: Moving step-by-step from mono data lake to decentralized 21st-century data mesh.</li> <li>There\u2019s More Than One Kind of Data Mesh: Three Types of Data Meshes</li> <li>Building a successful Data Mesh \u2013 More than just a technology initiative</li> <li>How the **ck (heck) do you build a Data Mesh?</li> <li>Data Mesh architecture patterns</li> <li>Data Mesh: Topologies and domain granularity</li> </ul>"},{"location":"resources/thesis/my-data-mesh-thesis/#acknowledgements","title":"Acknowledgements","text":"<p>I am so grateful to Data Mesh Learning Community founded and run by Scott Hirleman. I appreciate the help from the community, the meetups, and the resources published and organized. It has been a great point to start researching for this thesis.</p> <p>I also wanted to acknowledge all the authors from my sources for sharing their knowledge.</p>"},{"location":"resources/thesis/my-data-mesh-thesis/#references-and-links","title":"References and links","text":"<ul> <li>Dehghani, Z. Data Mesh: Delivering Data-Driven Value at Scale (1st ed.). O\u2019Reilly.</li> <li>Data Mesh Learning Community</li> <li>Building a Data Mesh: A beginners guide</li> <li>What is a Data Mesh and How Not to Mesh it Up</li> <li>More Resources like this here</li> </ul>"},{"location":"resources/thesis/my-data-product-thesis/","title":"My data product thesis","text":"<p>After the good reception of the My Data Mesh Thesis article, I wanted to go down a bit into the detail of the central component of the Data Mesh paradigm, the Data Product.</p> <p>During the Data Mesh presentations we had in my company, I realized that the concept of data product was not completely understood. The Data Mesh paradigm was well understood, but when we focused on the data product, questions arose about whether it was a set of tables,  infrastructure or a Data Lake on a smaller scale.</p> <p>With this post I intend to focus on the Data Product within the Data Mesh paradigm to clarify its mission, architecture and scope.</p>"},{"location":"resources/thesis/my-data-product-thesis/#1-what-is-a-data-product","title":"1. What is a Data Product","text":"<p>When we think of data in an analytical context, we imagine a set of tables in a repository, which has gone through a cleaning process guaranteeing quality and value to its consumers.</p> <p>On the other hand, if we think of a product, we understand a product as a set of characteristics and attributes that a buyer accepts to satisfy some needs.</p> <p>But how can we treat data as a product? If we try to think of data as a product, we can establish a series of assumptions:</p> <ul> <li>The type of product will be intangible.</li> <li>Your buyer will be a consumer.</li> <li>You always have to add value or respond to a need.</li> <li>It will have some attributes that define and limit it.</li> <li>The distribution channel will be direct and online.</li> </ul> <p>If we review these five assumptions and look for a simile in the market, we find business models based on eCommerce through a managed marketplace such as (Amazon, eBay, Etsy, etc.). To apply this simile in a data context, we must abandon the conception of data as a set of tables and start thinking of data as a product under a name, with defined attributes, exposed through a platform and that a consumer accepts with quality and value.</p> <p>A data product is a collection of datasets, metadata, code, and policies concerning a certain topic and serving a certain purpose in a self-serve manner. Even though, it can support multiple purposes or be used as a building block for multiple other data products.</p>"},{"location":"resources/thesis/my-data-product-thesis/#2-architecture-of-a-data-product","title":"2. Architecture of a Data Product","text":"<p>Data as a product introduces a new unit of logical architecture called, data product quantum, controlling and encapsulating all the structural components \u2014 data, metadata, code, policy and infrastructure dependencies \u2014 needed to share data as a product autonomously. (Dehghani, 2022)</p> <p>A Data product is the smallest unit of architecture that can be independently deployed with high functional cohesion, and includes all the structural elements required for its function.</p> <p>From an architectural point of view, we can divide a data product in five components: the product, the input ports, the control ports, the discovery ports, and the output ports.</p> <p>The product: is the core building block conformed by a collection of datasets, metadata, code, and policies concerning a certain topic and supported by the platform. It is the product with a value and quality itself.</p> <p>The input ports: the receiving mechanism for data that will constitute the data product. The input ports define the format and protocol in which data can be read. We distinguish here between operational source systems and other data products, which might be either internal or coming from other domains.</p> <p>The control ports: Monitoring, logs, metrics to manage and observe the data product and the public and self-description (ownership, organizational unit, license, version, expiration date, etc.) of the Data Product reachable through the marketplace.</p> <p>The output ports: define the format and consumption protocol in which data can be exposed. For example, the output port can be a database table, file, API, or reports. It can be accessed to a final consumer or by multiple other data products.</p> <p></p>"},{"location":"resources/thesis/my-data-product-thesis/#3-data-products-in-a-data-mesh-ecosystem","title":"3. Data Products in a Data Mesh ecosystem","text":"<p>The data product (1) is the most important independent unit of the Data Mesh paradigm. To get a better understanding of what it means, we must comprehend the elements interacting within the Data Mesh ecosystem. Recall that a data product belongs within a domain (2) where we can find other data products (7). Besides, a data product is managed by a Data Product Owner (3) and maintained and developed by a data product developer team (4). It is common to find a team of developers across multiple domains and data products, but there should always be a single data product owner per domain.</p> <p>Regarding its relationship with consumers (7), the data product is related to them through a marketplace (5) where they are published to be discovered. Once a consumer has located the data product that interests him, he can request access to the output ports (6) available in the data product.</p> <p></p>"},{"location":"resources/thesis/my-data-product-thesis/#4-types-of-data-products","title":"4. Types of Data Products","text":"<p>There isn't just one type of data product, as each data product may have different requirements and capabilities. We can classify them based on how aligned they are to consumption or origins and by the purpose they serve.</p>"},{"location":"resources/thesis/my-data-product-thesis/#41-classification-by-data-alignment","title":"4.1 Classification by data alignment","text":"<p>Compared to traditional architectures, the Data Mesh paradigm does not seek a \"Single Source of Truth.\" It is essential to consider that in a Data Mesh architecture, the concept of the golden record is discarded in order to provide data domains that are independent of each other and publish their fully interoperable data products. In this way, data products break the silos of traditional architectures and can be more or less aggregated.</p> <ul> <li>Source-aligned: Analytical data reflecting the business facts generated by the operational systems. This is also called native data product.</li> <li>Aggregated: Analytical data that is an aggregate of multiple upstream data products.</li> <li>Consumer-aligned: Analytical data transformed to fit the needs of one or multiple specific use cases and consuming applications. This is also called fit-for-purpose data.</li> </ul>"},{"location":"resources/thesis/my-data-product-thesis/#42-classification-by-consuming-purpose","title":"4.2 Classification by consuming purpose","text":"<p>Simon O'Regan makes a great classification of data product typologies based on their purpose. In this article Designing Data Products by Simon O'Regan, you can see in-depth this classification of types of data products. </p> <ul> <li>Raw data: exposed data as it is in the operational systems with small processing or cleansing steps, most of the processing work is done on the consumer's side.</li> <li>Derived data:  data exposed after being processed and enriched on the platform side.</li> <li>Algorithms: data run through an algorithm returning information or insights.</li> <li>Decision support: data exposed as relevant information in an easy-to-digest format to the consumer to help them with decision-making. (dashboards, reports, etc.)</li> <li>Automated decision-making: Here we outsource all of the intelligence within a given domain, allowing the algorithm to do the work and present the user with the final output. (E.g.: Netflix product recommendations or Spotify\u2019s Discover Weekly).</li> </ul> <p></p>"},{"location":"resources/thesis/my-data-product-thesis/#5-designing-a-data-product","title":"5. Designing a Data Product","text":"<p>In my experience the best way to design a data product is through its capabilities. Data products, after all, are published to serve different use cases and therefore do not have a fixed structure. They can respond to batch, streaming, advanced analytics, reporting needs or even be a mix of the above.</p> <p>Following this approach, one of the best ways I have found to design a data product is by dividing its capabilities into its three levels of data life cycle: data aquisition, data Processing and, data delivery.</p>"},{"location":"resources/thesis/my-data-product-thesis/#51-data-acquisition","title":"5.1 Data acquisition","text":"<p>Data acquisition is the process of acquiring data for immediate use or storage in a repository.</p> <p>Data can be streamed in real time, ingested in batches or virtualized. In real-time data ingestion, each data item is imported as the source emits it. When data is ingested in batches, data items are imported in discrete chunks at periodic intervals of time. When data is virtualize, data is organized in a logical layer across the disparate systems.</p> <p>Capabilities:</p> <ul> <li>Batch ingest</li> <li>Real-time ingest</li> <li>Virtualization</li> </ul>"},{"location":"resources/thesis/my-data-product-thesis/#52-data-storage","title":"5.2 Data storage","text":"<p>Data storage is the retention of information using technology specifically developed to keep that data and have it as accessible as necessary. A data store is a repository for persistently storing and managing collections of data.</p> <p>Capabilities:</p> <ul> <li>Structured data repository</li> <li>Unstructured data repository</li> </ul>"},{"location":"resources/thesis/my-data-product-thesis/#53-data-processing","title":"5.3 Data processing","text":"<p>Data processing is the collection and manipulation of data to produce meaningful information.</p> <p>Capabilities:</p> <ul> <li>Validation</li> <li>Aggregation</li> <li>Advance analytics</li> </ul>"},{"location":"resources/thesis/my-data-product-thesis/#54-data-delivery","title":"5.4 Data delivery","text":"<p>Data delivery is the process of sharing data as a product to a consumer or another Data Product.</p> <p>Capabilities: - API - Database table - BI (Reporting, dashboards and visualizations) - Files</p> <p></p>"},{"location":"resources/thesis/my-data-product-thesis/#6-identifying-a-data-product","title":"6. Identifying a Data Product","text":"<p>A great approach to start identifying data products is to introduce the principle of the data domain, selecting those domains with use cases close to the origins where your data is produced (orders, customers, suppliers, products, etc.) After unlocking these use cases by giving them an address and a directory, we allow consumers to discover them.</p> <p>There are several publications that discuss in depth how data products should be identified. I recommend leaning on these three publications:</p> <ul> <li>Thoughtworks: the Thoughtworks publication explains the difference between a data asset and a data product and proposes a template to define the scope of a data product.</li> <li>Agilelab: the Agilelab team identifies data products by analyzing business processes. In their article they show Alberto Brandolini's Event Storming technique and his Data Product Flow methodology.</li> <li>INNOQ: the INOQ team proposes a canvas to define a data product. The proposed canvas is suitable for working collaboratively on data products design.</li> </ul>"},{"location":"resources/thesis/my-data-product-thesis/#7-data-as-a-product","title":"7. Data as a Product","text":"<p>Data products are \u201cthe smallest unit of architect that can be independently deployed and managed.\u201d In the book Data Mesh, Delivering Data-Driven Value at Scale, Zhamak Dehghani says that Data Products are \u201cdiscoverable, understandable, trustworthy, addressable, interoperable, and composable, secure, natively accessible, and valuable on its own\u201d. She is applying widely used product thinking to a data domain.</p> <ul> <li>Discoverable: a centralized discoverability service allowing data consumers to discover all available data products with their meta information such as their owners, source of origin, lineage, sample datasets, etc.</li> <li>Understandable: they can be independently discovered, understood and consumed.</li> <li>Addressable: a unique address following a global convention that helps its users to programmatically access it.</li> <li>Secure: Data products ensure that all data is secure both at-rest and in-motion, applying access control policies centrally but applied at the time of access to each individual dataset product.</li> <li>Interoperable: the ability to correlate data across domains following certain standards and harmonization rules.</li> <li>Trustworthy: the owners of the data products must provide an acceptable Service Level Objective around the truthfulness of the data. The target value or range of a data integrity (quality) indicator vary between domain data products.</li> <li>Natively accessible: access to the discoverability service is open and native to potential consumers throughout the organization.</li> <li>Self-valuable: data products must have a value on their own without dependencies on other data products.</li> </ul>"},{"location":"resources/thesis/my-data-product-thesis/#71-product-thinking-in-practice","title":"7.1 Product thinking in practice","text":"<p>Product thinking is the journey from the problem space of the users to the solution space of the business. The goal of this journey is to reduce the gap between users and the business. </p> <p>When I talk at work about serving data as a product, I like to start with an example like the one in the image below. We can see an amazon product on the front of its marketplace, with which we are all familiar. From this point, if we translate Dehgani's product thinking principles into this example, we can see how each of them fits with the way Amazon offers its products.</p> <p></p>"},{"location":"resources/thesis/my-data-product-thesis/#72-serving-data-as-a-product","title":"7.2 Serving data as a product","text":"<p>As a first exercise, we can reconfigure the Amazon product delivery screen, but rethinking how it would be in a data marketplace. When solve this exercise we discover a screen similar to this one.</p> <p>The heart of a data product is how data is served to consumers, from discovery to consumption. The goal is to reduce the gap between consumers and data.</p> <p></p>"},{"location":"resources/thesis/my-data-product-thesis/#8-data-product-domain-model","title":"8. Data product domain model","text":"<p>The core idea behind the data mesh concept: Domain-oriented decentralization for analytical data. A data mesh architecture enables domain teams to perform cross-domain data analysis on their own and interconnects data, similar to APIs in a microservice architecture. The domain teams own and know their domain, including the information needs of the business. They ingest, process, analyze and deliver data on their own.</p> <p>While domains reside in a logical dimension, data products reside in a technical one aligned with platform capabilities. Being separated into two different dimensions allows data products to be self-contained and interoperable, permitting domains to grow and change over the life of the platform.</p> <p>Data domains are high level categories of data based on a logical grouping of items of interest to the organization, or areas of interest within the organization. A domain where experts analyze data and build reports themselves, with minimal IT support. A data domain should create and publish their data as a product for the rest of the business to consume as well.</p> <p></p>"},{"location":"resources/thesis/my-data-product-thesis/#9-conclusion","title":"9. Conclusion","text":"<p>To conclude, I would like to emphasize that a Domain and a Data Product are two components of the Data Mesh paradigm that work on different planes. While the Domain is a logical category at the organization level, the data product is a product with technical and infrastructure dependencies. In my experience, when we talk about the data product, we must consider the platform and the technologies we use when defining its scope. On the other hand, the data domain resides at a much higher level and is not limited by these decisions.</p> <p>I would also like to comment that ussually we approach Data Mesh as if it were a migration. In my belief, it's a mistake to come it in this way since a radical change is not necessary at all levels of the organization. It is better to proceed with developing a product with an agile methodology allowing you to take small but valuable steps. You can start generating the first decentralized domain, with an owner and a federated government to publish the first data product. In this way, more and more data products can be federated and pivoted toward the Data Mesh model that best fits our organization. </p> <p>To finish, let us remember that the final objective of the Data Product is to reduce the gap between the data and its consumers. We shouldn't value the Data Product for its content (the data), the principal value of a Data Product lies in its delivery and in the way in which it is received by its consumers. </p>"},{"location":"resources/thesis/my-data-product-thesis/#10-terminology","title":"10. Terminology","text":"<p>At the next table, I tried to extract the main vocabulary about Data Products.</p> Term Definition Data Domain Data domains are high level categories of data based on a logical grouping of items of interest to the organization, or areas of interest within the organization. Data Product It is a collection of datasets, metadata, code, and policies concerning a certain topic and serving a certain purpose in a self-serve manner. Input data ports Ports as receiving mechanism for data that will constitute the data product. The input ports define the format and protocol in which data can be read. Control data ports Ports to provide monitoring, logs, metrics to manage and observe the data product and the public and self-description of the Data Product reachable through the marketplace. Output data ports Ports to define the format and consumption protocol in which data can be exposed. Source-align Data product with analytical data reflecting the business facts generated by the operational systems. Aggregated-align Data product with analytical data that is an aggregate of multiple upstream data products. Consumer-align Data product with analytical data transformed to fit the needs of one or multiple specific use cases and consuming applications. Data acquisition It is the process of acquiring data for immediate use or storage in a repository. Data processing Data processing is the collection and manipulation of data to produce meaningful information. Data storage Data storage is the retention of information using technology specifically developed to keep that data and have it as accessible as necessary. Data delivery Data delivery is the process of sharing data as a product to a consumer or another Data Product Product thinking Product thinking is the journey from the problem space of the users to the solution space of the business. The goal of this journey is to reduce the gap between users and the business."},{"location":"resources/thesis/my-data-product-thesis/#references-and-links","title":"References and links","text":"<ul> <li>My Data Mesh Thesis</li> <li>Designing Data Products by INNOQ</li> <li>Designing Data Products: The 15 faces of Data Products are a little bit different. </li> <li>The Anatomy of a Data Product </li> <li>Data as a product vs data products. What are the differences?</li> <li>Applying Data Mesh principles to an IoT data architecture</li> <li>The Fundamentals of Building Better Data Products </li> <li>Product Thinking 101 by Naren Katakam</li> <li>More Resources like this here</li> </ul>"},{"location":"resources/tools/my-favorite-screen-splitter/","title":"Screen splitter","text":"<p>With this post, I would like to show my best screen splitters for Windows and Linux. A screen splitter helps me to organize my windows and tasks on the screen, and be more productive. It takes some time to get used to it, but once you get to it, they are your best allies and you could never go back to a regular screen.</p>"},{"location":"resources/tools/my-favorite-screen-splitter/#my-favorite-screen-splitter-for-windows","title":"My favorite Screen Splitter for windows","text":"<p>Big Thanks to @IvanYu for developing this stunning windows desktop screen splitter, and give it for free!!</p> <p>You can download this software from here.</p> <p>Sometimes we don't have admin rights to install software. So for these occasions, you can download the portable version here.</p> <p>WinDock is easy to use. Follow these steps to design your ideal configuration.</p> <ol> <li>Create a new profile and activate it.</li> <li>Add rules using specific corners, edges, or areas.</li> <li>Snap your windows against your selected regions and enjoy.</li> </ol> <p></p>"},{"location":"resources/tools/my-favorite-screen-splitter/#my-favorite-screen-splitter-for-ubuntu","title":"My favorite Screen Splitter for Ubuntu","text":"<p>Big Thanks to @negesti for developing this stunning Ubuntu desktop screen splitter, and give it for free!!</p> <p>You can download this software from here.</p> <p>PutWindows is more complicated to configure. It works by three defined sizes that will be used when you move a window to the same direction multiple times or using numeric commands like start + number. The number refers to a position on your screen. Once the rules are created it is so intuitive. Follow these steps to design your ideal configuration.</p> <ol> <li>Go to section Width &amp; Height.</li> <li>Select the three positions for each direction N, S, E, W.</li> <li>Go to section Keyboard Shortcuts.</li> <li>Customize your own triggered commands and enjoy.</li> </ol> <p></p>"},{"location":"resources/tools/my-favorite-screen-splitter/#my-daily-windows-profile","title":"My daily windows profile","text":"<p>My daily configurations are two, the first one (A) divides a third of the screen and half of the third. The second one (B) divides the screen into three columns.</p> <p>Configuration A: Allows me to focus on one main window while I have two auxiliary windows for uses like Spotify, the internet, or open directories.</p> <p>Configuration B: Allows me to work in parallel with three windows. It is very efficient for comparing files and multitask works.</p> <p>My Daily configurations</p>"},{"location":"resources/tools/my-favorite-screen-splitter/#references-and-links","title":"References and links","text":"<ul> <li>Ivan Yu personal site.</li> <li>Ivan Yu Github.</li> <li>PutWindows Github</li> <li>Negesti Github</li> <li>More Resources</li> </ul>"}]}